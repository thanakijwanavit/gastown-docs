# Gas Town Documentation — Full Content
# https://docs.gt.villamarket.ai
# Multi-agent orchestration for AI coding agents


========================================================================
# Gas Town Documentation
# URL: /docs/index
========================================================================

# Gas Town Documentation

**Gas Town** is a multi-agent orchestration system that coordinates fleets of AI coding agents working on your projects simultaneously. Built on top of [Claude Code](https://claude.ai/), it scales from a single agent to 20-30 concurrent workers — all managed through the `gt` CLI.

---

## What is Gas Town?

Gas Town solves the hard problems of multi-agent AI development:

| Challenge | Gas Town Solution |
|-----------|-------------------|
| Agents lose context on restart | Work persists in git-backed hooks |
| Manual coordination breaks at scale | Built-in mailboxes, identities, handoffs |
| Chaos beyond 4-10 agents | Scales to 20-30 agents comfortably |
| Lost work state | Stored in Beads issue tracking system |
| No merge coordination | Refinery serializes merges to main |

## How It Works

```mermaid
graph TD
    H[Human / Overseer] --> M[Mayor]
    M --> D[Deacon]
    D --> W1[Witness - Rig 1]
    D --> W2[Witness - Rig 2]
    D --> Dogs[Dogs - Cross-Rig]
    W1 --> P1[Polecats - Workers]
    W2 --> P2[Polecats - Workers]
    P1 --> R1[Refinery - Merge Queue]
    P2 --> R2[Refinery - Merge Queue]
    R1 --> Main1[Main Branch]
    R2 --> Main2[Main Branch]
```

You talk to the **Mayor**, who coordinates everything. The Mayor creates **Convoys** (batches of work), assigns tasks to **Polecats** (ephemeral workers), while **Witnesses** monitor health and the **Refinery** handles merges.

## Quick Start

```bash
# Install
brew install gastown

# Create workspace
gt install ~/gt --git
cd ~/gt

# Add a project
gt rig add myproject https://github.com/you/repo.git

# Start the Mayor and give instructions
gt mayor attach
```

Then tell the Mayor what to build. It handles the rest.

## Key Concepts at a Glance

- **Town** — Your workspace directory containing all projects
- **Rigs** — Project containers wrapping git repositories
- **Beads** — Git-backed issue tracking (`bd` CLI)
- **Convoys** — Batches of related work items tracked together
- **Hooks** — Persistent work state that survives crashes and restarts
- **Polecats** — Ephemeral worker agents that spawn, complete a task, then exit
- **Molecules** — Multi-step workflow execution units

## Navigation

| Section | Description |
|---------|-------------|
| [Getting Started](getting-started/index.md) | Installation, setup, and first convoy |
| [Architecture](architecture/index.md) | System design and agent hierarchy |
| [CLI Reference](cli-reference/index.md) | Complete `gt` command reference |
| [Agents](agents/index.md) | Detailed guide to each agent role |
| [Core Concepts](concepts/index.md) | Beads, hooks, convoys, molecules, and more |
| [Workflows](workflows/index.md) | Common workflow patterns |
| [Operations](operations/index.md) | Running, monitoring, and troubleshooting |
| [Guides](guides/index.md) | Usage guide, philosophy, and tips |

## Origin

Gas Town is created by [Steve Yegge](https://steve-yegge.medium.com/) and named after the oil refinery citadel in Mad Max. The alternative name "Gastown" references Vancouver B.C.'s historic Gastown district.

- [Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04) — Launch announcement
- [Gas Town Emergency User Manual](https://steve-yegge.medium.com/gas-town-emergency-user-manual-cf0e4556d74b) — Practical usage guide
- [Software Survival 3.0](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b) — Philosophy and predictions
- [GitHub Repository](https://github.com/steveyegge/gastown) — Source code


========================================================================
# Getting Started
# URL: /docs/getting-started
========================================================================

# Getting Started

This section covers everything you need to go from zero to running Gas Town with your first project.

## Prerequisites

Before installing Gas Town, ensure you have:

- **Go 1.23+** — Gas Town is written in Go
- **Git 2.25+** — Worktree support required
- **Beads (bd) 0.44.0+** — Issue tracking CLI
- **SQLite3** — Backend for Beads
- **Tmux 3.0+** — Recommended for multi-agent session management
- **Claude Code CLI** — Or a compatible AI coding agent runtime

## What You'll Learn

1. [Installation](installation.md) — Install Gas Town and its dependencies
2. [Quick Start](quickstart.md) — Set up your workspace and add your first project
3. [Your First Convoy](first-convoy.md) — Assign work and watch agents deliver

## The 30-Second Version

```bash
brew install gastown          # Install
gt install ~/gt --git         # Create workspace
cd ~/gt
gt rig add myapp git@github.com:you/app.git   # Add project
gt mayor attach               # Start the Mayor
# Tell the Mayor what to build
```


========================================================================
# Installation
# URL: /docs/getting-started/installation
========================================================================

# Installation

## Install Gas Town

<Tabs>
<TabItem value="homebrew-recommended-" label="Homebrew (Recommended)">

```bash
brew install gastown
```

</TabItem>
<TabItem value="npm" label="npm">

```bash
npm install -g @gastown/gt
```

</TabItem>
<TabItem value="build-from-source" label="Build from Source">

```bash
git clone https://github.com/steveyegge/gastown.git
cd gastown
go build -o gt ./cmd/gt/
sudo mv gt /usr/local/bin/
```

</TabItem>
</Tabs>

## Install Dependencies

### Git (Required)

Gas Town relies on git worktrees for file-level isolation between agents. Git 2.25+ is required.

```bash
git --version
# Should show 2.25.0 or higher
```

If your version is older, update via your package manager (`brew upgrade git`, `sudo apt install git`, etc.).

### Beads (Issue Tracking)

Beads is the git-backed issue tracking system that Gas Town uses for work management.

```bash
# Install beads
brew install beads
# Or: npm install -g @beads/bd
```

Verify installation:

```bash
bd --version
# Should show 0.44.0 or higher
```

### Tmux (Recommended)

Tmux enables Gas Town to manage multiple agent sessions in split panes.

```bash
# macOS
brew install tmux

# Ubuntu/Debian
sudo apt install tmux

# Amazon Linux
sudo yum install tmux
```

### Claude Code CLI

Gas Town builds on Claude Code as its primary runtime. Install it following the [official instructions](https://code.claude.com/docs/en/getting-started).

```bash
# Verify Claude Code is available
claude --version
```

## Shell Completions

Install tab completions for better CLI experience:

```bash
# Bash
gt completion bash > /etc/bash_completion.d/gt

# Zsh
gt completion zsh > "${fpath[1]}/_gt"

# Fish
gt completion fish > ~/.config/fish/completions/gt.fish
```

## Verify Installation

```bash
gt --version
gt help
```

You should see the Gas Town help output listing all available commands.

## Supported Runtimes

Gas Town supports multiple AI coding agent runtimes:

| Runtime | Command | Notes |
|---------|---------|-------|
| Claude Code | `claude` | Default, recommended |
| Gemini CLI | `gemini` | Google's coding agent |
| Codex | `codex` | OpenAI's coding agent |
| Cursor | `cursor` | Cursor IDE agent |
| Auggie | `auggie` | Augment agent |
| Amp | `amp` | Sourcegraph agent |

Configure the default runtime:

```bash
gt config default-agent claude
gt config agent set gemini "gemini"
```

## See Also

- **[Quickstart](quickstart.md)** -- Get up and running with your first Gas Town session
- **[Multi-Runtime Support](../guides/multi-runtime.md)** -- Detailed guide for configuring multiple AI runtimes


========================================================================
# Quick Start
# URL: /docs/getting-started/quickstart
========================================================================

# Quick Start

## Create Your Workspace

The **Town** is your top-level workspace directory. All projects (rigs) live inside it.

```bash
gt install ~/gt --git
cd ~/gt
```

This creates:

```
~/gt/
├── .beads/          # Town-level issue tracking
├── .claude/         # Claude Code integration
├── mayor/           # Mayor agent context
│   └── town.json    # Town metadata
├── deacon/          # Deacon agent context
├── settings/        # Configuration files
├── scripts/         # Utility scripts
├── plugins/         # Town-level plugins
└── CLAUDE.md        # Project context
```

## Add a Project (Rig)

Each project you manage with Gas Town is called a **Rig**.

```bash
gt rig add myproject https://github.com/you/repo.git
```

This creates the rig structure:

```
~/gt/myproject/
├── .beads/          # Rig-level issue tracking
├── config.json      # Rig configuration
├── refinery/rig/    # Canonical clone (merge queue)
├── mayor/rig/       # Mayor's working copy
├── crew/            # Human developer workspaces
├── witness/         # Health monitor
├── polecats/        # Worker directories
└── plugins/         # Rig-level plugins
```

## Create a Crew Workspace

Crew workspaces are persistent clones for human developers.

```bash
gt crew add myproject yourname
```

Enter your workspace:

```bash
cd ~/gt/myproject/crew/yourname
```

## Start Gas Town

Start the core agents:

```bash
# Start Mayor + Deacon
gt start

# Or start everything including Witnesses and Refineries
gt start --all
```

## Attach to the Mayor

The Mayor is your primary interface for coordinating work.

```bash
gt mayor attach
```

Now you can give natural language instructions. For example:

> "Fix the 5 failing tests in the auth module and add input validation to the user registration endpoint."

The Mayor will:

1. Create beads (issues) for each task
2. Bundle them into a convoy
3. Spawn polecats to work on each task
4. Monitor progress
5. Route completed work through the refinery for merging

## Check Status

```bash
# List all rigs
gt rig list

# Check convoy progress
gt convoy list

# View activity feed
gt feed

# Check what's ready for work
gt ready
```

## See Also

- **[Your First Convoy](first-convoy.md)** -- Detailed walkthrough of the convoy workflow
- **[CLI Reference](../cli-reference/index.md)** -- Full command documentation
- **[Architecture](../architecture/index.md)** -- Understand the system design
- **[Usage Guide](../guides/usage-guide.md)** -- Comprehensive usage patterns and tips


========================================================================
# Your First Convoy
# URL: /docs/getting-started/first-convoy
========================================================================

# Your First Convoy

A **Convoy** is Gas Town's primary unit for tracking batches of related work. This walkthrough takes you through creating and monitoring your first convoy.

## Step 1: Create Issues

First, create some beads (issues) to track:

```bash
bd create --title "Fix login bug" --type bug --priority high
# Created: gt-a1b2c

bd create --title "Add email validation" --type feature
# Created: gt-d3e4f

bd create --title "Update README" --type task
# Created: gt-g5h6i
```

## Step 2: Create a Convoy

Bundle the issues into a convoy:

```bash
gt convoy create "Auth System Fixes" gt-a1b2c gt-d3e4f gt-g5h6i
# Created: hq-cv-xyz
```

## Step 3: Assign Work

Use `gt sling` to assign issues to workers:

```bash
# Assign to a rig (auto-spawns a polecat)
gt sling gt-a1b2c myproject
gt sling gt-d3e4f myproject
gt sling gt-g5h6i myproject
```

Each `gt sling` command:

1. Hooks the bead to the target agent
2. Spawns a polecat worker in the rig
3. The polecat picks up the work immediately

## Step 4: Monitor Progress

```bash
# Check convoy status
gt convoy list
gt convoy show hq-cv-xyz

# Watch the activity feed
gt feed

# Check individual polecat status
gt polecat list
```

## Step 5: Watch the Merge Queue

As polecats complete work, they submit merge requests to the Refinery:

```bash
# View merge queue
gt mq list

# Check merge status
gt mq status
```

The Refinery:

1. Picks up the next MR
2. Rebases onto latest main
3. Runs validation (tests, builds)
4. Merges if clean
5. If conflict: spawns a fresh polecat to resolve

## Step 6: Convoy Completion

When all tracked issues are done, the convoy auto-closes:

```bash
gt convoy list
# hq-cv-xyz  Auth System Fixes  [COMPLETED]  3/3 done
```

## Using the Mayor Instead

For a more automated experience, attach to the Mayor and describe the work:

```bash
gt mayor attach
```

Then tell the Mayor:

> "Fix the login bug, add email validation to registration, and update the README with the new auth flow."

The Mayor handles convoy creation, issue tracking, and agent assignment automatically.

## Tips

- Use `gt convoy show` frequently to track progress
- If a polecat stalls, the Witness will detect and handle it
- Use `gt escalate` for issues that need human attention
- Convoys can span multiple rigs for cross-project work

## See Also

- **[Convoys](../concepts/convoys.md)** -- The convoy concept in depth
- **[Mayor Workflow](../workflows/mayor-workflow.md)** -- Full Mayor-driven workflow documentation
- **[Manual Convoy](../workflows/manual-convoy.md)** -- Fine-grained convoy control without the Mayor


========================================================================
# Architecture
# URL: /docs/architecture
========================================================================

# Architecture

Gas Town's architecture draws from Erlang's supervisor trees and mailbox patterns — battle-tested approaches to building reliable distributed systems.

## Sections

- [System Overview](overview.md) — High-level architecture and components
- [Agent Hierarchy](agent-hierarchy.md) — How agents supervise each other
- [Work Distribution](work-distribution.md) — How tasks flow through the system
- [Design Principles](design-principles.md) — Core patterns and philosophy

## At a Glance

```mermaid
graph TB
    subgraph Town ["Town (~/gt/)"]
        Mayor["Mayor<br/>Global Coordinator"]
        Deacon["Deacon<br/>Health Monitor"]
        Daemon["Daemon<br/>Background Scheduler"]
    end

    subgraph Rig1 ["Rig: myproject"]
        W1["Witness"]
        R1["Refinery"]
        P1a["Polecat: Toast"]
        P1b["Polecat: Alpha"]
        C1["Crew: dave"]
    end

    subgraph Rig2 ["Rig: docs"]
        W2["Witness"]
        R2["Refinery"]
        P2a["Polecat: Bravo"]
    end

    Daemon -->|heartbeat 3m| Deacon
    Mayor -->|strategy| Deacon
    Deacon -->|monitors| W1
    Deacon -->|monitors| W2
    W1 -->|watches| P1a
    W1 -->|watches| P1b
    P1a -->|gt done → MR| R1
    P1b -->|gt done → MR| R1
    P2a -->|gt done → MR| R2
    R1 -->|merge| Main1[main]
    R2 -->|merge| Main2[main]
```

Gas Town is a hierarchical supervisor system where each level monitors the level below it, ensuring work progresses reliably even when individual agents crash or stall.


========================================================================
# System Overview
# URL: /docs/architecture/overview
========================================================================

# System Overview

## Components

Gas Town consists of five layers:

### 1. The Town (Workspace)

The town is the root directory (typically `~/gt/`) containing all projects and coordination state. It holds:

- Town-level beads database for cross-project tracking
- Configuration and settings
- Mayor and Deacon agent contexts
- Daemon process state

### 2. Rigs (Project Containers)

Each rig wraps a git repository with the full agent infrastructure:

```
myproject/
├── .beads/           # Rig-level issue tracking (SQLite + JSONL export)
├── metadata.json     # Rig configuration and identity
├── AGENTS.md         # Agent role descriptions
├── refinery/         # Refinery merge queue processing
├── mayor/            # Mayor's coordination workspace
├── crew/             # Human developer workspaces
│   ├── dave/
│   └── emma/
├── witness/          # Rig-level health monitor
├── polecats/         # Ephemeral worker directories (git worktrees)
│   ├── toast/
│   └── alpha/
└── plugins/          # Rig-level plugins
```

### 3. Agents (Workers)

Seven agent roles form the hierarchy:

| Agent | Scope | Lifecycle | Purpose |
|-------|-------|-----------|---------|
| **Mayor** | Town | Persistent | Global coordination and strategy |
| **Deacon** | Town | Persistent | Health monitoring and lifecycle |
| **Witness** | Per-rig | Persistent | Polecat supervision |
| **Refinery** | Per-rig | Persistent | Merge queue processing |
| **Polecats** | Per-rig | Ephemeral | Feature work (self-cleaning) |
| **Crew** | Per-rig | Managed | Human developer workspaces |
| **Dogs** | Town | Reusable | Infrastructure tasks |

Additionally, the **Boot** dog is a special triage agent spawned by the Deacon to assess new work or problems.

### 4. Daemon (Scheduler)

A simple Go process that:

- Sends periodic heartbeats to the Deacon
- Processes lifecycle requests (start/stop agents)
- Restarts sessions when requested
- Polls external services (Discord, etc.)

The daemon is intentionally "dumb" — all intelligence lives in the agents.

### 5. Communication Layer

Agents communicate through:

- **Mail** — Async message passing between agents
- **Nudge** — Synchronous message delivery
- **Escalations** — Priority-routed alerts
- **Hooks** — Persistent work state attachment
- **Beads** — Shared issue tracking state

## Data Flow

```
Human gives Mayor instructions
    → Mayor creates beads + convoy
    → Mayor slings work to rigs
    → Polecats spawn and execute
    → Polecats submit MRs via gt done
    → Refinery processes merge queue
    → Code lands on main branch
    → Witness cleans up polecat sandbox
    → Convoy auto-closes when all done
```

## State Management

All state is persisted in git or the filesystem:

| State | Storage | Survives |
|-------|---------|----------|
| Issues | `.beads/beads.db` (SQLite) | Everything |
| Issue export | `.beads/issues.jsonl` | Everything |
| Work hooks | Git worktrees | Crashes, restarts |
| Mail | Filesystem JSONL | Session restarts |
| Config | `metadata.json`, `.beads/config.yaml` | Everything |
| Agent context | CLAUDE.md files | Everything |
| Activity log | `.events.jsonl` | Everything |

## See Also

- **[Agent Hierarchy](agent-hierarchy.md)** -- How agents are organized and supervised
- **[Work Distribution](work-distribution.md)** -- How work flows through the system
- **[Design Principles](design-principles.md)** -- Core principles governing Gas Town's architecture
- **[Rigs](../concepts/rigs.md)** -- Project containers that form the building blocks of a town


========================================================================
# Agent Hierarchy
# URL: /docs/architecture/agent-hierarchy
========================================================================

# Agent Hierarchy

Gas Town uses a supervisor tree pattern inspired by Erlang/OTP. Each agent level monitors the level below it, providing fault tolerance and automatic recovery.

## Supervision Tree

```mermaid
graph TD
    Daemon["Daemon<br/>(Go process)"]
    Deacon["Deacon<br/>(Town supervisor)"]
    Mayor["Mayor<br/>(Coordinator)"]
    Boot["Boot<br/>(Triage dog)"]

    Daemon -->|heartbeat 3m| Deacon
    Mayor -->|strategic direction| Deacon
    Deacon -->|spawns for triage| Boot

    subgraph "Per-Rig Supervision"
        Witness["Witness<br/>(Rig supervisor)"]
        Refinery["Refinery"]
        P1["Polecat 1"]
        P2["Polecat 2"]
        P3["Polecat 3"]
        Crew["Crew<br/>(Human devs)"]
    end

    Deacon -->|monitors| Witness
    Witness -->|watches| P1
    Witness -->|watches| P2
    Witness -->|watches| P3
    P1 -->|gt done → MR| Refinery
    P2 -->|gt done → MR| Refinery
    Refinery -->|merge| Main[main branch]
```

## Monitoring Chain

| Monitor | Watches | Detects | Action |
|---------|---------|---------|--------|
| Daemon | Deacon | Unresponsive | Restart Deacon session |
| Deacon | All Witnesses | Stuck/dead Witness | Restart Witness |
| Deacon | Boot dog | Triage needed | Spawn Boot for assessment |
| Witness | Polecats in rig | Stalled/crashed | Nudge, then nuke zombie |
| Witness | Refinery | Merge failures | Escalate to Mayor |

## Patrol Cycles

Persistent agents run patrol cycles — periodic health checks:

| Agent | Interval | Actions |
|-------|----------|---------|
| **Deacon** | 5 min | Check Witnesses, process lifecycle requests, run Boot triage |
| **Witness** | 5 min | Check polecats, detect stalls, clean zombies |
| **Refinery** | 5 min | Process merge queue, rebase and validate |
| **Daemon** | 3 min | Send heartbeat to Deacon |

## Boot Dog: The Triage Agent

The Boot dog is a special agent spawned by the Deacon to assess situations that need triage — new work arriving, health check failures, or ambiguous states. Boot performs a quick assessment and reports back to the Deacon, which then takes action (spawn polecats, escalate, etc.). Boot is short-lived and focused: assess, report, exit.

## Escalation Path

When an agent encounters a problem it cannot resolve:

```
Polecat (stuck)
  → Witness detects stall (patrol cycle)
    → Witness nudges polecat
      → If still stuck: Witness escalates to Deacon
        → Deacon escalates to Mayor
          → Mayor escalates to Human/Overseer
```

Agents can also self-escalate using `gt escalate`:

```bash
gt escalate "Brief description" -s HIGH -m "Details"
```

Severity levels control routing:

| Level | Code | Route |
|-------|------|-------|
| Critical | P0 | Bead → Mail:Mayor → Email:Human → SMS:Human |
| High | P1 | Bead → Mail:Mayor → Email:Human |
| Medium | P2 | Bead → Mail:Mayor |
| Low | P3 | Bead only |

## See Also

- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- The periodic health monitoring that agents perform
- **[Escalations](../operations/escalations.md)** -- How problems travel up the hierarchy
- **[Design Principles](design-principles.md)** -- Erlang-inspired supervision and other architectural principles


========================================================================
# Work Distribution
# URL: /docs/architecture/work-distribution
========================================================================

# Work Distribution

## The Work Lifecycle

Every piece of work in Gas Town follows a defined lifecycle from creation to completion.

### Work States

```mermaid
stateDiagram-v2
    [*] --> Open: bd create
    Open --> Hooked: gt sling (assigns to agent)
    Hooked --> InProgress: Agent picks up from hook
    InProgress --> Completed: gt done / bd close
    Completed --> [*]

    InProgress --> Escalated: Blocker hit
    InProgress --> Deferred: Paused
    Escalated --> InProgress: Resolved
    Deferred --> Open: gt release
```

### Exit States (Polecat Completion)

When a polecat finishes work, it exits in one of four states:

| State | Meaning | Next Action |
|-------|---------|-------------|
| `COMPLETED` | Work done, MR submitted | Refinery processes merge |
| `ESCALATED` | Hit blocker, needs human | Escalation routes to Mayor/Overseer |
| `DEFERRED` | Paused, still open | Another agent can pick up later |
| `PHASE_COMPLETE` | Phase done, waiting | Gate opens, next phase begins |

## Work Assignment

### The Sling Command

`gt sling` is the primary command for assigning work:

```bash
# Assign to a rig (auto-spawns polecat)
gt sling gt-abc12 myproject

# Assign to a specific agent
gt sling gt-abc12 myproject --agent cursor

# Assign multiple items
gt sling gt-abc12 gt-def34 myproject
```

What happens:

1. Bead status changes to `hooked`
2. Work attaches to target's hook
3. Polecat spawns in the rig
4. Polecat's startup hook finds the work
5. Polecat begins execution

### Hook Persistence

The **hook** is Gas Town's durability primitive. Work on a hook survives:

- Session restarts
- Context compaction
- Handoffs between sessions
- Agent crashes

```bash
# Check what's on your hook
gt hook

# Manually attach work
gt hook gt-abc12

# Remove from hook
gt unsling gt-abc12
```

## The Propulsion Principle

> "If it's on your hook, YOU RUN IT."

This is Gas Town's core work scheduling rule. When an agent starts a session:

1. Check hook for attached work
2. If work found → execute it immediately
3. If no work → check inbox → wait for instructions

This creates automatic momentum — agents always know what to do.

## Convoy Tracking

Convoys bundle related work for tracking:

```bash
# Create convoy from issues
gt convoy create "Feature X" gt-a1 gt-b2 gt-c3

# Check progress
gt convoy list
gt convoy show hq-cv-001

# Add more issues
gt convoy add hq-cv-001 gt-d4

# Find stranded convoys with ready work
gt convoy stranded
```

Convoys auto-close when all tracked issues complete.

## Molecules: Structured Workflows

Molecules define multi-step workflows for agents. A molecule is an epic bead with child step beads that guide execution:

```bash
# Agents check their current steps
bd ready                    # Shows steps with no blockers

# Work through steps sequentially
bd update <step> --status=in_progress
# ... do the work ...
bd close <step>
bd ready                    # Next step appears
```

The standard polecat workflow molecule (`mol-polecat-work`) includes steps like:

1. Load context and verify assignment
2. Set up working branch
3. Verify tests pass on main
4. Implement the solution
5. Self-review changes
6. Run tests and verify coverage
7. Clean up workspace
8. Prepare work for review
9. Submit work and self-clean

Molecules provide crash recovery — if an agent restarts, `bd ready` shows the next incomplete step, so work resumes from where it left off.

## Cross-Rig Work

For work spanning multiple projects:

- **Dogs** handle infrastructure tasks across rigs
- **Convoys** track issues across multiple rigs
- **Mayor** coordinates cross-rig strategy

```bash
# Dogs handle infrastructure
gt dog list

# Prefix-based routing lets you reference any rig's beads
bd show gt-abc12            # Routes to gastown rig
bd show hq-abc              # Routes to town-level beads
```

## See Also

- **[Convoys](../concepts/convoys.md)** -- Batch tracking for distributed work
- **[Hooks](../concepts/hooks.md)** -- How work is assigned to agents
- **[GUPP](../concepts/gupp.md)** -- The propulsion principle driving work execution
- **[Sling CLI](../cli-reference/sling.md)** -- The command for dispatching work


========================================================================
# Design Principles
# URL: /docs/architecture/design-principles
========================================================================

# Design Principles

Gas Town is built on several core design principles that guide its architecture and behavior.

## 1. The Propulsion Principle

> "If it's on your hook, YOU RUN IT."

Work attached to an agent's hook drives that agent's behavior. The hook is the primary scheduling mechanism — no central scheduler decides what agents do. Each agent is self-propelled by its hook.

## 2. Erlang-Inspired Supervision

Gas Town borrows heavily from Erlang/OTP patterns:

- **Supervisor trees** — Each level monitors the level below
- **Mailboxes** — Agents communicate via async messages
- **Let it crash** — Agents can crash; supervisors handle recovery
- **Process isolation** — Each agent runs in its own session

## 3. Git as Ground Truth

All persistent state lives in git or git-adjacent storage:

- Beads (issues) stored in SQLite with JSONL export for portability
- Hooks implemented as git worktrees
- Agent context in CLAUDE.md files (committed)
- Configuration in tracked JSON/YAML files
- Rig-level `.beads/` is gitignored (local runtime state), but issue data persists via export

This means state survives anything — crashes, restarts, even machine failures.

## 4. Dumb Scheduler, Smart Agents

The daemon is intentionally simple — it just sends heartbeats and processes lifecycle requests. All intelligence lives in the agents themselves:

- The **Mayor** decides strategy
- The **Deacon** decides health actions
- **Witnesses** decide polecat management
- **Polecats** decide how to implement features

## 5. Self-Cleaning Workers

Polecats follow a strict lifecycle:

```
Spawn → Work → Done → Nuke
```

They are never idle. A polecat is either:

- **Working** — Actively executing a task
- **Stalled** — Stuck (Witness will detect)
- **Zombie** — Crashed (Witness will clean up)

When a polecat finishes, it runs `gt done` to submit its MR and exit. The Witness nukes the sandbox.

## 6. Nondeterministic Idempotence

Work can be safely retried:

- If a polecat crashes, another can pick up the same bead
- `gt release` recovers stuck in_progress issues
- The Refinery handles merge conflicts by spawning fresh workers
- Convoys track completion regardless of which agent did the work

## 7. Role Separation

Each agent role has clear, non-overlapping responsibilities:

| Role | Does | Does NOT |
|------|------|----------|
| Mayor | Coordinate strategy | Monitor health |
| Deacon | Monitor health | Assign features |
| Witness | Watch polecats | Process merges |
| Refinery | Merge code | Write features |
| Polecat | Implement features | Monitor others |
| Crew | Human dev work | Agent coordination |
| Dog | Infrastructure tasks | Feature work |
| Boot | Triage assessments | Long-running work |

## 8. Communication Over Shared State

Agents communicate explicitly through:

- **Mail** for async messages
- **Nudge** for sync messages
- **Escalations** for priority alerts
- **Beads** for work state

Rather than reading shared state and inferring what to do.

## 9. Persistent vs Ephemeral

Gas Town distinguishes between:

- **Persistent agents** (Mayor, Deacon, Witness, Refinery) — Long-running, survive restarts
- **Ephemeral agents** (Polecats) — Single-task, self-destructing
- **Reusable agents** (Dogs, Crew) — Multiple tasks, managed lifecycle

This three-tier model optimizes resource usage while maintaining reliability.

## 10. Human in the Loop

The Overseer (human) sits at the top of the escalation chain:

- Can intervene at any level
- Receives escalations for critical issues
- Approves human gates
- Can manually sling, release, or reassign work

Gas Town automates everything it can, but keeps humans in control.

## 11. The Scotty Principle

> "Never walk past a warp core leak."

Named after the Star Trek engineer: Gas Town agents never proceed past failures. The [Refinery](../agents/refinery.md) does not merge code that fails validation. Polecats run [preflight tests](../agents/polecats.md) before starting implementation to ensure `main` is clean. If something is broken, you fix it or file it -- you don't skip past it.

This principle prevents failure cascading through the system. One broken test, left unaddressed, can waste dozens of polecat hours.

## 12. Discovery Over Tracking

Gas Town favors agents discovering what needs to happen over centralized tracking that tells them. The [Witness](../agents/witness.md) discovers stale polecats by inspecting them, not by reading a checklist. The [Deacon](../agents/deacon.md) discovers zombies by scanning processes, not by maintaining a process table.

This makes the system resilient to state corruption: even if tracking data is lost, agents can recover by rediscovering the current state. See [Patrol Cycles](../concepts/patrol-cycles.md) for a deep dive on how this principle is implemented.

## See Also

- **[GUPP & NDI](../concepts/gupp.md)** -- Deep dive on the two foundational design principles
- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- Discovery over Tracking in practice
- **[Agent Hierarchy](agent-hierarchy.md)** -- Erlang-inspired supervision trees in practice


========================================================================
# CLI Reference
# URL: /docs/cli-reference
========================================================================

# CLI Reference

The `gt` CLI is the primary interface for interacting with Gas Town. It manages workspaces, agents, work distribution, communication, and diagnostics across your entire multi-agent development environment.

## Usage

```bash
gt <command> [subcommand] [options] [arguments]
```

Global flags available on all commands:

| Flag | Description |
|------|-------------|
| `--help`, `-h` | Show help for any command |
| `--version`, `-v` | Print the Gas Town version |

Many subcommands also support `--json`, `--verbose`, `--quiet`, and `--rig <name>` flags. See individual command documentation for details.

## Command Categories

| Category | Description | Key Commands |
|----------|-------------|--------------|
| [Workspace Management](workspace.md) | Install, initialize, manage services, and configure your Gas Town workspace | `gt install`, `gt start`, `gt up`, `gt down`, `gt shutdown`, `gt status` |
| [Agent Operations](agents.md) | Start, stop, and manage the agent hierarchy | `gt mayor`, `gt deacon`, `gt witness`, `gt polecat`, `gt crew` |
| [Work Management](work.md) | Create, assign, track, and complete work items | `gt sling`, `gt hook`, `gt done`, `gt commit`, `bd create` |
| [Convoy & Tracking](convoys.md) | Bundle and track batches of related work | `gt convoy create`, `gt convoy status`, `gt synthesis` |
| [Communication](communication.md) | Send and receive messages between agents and humans | `gt mail`, `gt nudge`, `gt broadcast`, `gt escalate` |
| [Merge Queue](merge-queue.md) | Manage the refinery merge pipeline | `gt mq list`, `gt mq submit`, `gt mq status` |
| [Rig Management](rigs.md) | Add, configure, and manage project containers | `gt rig add`, `gt rig start`, `gt rig config` |
| [Session & Handoff](sessions.md) | Manage agent sessions, handoffs, and molecules | `gt handoff`, `gt resume`, `gt prime`, `gt mol`, `gt cycle` |
| [Diagnostics](diagnostics.md) | Monitor, audit, and troubleshoot the system | `gt activity`, `gt doctor`, `gt dashboard`, `gt status` |
| [Configuration](configuration.md) | Configure agents, accounts, themes, hooks, and plugins | `gt config`, `gt account`, `gt theme`, `gt hooks`, `gt plugin` |
| [Formula](formula.md) | Manage workflow formula templates | `gt formula list`, `gt formula run`, `gt formula show` |
| [Dolt](dolt.md) | Manage the Dolt SQL server for beads storage | `gt dolt start`, `gt dolt status`, `gt dolt sql` |
| [Warrant](warrant.md) | File and execute death warrants for stuck agents | `gt warrant file`, `gt warrant list`, `gt warrant execute` |
| [Patrol](patrol.md) | Aggregate patrol cycle digests | `gt patrol digest` |
| [KRC](krc.md) | TTL-based lifecycle for ephemeral data | `gt krc stats`, `gt krc prune`, `gt krc config` |

## Command Deep Dives

Comprehensive reference for frequently used commands with all flags, subcommands, and examples:

| Command | Description |
|---------|-------------|
| [`gt sling`](sling.md) | Assign work to agents and rigs, with formula support and batch dispatch |
| [`gt refinery`](refinery-commands.md) | Manage the per-rig merge queue processor |
| [`gt polecat`](polecat-commands.md) | Manage polecat lifecycle: list, nuke, stale detection, identities |
| [`gt nudge`](nudge.md) | Send synchronous messages to running agent sessions |
| [`gt session`](session-commands.md) | Manage tmux sessions: start, stop, attach, capture output |
| [`gt formula`](formula.md) | Manage workflow formulas — reusable molecule templates |
| [`gt dolt`](dolt.md) | Manage Dolt SQL server for multi-client beads access |
| [`gt warrant`](warrant.md) | Death warrant lifecycle for zombie agent cleanup |
| [`gt patrol`](patrol.md) | Patrol cycle digest aggregation |
| [`gt krc`](krc.md) | Key Record Chronicle — ephemeral data TTL management |

## Related Tools

Gas Town integrates with the **Beads** issue tracker (`bd` CLI). Beads commands are documented in the [Work Management](work.md) section alongside `gt` work commands.

## Quick Examples

```bash
# Set up a new workspace
gt install ~/gt --git

# Add a project and start working
gt rig add myapp https://github.com/you/app.git
gt start

# Check system status
gt status
gt rig list
gt feed

# Diagnose issues
gt doctor
gt dashboard
```

:::tip[Shell Completions]

Install tab completions for a better CLI experience. See [Workspace Management](workspace.md#gt-completion) for setup instructions.

:::

:::note[Context-Aware Commands]

Many `gt` commands auto-detect the current rig based on your working directory. Use `--rig <name>` to override this when needed.


:::


========================================================================
# Workspace Management
# URL: /docs/cli-reference/workspace
========================================================================

# Workspace Management

Commands for installing, initializing, and managing your Gas Town workspace (the "Town"). These commands handle the foundational setup that all other operations depend on.

---

### `gt install`

Create a new Gas Town workspace.

```bash
gt install <directory> [options]
```

**Description:** Initializes a new town directory with all required structure including `.beads/`, `mayor/`, `deacon/`, `settings/`, and configuration files. This is typically the first command you run.

**Options:**

| Flag | Description |
|------|-------------|
| `--git` | Initialize a git repository in the workspace |
| `--force` | Overwrite an existing workspace |
| `--agent <runtime>` | Set default agent runtime (default: `claude`) |
| `--no-daemon` | Skip daemon setup |

**Example:**

```bash
# Standard installation with git
gt install ~/gt --git

# Install with Gemini as default agent
gt install ~/gt --git --agent gemini
```

**Created structure:**

```
~/gt/
├── .beads/          # Town-level issue tracking
├── .claude/         # Claude Code integration
├── mayor/           # Mayor agent context
├── deacon/          # Deacon agent context
├── settings/        # Configuration files
├── scripts/         # Utility scripts
├── plugins/         # Town-level plugins
├── CLAUDE.md        # Project context file
└── .events.jsonl    # Activity log
```

:::warning

Running `gt install` on an existing workspace without `--force` will abort to prevent accidental data loss.

:::

---

### `gt init`

Initialize Gas Town in an existing directory.

```bash
gt init [options]
```

**Description:** Sets up Gas Town structure in the current directory without creating a new directory. Useful for adding Gas Town to an existing project layout.

**Options:**

| Flag | Description |
|------|-------------|
| `--git` | Initialize a git repository |
| `--force` | Overwrite existing Gas Town configuration |
| `--minimal` | Create only essential directories |

**Example:**

```bash
cd ~/my-workspace
gt init --git
```

---

### `gt uninstall`

Remove Gas Town from a workspace.

```bash
gt uninstall [directory] [options]
```

**Description:** Removes Gas Town configuration and infrastructure from a workspace. Does not remove your project source code or git repositories by default.

**Options:**

| Flag | Description |
|------|-------------|
| `--all` | Remove everything including rig source directories |
| `--keep-beads` | Preserve the beads database |
| `--force` | Skip confirmation prompts |
| `--dry-run` | Show what would be removed without removing it |

**Example:**

```bash
# Remove Gas Town but keep project files
gt uninstall ~/gt

# Full removal
gt uninstall ~/gt --all --force
```

:::danger

Using `--all` permanently deletes all rig data, worktrees, and agent state. This cannot be undone.

:::

---

### `gt git-init`

Initialize or repair git configuration for a Gas Town workspace.

```bash
gt git-init [options]
```

**Description:** Sets up git tracking for the town workspace, including `.gitignore` rules, `.gitattributes`, and initial commit structure. Also useful for repairing corrupted git state.

**Options:**

| Flag | Description |
|------|-------------|
| `--repair` | Repair existing git configuration |
| `--force` | Overwrite existing git setup |

**Example:**

```bash
# Initialize git in an existing town
gt git-init

# Repair corrupted git state
gt git-init --repair
```

---

### `gt enable`

Enable Gas Town for all agentic coding tools.

```bash
gt enable
```

**Description:** Activates Gas Town system-wide. When enabled, shell hooks set `GT_TOWN_ROOT` and `GT_RIG` environment variables, Claude Code `SessionStart` hooks run `gt prime` for context, and git repos are auto-registered as rigs.

Use environment variables for per-session overrides: `GASTOWN_DISABLED=1` to disable, `GASTOWN_ENABLED=1` to enable.

**Example:**

```bash
gt enable
```

---

### `gt disable`

Disable Gas Town for all agentic coding tools.

```bash
gt disable [options]
```

**Description:** Disables Gas Town system-wide. When disabled, shell hooks become no-ops, Claude Code `SessionStart` hooks skip `gt prime`, and tools work vanilla without Gas Town behavior. The workspace (`~/gt`) is preserved.

**Options:**

| Flag | Description |
|------|-------------|
| `--clean` | Also remove shell integration from `~/.zshrc`/`~/.bashrc` |

**Example:**

```bash
# Disable Gas Town
gt disable

# Disable and remove shell integration
gt disable --clean
```

---

### `gt stale`

Check if the `gt` binary needs rebuilding.

```bash
gt stale [options]
```

**Description:** Compares the commit hash embedded in the binary at build time with the current HEAD of the gastown repository. Reports whether the binary is out of date.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |
| `--quiet`, `-q` | Exit code only (0=stale, 1=fresh) |

**Exit codes:**

| Code | Meaning |
|------|---------|
| `0` | Binary is stale (needs rebuild) |
| `1` | Binary is fresh (up to date) |
| `2` | Error (could not determine staleness) |

**Example:**

```bash
# Check if binary is stale
gt stale

# Machine-readable output
gt stale --json

# Script-friendly check
gt stale --quiet && echo "Rebuild needed"
```

---

### `gt info`

Display Gas Town version and release notes.

```bash
gt info [options]
```

**Description:** Shows the current Gas Town version and optionally displays agent-relevant changes from recent versions.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--whats-new` | Show agent-relevant changes from recent versions |

**Example:**

```bash
# Show version info
gt info

# Show what's new
gt info --whats-new

# Machine-readable output
gt info --whats-new --json
```

---

### `gt help`

Display help information for any command.

```bash
gt help [command] [subcommand]
```

**Description:** Shows usage, options, and examples for any `gt` command. When called without arguments, displays the top-level help with all available commands.

**Example:**

```bash
# Top-level help
gt help

# Help for a specific command
gt help sling

# Help for a subcommand
gt help convoy create

# Alternative syntax
gt convoy create --help
```

---

### `gt completion`

Generate shell completion scripts.

```bash
gt completion <shell>
```

**Description:** Generates tab-completion scripts for bash, zsh, fish, or PowerShell. These scripts enable tab completion for all `gt` commands, subcommands, and flags.

**Supported shells:** `bash`, `zsh`, `fish`, `powershell`

**Example:**

```bash
# Bash
gt completion bash > /etc/bash_completion.d/gt

# Zsh
gt completion zsh > "${fpath[1]}/_gt"

# Fish
gt completion fish > ~/.config/fish/completions/gt.fish

# PowerShell
gt completion powershell > gt.ps1
```

:::tip

After installing completions, restart your shell or source the completion file for immediate effect.

:::

---

### `gt shell`

Launch an interactive Gas Town shell.

```bash
gt shell [options]
```

**Description:** Opens an interactive shell session with Gas Town context pre-loaded. Provides enhanced tab completion, prompt integration showing current rig and agent status, and shorthand command aliases.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Start in the context of a specific rig |
| `--role <agent>` | Set the shell role identity |

**Example:**

```bash
# Launch Gas Town shell
gt shell

# Launch in context of a specific rig
gt shell --rig myproject
```

:::note

The Gas Town shell sets the `GT_ROLE` environment variable and configures the prompt to show your current context.


:::

---

### `gt version`

Print Gas Town version information.

```bash
gt version
```

**Example:**

```bash
gt version
```

---

### `gt status`

Display overall town status.

```bash
gt status [options]
```

**Description:** Shows town name, registered rigs, active polecats, and agent status at a glance.

**Options:**

| Flag | Description |
|------|-------------|
| `--fast` | Skip mail lookups for faster execution |
| `--json` | Output as JSON |
| `--verbose`, `-v` | Show detailed multi-line output per agent |
| `--watch`, `-w` | Watch mode: refresh status continuously |
| `--interval`, `-n` | Refresh interval in seconds (default: `2`) |

**Example:**

```bash
# Quick status
gt status

# Watch mode
gt status --watch

# Fast mode (skip mail)
gt status --fast
```

---

### `gt whoami`

Show the identity used for mail commands.

```bash
gt whoami
```

**Description:** Displays the current identity determined by the `GT_ROLE` environment variable. If `GT_ROLE` is set, you are an agent. If not, you are the overseer (human).

**Example:**

```bash
gt whoami
```

---

## Services

Commands for managing Gas Town background services and infrastructure.

### `gt start`

Start Gas Town by launching the Deacon and Mayor.

```bash
gt start [path] [options]
```

**Description:** Launches the Deacon (health-check orchestrator) and Mayor (global coordinator). Other agents (Witnesses, Refineries) are started lazily as needed. If a path like `rig/crew/name` is provided, starts that crew workspace instead.

**Options:**

| Flag | Description |
|------|-------------|
| `--agent <runtime>` | Agent alias to run Mayor/Deacon with |
| `--all`, `-a` | Also start Witnesses and Refineries for all rigs |

**Example:**

```bash
# Start Gas Town
gt start

# Start with all rig agents
gt start --all

# Start a crew workspace
gt start myproject/crew/dave
```

---

### `gt up`

Bring up all Gas Town long-lived services.

```bash
gt up [options]
```

**Description:** Idempotent boot command that ensures all infrastructure agents are running: Daemon, Deacon, Mayor, Witnesses, and Refineries. Polecats are NOT started (they are transient workers spawned on demand). Running `gt up` multiple times is safe.

**Options:**

| Flag | Description |
|------|-------------|
| `--quiet`, `-q` | Only show errors |
| `--restore` | Also restore crew (from settings) and polecats (from hooks) |

**Example:**

```bash
gt up
gt up --restore
```

---

### `gt down`

Stop Gas Town services (reversible pause).

```bash
gt down [options]
```

**Description:** Stops infrastructure agents (Refineries, Witnesses, Mayor, Boot, Deacon, Daemon). This is a "pause" operation -- use `gt start` to bring everything back up. For permanent cleanup, use `gt shutdown` instead.

**Options:**

| Flag | Description |
|------|-------------|
| `--polecats`, `-p` | Also stop all polecat sessions |
| `--all`, `-a` | Also stop bd daemons/activity and verify shutdown |
| `--nuke` | Kill entire tmux server (DESTRUCTIVE -- kills non-GT sessions) |
| `--force`, `-f` | Force kill without graceful shutdown |
| `--quiet`, `-q` | Only show errors |
| `--dry-run` | Preview what would be stopped |

**Example:**

```bash
# Pause infrastructure
gt down

# Stop everything including polecats
gt down --polecats

# Preview what would stop
gt down --dry-run
```

:::warning

`gt down --nuke` kills the entire tmux server, including non-Gas Town sessions. Use with care.

:::

---

### `gt shutdown`

Shut down Gas Town with full cleanup.

```bash
gt shutdown [options]
```

**Description:** The "done for the day" command. Stops all agents, cleans up polecat worktrees and branches, and puts the town in a fully stopped state. Polecats with uncommitted work are protected (skipped) unless `--nuclear` is used.

**Options:**

| Flag | Description |
|------|-------------|
| `--all`, `-a` | Also stop crew sessions |
| `--polecats-only` | Only stop polecats (minimal shutdown) |
| `--force`, `-f` | Skip confirmation prompt |
| `--yes`, `-y` | Skip confirmation prompt |
| `--graceful`, `-g` | Allow agents time to save state before killing |
| `--wait`, `-w` | Seconds to wait for graceful shutdown (default: `30`) |
| `--nuclear` | Force cleanup even if polecats have uncommitted work (DANGER) |
| `--cleanup-orphans` | Clean up orphaned Claude processes |
| `--cleanup-orphans-grace-secs` | Grace period between SIGTERM and SIGKILL (default: `60`) |

**Comparison with `gt down`:**

| | `gt down` | `gt shutdown` |
|---|-----------|--------------|
| Stops agents | Yes | Yes |
| Removes worktrees | No | Yes |
| Reversible | Yes (`gt start`) | Permanent cleanup |

**Example:**

```bash
# Standard shutdown
gt shutdown

# Graceful shutdown with drain
gt shutdown --graceful

# Shutdown including crew
gt shutdown --all --force
```

:::danger

`gt shutdown --nuclear` forces cleanup even when polecats have uncommitted work. This may result in lost work.

:::

---

### `gt daemon`

Manage the Gas Town background daemon.

```bash
gt daemon <subcommand>
```

**Description:** The daemon is a Go background process that pokes agents periodically (heartbeat), processes lifecycle requests, and restarts sessions when agents request cycling. It is a "dumb scheduler" -- all intelligence is in the agents.

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt daemon start` | Start the daemon |
| `gt daemon stop` | Stop the daemon |
| `gt daemon status` | Show daemon status |
| `gt daemon logs` | View daemon logs |

**Example:**

```bash
gt daemon status
gt daemon logs
```

---

### `gt dolt`

Manage the Dolt SQL server for beads.

```bash
gt dolt <subcommand>
```

**Description:** The Dolt server provides multi-client access to all rig databases, avoiding the single-writer limitation of embedded Dolt mode. Runs on port 3307 (to avoid conflicts with MySQL on 3306).

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt dolt start` | Start the Dolt server |
| `gt dolt stop` | Stop the Dolt server |
| `gt dolt status` | Show Dolt server status |
| `gt dolt logs` | View Dolt server logs |
| `gt dolt sql` | Open Dolt SQL shell |
| `gt dolt list` | List available rig databases |
| `gt dolt init-rig` | Initialize a new rig database |
| `gt dolt migrate` | Migrate existing databases to centralized data directory |

**Example:**

```bash
gt dolt status
gt dolt sql
gt dolt list
```

---

### `gt namepool`

Manage themed name pools for polecats.

```bash
gt namepool [subcommand] [options]
```

**Description:** Polecats get themed names from a pool (default: Mad Max universe, e.g., furiosa, nux, slit). You can change the theme or add custom names.

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt namepool themes` | List available themes and their names |
| `gt namepool set <theme>` | Set the namepool theme for this rig |
| `gt namepool add <name>` | Add a custom name to the pool |
| `gt namepool reset` | Reset the pool state (release all names) |

**Options:**

| Flag | Description |
|------|-------------|
| `--list`, `-l` | List available themes |

**Example:**

```bash
# Show current pool status
gt namepool

# List available themes
gt namepool themes

# Set theme to minerals
gt namepool set minerals

# Add a custom name
gt namepool add ember

# Reset pool state
gt namepool reset
```

---

### `gt worktree`

Create a git worktree in another rig for cross-rig work.

```bash
gt worktree <rig> [options]
```

**Description:** For crew workers who need to work on another rig's codebase while maintaining their identity. Creates a worktree in the target rig's `crew/` directory named after your source rig and identity.

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt worktree list` | List all cross-rig worktrees owned by current crew member |
| `gt worktree remove` | Remove a cross-rig worktree |

**Options:**

| Flag | Description |
|------|-------------|
| `--no-cd` | Just print the path (do not print cd command) |

**Example:**

```bash
# Create worktree in beads rig
gt worktree beads

# List your cross-rig worktrees
gt worktree list

# Remove a worktree
gt worktree remove beads
```

## See Also

- [Architecture Overview](../architecture/overview.md) — How the workspace fits into Gas Town's architecture
- [Rig Management](rigs.md) — Managing individual project containers within the workspace
- [Configuration](configuration.md) — Town-level settings and agent configuration
- [gt dolt](dolt.md) — Dolt SQL server that backs the beads database


========================================================================
# Agent Operations
# URL: /docs/cli-reference/agents
========================================================================

# Agent Operations

Commands for starting, stopping, monitoring, and managing the Gas Town agent hierarchy. Each agent role has dedicated lifecycle commands, plus there are cross-cutting commands for role management.

---

## General Agent Commands

### `gt agents`

List all agents and their current status.

```bash
gt agents [options]
```

**Description:** Displays all agents across the town, organized by role. Shows running status, current activity, and resource usage.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Filter to agents in a specific rig |
| `--role <role>` | Filter to a specific role (mayor, deacon, witness, etc.) |
| `--running` | Show only running agents |
| `--json` | Output in JSON format |

**Example:**

```bash
# List all agents
gt agents

# Show only running agents
gt agents --running

# Show agents for a specific rig
gt agents --rig myproject
```

**Sample output:**

```
ROLE        RIG          STATUS     PID    AGE
mayor       (town)       running    1234   2h
deacon      (town)       running    1235   2h
witness     myproject    running    1240   1h
witness     docs         running    1241   1h
refinery    myproject    running    1250   1h
polecat     myproject    running    1260   15m   [toast] gt-abc12
polecat     myproject    running    1261   10m   [alpha] gt-def34
dog         (town)       idle       -      -     [boot]
```

---

### `gt role`

Display or set the current agent role context.

```bash
gt role [role-name]
```

**Description:** Without arguments, displays the current role set by `GT_ROLE`. With an argument, sets the role for the current session. The role determines which identity and capabilities the current agent session operates under.

**Valid roles:** `mayor`, `deacon`, `witness`, `refinery`, `polecat`, `dog`, `crew`, `overseer`

**Example:**

```bash
# Show current role
gt role

# Set role
gt role witness
```

:::warning

Changing roles mid-session can cause unexpected behavior. This is primarily used during `gt prime` initialization.

:::

---

## Mayor

The Mayor is the top-level coordinator for the entire town. It receives instructions from the human overseer, creates work plans, and delegates to other agents.

### `gt mayor start`

Start the Mayor agent.

```bash
gt mayor start [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--attach` | Start and immediately attach to the session |
| `--agent <runtime>` | Agent runtime to use (default: configured default) |
| `--resume` | Resume from a previous session checkpoint |

**Example:**

```bash
gt mayor start
gt mayor start --attach
gt mayor start --agent claude
```

---

### `gt mayor stop`

Stop the Mayor agent.

```bash
gt mayor stop [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Force stop without graceful shutdown |
| `--checkpoint` | Save a checkpoint before stopping |

**Example:**

```bash
gt mayor stop
gt mayor stop --checkpoint
```

---

### `gt mayor status`

Show Mayor status and current activity.

```bash
gt mayor status [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--verbose` | Show extended status including mail queue and hook |

**Example:**

```bash
gt mayor status
```

**Sample output:**

```
Mayor: running (PID 1234)
Session: sess-abc123
Uptime: 2h 15m
Hook: empty
Inbox: 3 unread
Active convoys: 2
```

---

## Deacon

The Deacon is the health monitoring supervisor for the town. It runs patrol cycles, monitors all Witnesses, and handles lifecycle requests.

### `gt deacon start`

Start the Deacon agent.

```bash
gt deacon start [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--attach` | Start and attach to the session |
| `--agent <runtime>` | Agent runtime to use |

**Example:**

```bash
gt deacon start
```

---

### `gt deacon stop`

Stop the Deacon agent.

```bash
gt deacon stop [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Force stop without graceful shutdown |

---

### `gt deacon status`

Show Deacon status.

```bash
gt deacon status [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt deacon status
```

---

## Witness

Witnesses are per-rig supervisors that monitor polecats, detect stalls, and manage worker lifecycle within a single rig.

### `gt witness start`

Start a Witness agent for a rig.

```bash
gt witness start <rig> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--attach` | Start and attach to the session |
| `--agent <runtime>` | Agent runtime to use |

**Example:**

```bash
gt witness start myproject
```

---

### `gt witness stop`

Stop a Witness agent.

```bash
gt witness stop <rig> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Force stop without graceful shutdown |

**Example:**

```bash
gt witness stop myproject
```

---

### `gt witness status`

Show Witness status for a rig.

```bash
gt witness status [rig] [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--all` | Show all Witnesses across all rigs |
| `--json` | Output in JSON format |

**Example:**

```bash
gt witness status myproject
gt witness status --all
```

---

## Refinery

The Refinery processes the merge queue for a rig, rebasing, validating, and merging pull requests onto the main branch.

### `gt refinery start`

Start the Refinery agent for a rig.

```bash
gt refinery start <rig> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--attach` | Start and attach to the session |
| `--agent <runtime>` | Agent runtime to use |

**Example:**

```bash
gt refinery start myproject
```

---

### `gt refinery stop`

Stop the Refinery agent.

```bash
gt refinery stop <rig> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Force stop without graceful shutdown |

---

### `gt refinery status`

Show Refinery status for a rig.

```bash
gt refinery status [rig] [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--all` | Show all Refineries across all rigs |
| `--json` | Output in JSON format |

---

## Polecats

Polecats are ephemeral worker agents. They spawn, execute a single task, submit their work, and exit. Managed by the Witness.

### `gt polecat list`

List all polecats.

```bash
gt polecat list [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Filter to a specific rig |
| `--status <state>` | Filter by status: `running`, `stalled`, `zombie`, `completed` |
| `--json` | Output in JSON format |

**Example:**

```bash
# List all polecats
gt polecat list

# List running polecats in a rig
gt polecat list --rig myproject --status running
```

**Sample output:**

```
NAME     RIG          STATUS    BEAD       AGE     BRANCH
toast    myproject    running   gt-abc12   15m     fix/login-bug
alpha    myproject    running   gt-def34   10m     feat/email-validation
bravo    docs         running   gt-ghi56   5m      docs/update-readme
```

---

### `gt polecat status`

Show detailed status of a specific polecat.

```bash
gt polecat status <name> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt polecat status toast
```

---

### `gt polecat nuke`

Destroy a polecat and clean up its resources.

```bash
gt polecat nuke <name> [options]
```

**Description:** Terminates the polecat process, removes its worktree, and cleans up all associated state. Used for zombie polecats or when a task needs to be reassigned.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Skip confirmation |
| `--keep-branch` | Preserve the git branch |

**Example:**

```bash
gt polecat nuke toast
gt polecat nuke toast --force
```

:::warning

Nuking a polecat destroys all uncommitted work in its worktree. Ensure the polecat has committed or pushed its changes before nuking.

:::

---

### `gt polecat gc`

Garbage collect finished polecat directories.

```bash
gt polecat gc [options]
```

**Description:** Cleans up directories and branches from polecats that have completed their work or have been abandoned.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Garbage collect for a specific rig |
| `--all` | Garbage collect across all rigs |
| `--dry-run` | Show what would be cleaned without doing it |
| `--age <duration>` | Only clean up polecats older than this (default: `1h`) |

**Example:**

```bash
gt polecat gc --all
gt polecat gc --rig myproject --dry-run
```

---

### `gt polecat stale`

List polecats that appear to be stalled or unresponsive.

```bash
gt polecat stale [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Check a specific rig |
| `--age <duration>` | Stale threshold (default: `30m`) |
| `--json` | Output in JSON format |

**Example:**

```bash
gt polecat stale
gt polecat stale --age 15m
```

---

## Dogs

Dogs are reusable agents that handle infrastructure and cross-rig tasks. They persist between tasks, unlike ephemeral polecats.

### `gt dog list`

List all dogs and their current status.

```bash
gt dog list [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt dog list
```

**Sample output:**

```
NAME     STATUS    CURRENT TASK     SINCE
boot     idle      -                -
fetch    running   sync-upstream    5m
lint     idle      -                -
```

---

### `gt dog status`

Show detailed status of a specific dog.

```bash
gt dog status <name> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt dog status boot
```

---

### `gt dog add`

Register a new dog agent.

```bash
gt dog add <name> [options]
```

**Description:** Creates a new dog with a specific name and optional configuration. Dogs persist in the `deacon/dogs/` directory.

**Options:**

| Flag | Description |
|------|-------------|
| `--agent <runtime>` | Agent runtime for this dog |
| `--role <purpose>` | Dog's specialization (e.g., `triage`, `infrastructure`) |

**Example:**

```bash
gt dog add fetch --role infrastructure
gt dog add lint --agent claude
```

---

## Boot

The Boot agent is a special triage dog that spawns to assess and route incoming work.

### `gt boot spawn`

Spawn the Boot triage agent.

```bash
gt boot spawn [options]
```

**Description:** Starts the Boot dog to perform triage on pending work items, assess complexity, and recommend assignment strategies.

**Options:**

| Flag | Description |
|------|-------------|
| `--attach` | Attach to the Boot session |

**Example:**

```bash
gt boot spawn
```

---

### `gt boot status`

Show Boot agent status.

```bash
gt boot status [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

---

## Callbacks

### `gt callbacks`

Handle callbacks from agents during Deacon patrol.

```bash
gt callbacks <subcommand>
```

**Description:** Processes messages sent to the Mayor from Witnesses, Refineries, polecats, and external triggers. Routes messages to other agents or updates state as needed.

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt callbacks process` | Process pending callbacks |

**Example:**

```bash
gt callbacks process
```

:::note

Callbacks are typically processed automatically during Deacon patrol cycles. Manual invocation is for debugging or manual intervention.

:::

---

## Crew

Crew members are persistent workspaces for human developers. They get their own git clone within a rig and can run agent sessions.

### `gt crew start`

Start an agent session in a crew workspace.

```bash
gt crew start <rig> <member> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--attach` | Start and attach to the session |
| `--agent <runtime>` | Agent runtime to use |

**Example:**

```bash
gt crew start myproject dave --attach
```

---

### `gt crew stop`

Stop a crew agent session.

```bash
gt crew stop <rig> <member> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Force stop |

---

### `gt crew add`

Add a new crew member workspace to a rig.

```bash
gt crew add <rig> <name> [options]
```

**Description:** Creates a new persistent git clone for a human developer within the specified rig.

**Options:**

| Flag | Description |
|------|-------------|
| `--branch <name>` | Check out a specific branch |
| `--agent <runtime>` | Default agent runtime for this crew member |

**Example:**

```bash
gt crew add myproject dave
gt crew add myproject emma --branch develop
```

---

### `gt crew list`

List crew members.

```bash
gt crew list [rig] [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--all` | List crew across all rigs |
| `--json` | Output in JSON format |

**Example:**

```bash
gt crew list myproject
gt crew list --all
```

---

### `gt crew at`

Show what a crew member is currently working on.

```bash
gt crew at <rig> <member>
```

**Example:**

```bash
gt crew at myproject dave
```

---

### `gt crew remove`

Remove a crew member workspace.

```bash
gt crew remove <rig> <name> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Skip confirmation and force removal |
| `--keep-branch` | Preserve the git branch |

**Example:**

```bash
gt crew remove myproject dave
```

---

### `gt crew refresh`

Refresh a crew workspace by pulling latest changes.

```bash
gt crew refresh <rig> <member> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--rebase` | Rebase local changes onto latest main |
| `--all` | Refresh all crew workspaces in the rig |

**Example:**

```bash
gt crew refresh myproject dave --rebase
```

---

### `gt crew restart`

Restart a crew agent session.

```bash
gt crew restart <rig> <member> [options]
```

**Description:** Stops and restarts the agent session for a crew member, preserving hook state and context.

**Options:**

| Flag | Description |
|------|-------------|
| `--agent <runtime>` | Switch to a different agent runtime |

**Example:**

```bash
gt crew restart myproject dave
```

## See Also

- [Agent Hierarchy](../architecture/agent-hierarchy.md) — How agents are organized
- [Lifecycle](../operations/lifecycle.md) — Agent lifecycle management
- [gt polecat](polecat-commands.md) — Polecat-specific commands
- [gt session](session-commands.md) — Session management for agents


========================================================================
# Work Management
# URL: /docs/cli-reference/work
========================================================================

# Work Management

Commands for creating, assigning, tracking, and completing work items. This includes both `gt` commands for work orchestration and `bd` (Beads) commands for issue tracking.

---

## Work Orchestration

### `gt ready`

List work items that are ready for assignment.

```bash
gt ready [options]
```

**Description:** Shows beads in `pending` or `open` status that are not currently assigned to any agent. These are available for slinging to workers.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Filter to a specific rig |
| `--priority <level>` | Filter by priority: `critical`, `high`, `medium`, `low` |
| `--type <type>` | Filter by type: `bug`, `feature`, `task`, `chore` |
| `--convoy <id>` | Show only items in a specific convoy |
| `--json` | Output in JSON format |

**Example:**

```bash
# Show all ready work
gt ready

# Show high-priority bugs ready for work
gt ready --priority high --type bug

# Show ready work for a specific rig
gt ready --rig myproject
```

**Sample output:**

```
ID         PRIORITY   TYPE      TITLE                           RIG
gt-abc12   high       bug       Fix login redirect loop         myproject
gt-def34   medium     feature   Add email validation            myproject
gt-ghi56   low        task      Update API documentation        docs
```

---

### `gt sling`

Assign work to a rig or agent.

```bash
gt sling <bead-id>... <target> [options]
```

**Description:** The primary command for assigning work. Hooks the bead to the target, updates its status, and spawns a polecat to execute the work. This is the central work distribution command in Gas Town.

**Options:**

| Flag | Description |
|------|-------------|
| `--agent <runtime>` | Agent runtime for the spawned polecat |
| `--name <name>` | Name for the spawned polecat |
| `--priority` | Override bead priority for scheduling |
| `--no-spawn` | Hook the work but do not spawn a polecat |

**Example:**

```bash
# Assign a single bead to a rig (auto-spawns polecat)
gt sling gt-abc12 myproject

# Assign multiple beads
gt sling gt-abc12 gt-def34 myproject

# Assign with a specific agent
gt sling gt-abc12 myproject --agent cursor

# Hook work without spawning (manual pickup later)
gt sling gt-abc12 myproject --no-spawn
```

**What happens:**

1. Bead status changes to `hooked`
2. Work attaches to the target's hook
3. A polecat spawns in the rig (unless `--no-spawn`)
4. The polecat's startup hook finds and begins the work

:::tip

The Mayor typically handles slinging automatically. Use `gt sling` for manual assignment or when fine-grained control is needed.

:::

---

### `gt hook`

View or attach work to the current agent's hook.

```bash
gt hook [bead-id] [options]
```

**Description:** Without arguments, shows what is currently on the agent's hook. With a bead ID, attaches that work item to the hook. The hook is Gas Town's durability primitive -- work on a hook survives session restarts, compaction, and crashes.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
# Show current hook
gt hook

# Attach work to hook
gt hook gt-abc12
```

**Sample output:**

```
Hook: gt-abc12 "Fix login redirect loop" [in_progress]
  Rig: myproject
  Branch: fix/login-bug
  Convoy: hq-cv-001
  Hooked: 15m ago
```

---

### `gt unsling`

Remove work from a hook without completing it.

```bash
gt unsling <bead-id> [options]
```

**Description:** Detaches work from an agent's hook and sets the bead back to an assignable state. Use this when work needs to be reassigned or when a polecat should not continue with a task.

**Options:**

| Flag | Description |
|------|-------------|
| `--release` | Also release the bead back to `open` status |
| `--force` | Force unsling even if the agent is actively working |

**Example:**

```bash
# Unsling from current agent
gt unsling gt-abc12

# Unsling and release back to ready pool
gt unsling gt-abc12 --release
```

---

### `gt done`

Mark work as complete and submit a merge request.

```bash
gt done [options]
```

**Description:** The standard polecat exit command. Commits any remaining changes, pushes the branch, creates a merge request for the Refinery, updates the bead status, and exits the polecat session. This is the happy-path completion for any piece of work.

**Options:**

| Flag | Description |
|------|-------------|
| `--message <msg>` | MR description / completion summary |
| `--no-mr` | Complete without creating a merge request |
| `--escalate` | Exit with escalation instead of completion |
| `--defer` | Exit with deferred status (work paused, not done) |
| `--phase` | Exit with phase-complete status (gate point) |

**Example:**

```bash
# Standard completion
gt done --message "Fixed login redirect by correcting OAuth callback URL"

# Complete without MR (e.g., documentation-only changes)
gt done --no-mr --message "Updated local docs only"

# Escalate a blocker
gt done --escalate --message "Blocked: need API credentials for staging"
```

**Exit states:**

| Flag | Exit State | Meaning |
|------|-----------|---------|
| (default) | `COMPLETED` | Work done, MR submitted to Refinery |
| `--escalate` | `ESCALATED` | Hit a blocker, needs human input |
| `--defer` | `DEFERRED` | Paused, another agent can pick up later |
| `--phase` | `PHASE_COMPLETE` | Phase done, waiting for gate |

---

### `gt close`

Close a bead without going through the done workflow.

```bash
gt close <bead-id> [options]
```

**Description:** Manually closes a bead. Useful for closing duplicate issues, items resolved by other means, or administrative cleanup.

**Options:**

| Flag | Description |
|------|-------------|
| `--reason <text>` | Reason for closing |
| `--wontfix` | Close as won't fix |
| `--duplicate <id>` | Close as duplicate of another bead |

**Example:**

```bash
gt close gt-abc12 --reason "Resolved by upstream fix"
gt close gt-def34 --duplicate gt-abc12
gt close gt-ghi56 --wontfix
```

---

### `gt release`

Release a stuck in-progress bead back to the ready pool.

```bash
gt release <bead-id> [options]
```

**Description:** Frees a bead that is stuck in `in_progress` or `hooked` status, making it available for reassignment. Essential for recovering from polecat crashes or stalled work.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Release even if an agent appears to still be working on it |

**Example:**

```bash
gt release gt-abc12
gt release gt-abc12 --force
```

:::tip

The Witness automatically detects stalled polecats and can release their work. Use `gt release` for manual intervention.

:::

---

### `gt show`

Show detailed information about a bead or work item.

```bash
gt show <bead-id> [options]
```

**Description:** Displays comprehensive information about a bead including its status, history, assigned agent, convoy membership, and related activity.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--history` | Include full status change history |
| `--comments` | Include all comments |

**Example:**

```bash
gt show gt-abc12
gt show gt-abc12 --history
```

**Sample output:**

```
Bead: gt-abc12
Title: Fix login redirect loop
Type: bug
Priority: high
Status: in_progress
Rig: myproject
Agent: polecat/toast
Branch: fix/login-bug
Convoy: hq-cv-001
Created: 2h ago
Updated: 15m ago
```

---

### `gt cat`

Output the raw content of a bead or work artifact.

```bash
gt cat <bead-id> [options]
```

**Description:** Prints the raw bead content, including description, comments, and metadata. Useful for piping into other tools or for programmatic access.

**Options:**

| Flag | Description |
|------|-------------|
| `--field <name>` | Output only a specific field |
| `--format <fmt>` | Output format: `text`, `json`, `yaml` |

**Example:**

```bash
# Full bead content
gt cat gt-abc12

# Just the description
gt cat gt-abc12 --field description

# JSON output for scripting
gt cat gt-abc12 --format json
```

---

### `gt commit`

Git commit with automatic agent identity.

```bash
gt commit [flags] [-- git-commit-args...]
```

**Description:** A git commit wrapper that automatically sets the git author identity for agents. When run by an agent (with `GT_ROLE` set), it detects the agent identity from environment variables and converts it to a git-friendly name and email. When run by a human (no `GT_ROLE`), it passes through to plain `git commit`.

**Example:**

```bash
# Commit as current agent
gt commit -m "Fix bug"

# Stage all and commit
gt commit -am "Quick fix"

# Amend last commit
gt commit -- --amend
```

**Identity mapping:**

```
Agent: gastown/crew/jack  →  Name: gastown/crew/jack
                              Email: gastown.crew.jack@gastown.local
```

:::tip

The email domain is configurable in town settings (`agent_email_domain`). Default: `gastown.local`.

:::

---

### `gt gate`

Gate coordination for async workflows.

```bash
gt gate <subcommand>
```

**Description:** Gates provide async coordination points in workflows. Most gate operations are in the `bd` CLI (`bd gate create`, `bd gate show`, `bd gate list`, `bd gate close`, `bd gate approve`, `bd gate eval`). The `gt gate` command adds Gas Town integration.

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt gate wake` | Send wake mail to gate waiters after a gate closes |

**Example:**

```bash
gt gate wake <gate-id>
```

---

## Beads (Issue Tracking)

Beads is the git-backed issue tracking system integrated into Gas Town. The `bd` CLI manages beads directly.

### `bd create`

Create a new bead (issue).

```bash
bd create [options]
```

**Description:** Creates a new bead in the beads database. Beads are the fundamental work unit in Gas Town.

**Options:**

| Flag | Description |
|------|-------------|
| `--title <text>` | Bead title (required) |
| `--type <type>` | Type: `bug`, `feature`, `task`, `chore`, `epic` |
| `--priority <level>` | Priority: `critical`, `high`, `medium`, `low` |
| `--description <text>` | Detailed description |
| `--rig <name>` | Assign to a specific rig |
| `--label <label>` | Add labels (can be repeated) |
| `--parent <id>` | Set parent bead for hierarchical tracking |
| `--convoy <id>` | Add to an existing convoy |

**Example:**

```bash
bd create --title "Fix login bug" --type bug --priority high
# Created: gt-abc12

bd create --title "Add email validation" --type feature --description "Validate email format on registration form" --rig myproject
# Created: gt-def34

bd create --title "Auth epic" --type epic
# Created: gt-epc01
```

---

### `bd list`

List beads with optional filters.

```bash
bd list [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--status <status>` | Filter: `open`, `in_progress`, `closed`, `pending`, `hooked` |
| `--type <type>` | Filter by type |
| `--priority <level>` | Filter by priority |
| `--rig <name>` | Filter by rig |
| `--label <label>` | Filter by label |
| `--limit <n>` | Maximum number of results |
| `--sort <field>` | Sort by: `created`, `updated`, `priority` |
| `--json` | Output in JSON format |

**Example:**

```bash
# List all open beads
bd list --status open

# List high-priority bugs
bd list --type bug --priority high

# List recent 10 beads
bd list --limit 10 --sort updated
```

---

### `bd show`

Show detailed information about a bead.

```bash
bd show <bead-id> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--comments` | Include comments |

**Example:**

```bash
bd show gt-abc12
```

---

### `bd update`

Update a bead's fields.

```bash
bd update <bead-id> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--title <text>` | Update title |
| `--priority <level>` | Update priority |
| `--type <type>` | Update type |
| `--status <status>` | Update status |
| `--description <text>` | Update description |
| `--label <label>` | Add a label |
| `--remove-label <label>` | Remove a label |
| `--comment <text>` | Add a comment |
| `--assign <agent>` | Assign to an agent |

**Example:**

```bash
bd update gt-abc12 --priority critical --comment "This is blocking production"
bd update gt-def34 --status in_progress --assign polecat/toast
```

---

### `bd close`

Close a bead.

```bash
bd close <bead-id> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--reason <text>` | Closure reason |
| `--comment <text>` | Add a final comment |

**Example:**

```bash
bd close gt-abc12 --reason "Fixed in PR #42"
```

---

### `bd sync`

Synchronize the beads database.

```bash
bd sync [options]
```

**Description:** Syncs the local beads SQLite database with the git-backed storage. Ensures all beads are consistent across agents and workspaces.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Force full resync |
| `--rig <name>` | Sync a specific rig's beads only |

**Example:**

```bash
bd sync
bd sync --rig myproject
```

---

## Bead Subcommands (gt)

### `gt bead show`

Show bead details through the `gt` interface.

```bash
gt bead show <bead-id> [options]
```

**Description:** Similar to `bd show` but integrates with Gas Town context, showing additional information like hook status, convoy membership, and agent assignment.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--verbose` | Show extended details |

**Example:**

```bash
gt bead show gt-abc12
```

---

### `gt bead read`

Read a bead's full content into the agent context.

```bash
gt bead read <bead-id>
```

**Description:** Loads the complete bead content (description, comments, history) into the current agent's working context. Primarily used by agents to understand their assigned work.

**Example:**

```bash
gt bead read gt-abc12
```

---

### `gt bead move`

Move a bead between rigs.

```bash
gt bead move <bead-id> <target-rig> [options]
```

**Description:** Transfers a bead from one rig to another. Useful when work is reassigned to a different project.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Move even if the bead is currently hooked |

**Example:**

```bash
gt bead move gt-abc12 docs
gt bead move gt-def34 myproject --force
```

:::note

Moving a hooked bead without `--force` will fail. Unsling it first, or use `--force` to automatically unsling before moving.

:::

## See Also

- [Beads](../concepts/beads.md) — The work tracking primitive
- [Hooks](../concepts/hooks.md) — How agents claim and track work
- [GUPP](../concepts/gupp.md) — The propulsion principle behind work assignment
- [gt sling](sling.md) — Detailed sling command reference
- [Convoy & Tracking](convoys.md) — Batch work management


========================================================================
# Convoy & Tracking
# URL: /docs/cli-reference/convoys
========================================================================

# Convoy & Tracking

Convoys are Gas Town's primary mechanism for bundling and tracking batches of related work. A convoy groups multiple beads together, monitors their collective progress, and auto-closes when all tracked items complete.

---

### `gt convoy create`

Create a new convoy.

```bash
gt convoy create <title> [bead-id...] [options]
```

**Description:** Creates a new convoy with an optional set of initial beads. Convoys are the standard unit for tracking a batch of related work such as a feature set, a bug-fix sprint, or a documentation effort.

**Options:**

| Flag | Description |
|------|-------------|
| `--description <text>` | Detailed convoy description |
| `--rig <name>` | Associate with a specific rig |
| `--priority <level>` | Set convoy priority: `critical`, `high`, `medium`, `low` |
| `--deadline <date>` | Set a target completion date |

**Example:**

```bash
# Create with initial beads
gt convoy create "Auth System Fixes" gt-a1b2c gt-d3e4f gt-g5h6i
# Created: hq-cv-001

# Create empty convoy (add beads later)
gt convoy create "Q1 Performance Sprint" --priority high

# Create with description
gt convoy create "API Refactor" --description "Migrate all endpoints from v1 to v2 schema"
```

---

### `gt convoy add`

Add beads to an existing convoy.

```bash
gt convoy add <convoy-id> <bead-id>... [options]
```

**Description:** Adds one or more beads to an existing convoy. The convoy's completion tracking updates automatically.

**Example:**

```bash
# Add a single bead
gt convoy add hq-cv-001 gt-j7k8l

# Add multiple beads
gt convoy add hq-cv-001 gt-j7k8l gt-m9n0o gt-p1q2r
```

:::tip

Beads can belong to multiple convoys. This is useful when a single fix addresses multiple work streams.

:::

---

### `gt convoy list`

List all convoys.

```bash
gt convoy list [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--status <status>` | Filter: `active`, `completed`, `stalled`, `cancelled` |
| `--rig <name>` | Filter to a specific rig |
| `--limit <n>` | Maximum number of results |
| `--json` | Output in JSON format |

**Example:**

```bash
# List all convoys
gt convoy list

# List active convoys only
gt convoy list --status active
```

**Sample output:**

```
ID          TITLE                    STATUS     PROGRESS   AGE
hq-cv-001   Auth System Fixes        active     2/3        2h
hq-cv-002   API Refactor             active     0/5        30m
hq-cv-003   Bug Fix Sprint           completed  4/4        1d
```

---

### `gt convoy status`

Show summary status of all convoys or a specific convoy.

```bash
gt convoy status [convoy-id] [options]
```

**Description:** Without an ID, shows an overview of all active convoys. With a convoy ID, shows detailed progress information.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
# Overview of all active convoys
gt convoy status

# Status of a specific convoy
gt convoy status hq-cv-001
```

---

### `gt convoy show`

Show detailed information about a convoy.

```bash
gt convoy show <convoy-id> [options]
```

**Description:** Displays comprehensive convoy details including all tracked beads, their individual statuses, assigned agents, and overall progress metrics.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--verbose` | Include bead details and history |

**Example:**

```bash
gt convoy show hq-cv-001
```

**Sample output:**

```
Convoy: hq-cv-001
Title: Auth System Fixes
Status: active
Progress: 2/3 (67%)
Created: 2h ago

BEAD       STATUS         AGENT           TITLE
gt-a1b2c   completed      polecat/toast   Fix login redirect
gt-d3e4f   in_progress    polecat/alpha   Add email validation
gt-g5h6i   pending        -               Update auth docs
```

---

### `gt convoy close`

Manually close a convoy.

```bash
gt convoy close <convoy-id> [options]
```

**Description:** Closes a convoy regardless of whether all tracked beads are complete. Use for administrative cleanup or when remaining items are no longer relevant.

**Options:**

| Flag | Description |
|------|-------------|
| `--reason <text>` | Reason for manual closure |
| `--force` | Close even if beads are still open |

**Example:**

```bash
# Close a completed convoy
gt convoy close hq-cv-001

# Force-close an incomplete convoy
gt convoy close hq-cv-002 --force --reason "Requirements changed, work no longer needed"
```

:::note

Convoys auto-close when all tracked beads reach a terminal state (completed, closed, or won't-fix). Manual closure is only needed for exceptional situations.

:::

---

### `gt convoy check`

Check convoy health and consistency.

```bash
gt convoy check [convoy-id] [options]
```

**Description:** Validates the convoy state, checking for inconsistencies between convoy tracking and bead statuses. Reports any beads that may be stuck, orphaned, or in an unexpected state.

**Options:**

| Flag | Description |
|------|-------------|
| `--all` | Check all convoys |
| `--fix` | Attempt to fix inconsistencies |
| `--json` | Output in JSON format |

**Example:**

```bash
# Check a specific convoy
gt convoy check hq-cv-001

# Check all convoys and fix issues
gt convoy check --all --fix
```

---

### `gt convoy stranded`

Find convoys with work that is ready but unassigned.

```bash
gt convoy stranded [options]
```

**Description:** Identifies convoys where one or more beads are in a ready state (pending or open) but not assigned to any agent. These represent stalled work that needs attention.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt convoy stranded
```

**Sample output:**

```
CONVOY       TITLE                    STRANDED   TOTAL
hq-cv-002    API Refactor             3          5
hq-cv-004    Documentation Update     1          2
```

:::warning

Stranded convoys indicate work that has fallen through the cracks. The Mayor should be notified to reassign this work, or use `gt sling` to assign it manually.

:::

---

### `gt synthesis`

Manage convoy synthesis steps.

```bash
gt synthesis <subcommand> <convoy-id>
```

**Description:** Synthesis is the final step in a convoy workflow that combines outputs from all parallel legs into a unified deliverable. This is a top-level command separate from `gt convoy synthesis` (which generates reports).

**Aliases:** `synth`

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt synthesis status <convoy-id>` | Check if convoy is ready for synthesis |
| `gt synthesis start <convoy-id>` | Start synthesis for a convoy (checks all legs complete) |
| `gt synthesis close <convoy-id>` | Close convoy after synthesis complete |

**Example:**

```bash
# Check readiness
gt synthesis status hq-cv-abc

# Start synthesis step
gt synthesis start hq-cv-abc

# Close after synthesis
gt synthesis close hq-cv-abc
```

---

### `gt convoy synthesis`

Generate a synthesis report for a convoy.

```bash
gt convoy synthesis <convoy-id> [options]
```

**Description:** Produces a summary report of the convoy's progress, including what was accomplished, what remains, any blockers encountered, and time metrics. Useful for status updates and retrospectives.

**Options:**

| Flag | Description |
|------|-------------|
| `--format <fmt>` | Output format: `text`, `markdown`, `json` |
| `--verbose` | Include per-bead details |

**Example:**

```bash
# Generate text synthesis
gt convoy synthesis hq-cv-001

# Generate markdown report
gt convoy synthesis hq-cv-001 --format markdown

# Detailed synthesis
gt convoy synthesis hq-cv-001 --verbose
```

**Sample output:**

```
Convoy Synthesis: Auth System Fixes (hq-cv-001)
================================================

Progress: 2/3 completed (67%)
Duration: 2h 15m
Agents used: 3 polecats

Completed:
  - gt-a1b2c: Fix login redirect loop (polecat/toast, 45m)
  - gt-d3e4f: Add email validation (polecat/alpha, 1h 10m)

Remaining:
  - gt-g5h6i: Update auth docs (pending, unassigned)

Blockers: none
Merge status: 2 merged, 0 in queue
```

## See Also

- [Convoys](../concepts/convoys.md) — Conceptual overview of convoy-based work batching
- [Work Management](work.md) — Individual bead lifecycle commands
- [gt sling](sling.md) — Dispatching convoy beads to rigs


========================================================================
# Communication
# URL: /docs/cli-reference/communication
========================================================================

# Communication

Commands for inter-agent messaging, notifications, escalations, and broadcasts. Gas Town's communication layer is built on Erlang-inspired mailbox patterns with asynchronous message passing as the default.

---

## Mail

The mail system provides asynchronous message passing between agents. Each agent has a mailbox that persists across session restarts.

### `gt mail inbox`

View incoming messages.

```bash
gt mail inbox [options]
```

**Description:** Lists messages in the current agent's inbox. Messages are stored as JSONL files and persist across restarts.

**Options:**

| Flag | Description |
|------|-------------|
| `--unread` | Show only unread messages |
| `--from <agent>` | Filter by sender |
| `--limit <n>` | Maximum number of messages to show |
| `--since <duration>` | Show messages from the last N hours/minutes |
| `--json` | Output in JSON format |

**Example:**

```bash
# View all inbox
gt mail inbox

# View unread only
gt mail inbox --unread

# Messages from the Mayor in the last hour
gt mail inbox --from mayor --since 1h
```

**Sample output:**

```
ID     FROM        TIME     READ   SUBJECT
m-001  deacon      5m ago   *      Witness myproject unresponsive
m-002  polecat     15m ago  .      gt-abc12 completed
m-003  mayor       30m ago  .      New convoy assigned: hq-cv-002
```

---

### `gt mail send`

Send a message to another agent.

```bash
gt mail send <recipient> [options]
```

**Description:** Sends an asynchronous message to another agent's mailbox. The recipient will see it on their next inbox check.

**Options:**

| Flag | Description |
|------|-------------|
| `--subject <text>` | Message subject |
| `--body <text>` | Message body |
| `--priority <level>` | Message priority: `high`, `normal`, `low` |
| `--channel <name>` | Send to a named channel instead of an agent |
| `--attach <file>` | Attach a file to the message |

**Example:**

```bash
gt mail send mayor --subject "Need guidance" --body "Blocked on API design decision for auth module"
gt mail send deacon --subject "Health alert" --body "Refinery queue backing up" --priority high
```

---

### `gt mail read`

Read a specific message.

```bash
gt mail read <message-id>
```

**Description:** Displays the full content of a message and marks it as read.

**Example:**

```bash
gt mail read m-001
```

---

### `gt mail mark-read`

Mark messages as read without archiving.

```bash
gt mail mark-read <message-id>...
```

**Example:**

```bash
gt mail mark-read m-001 m-002
```

---

### `gt mail mark-unread`

Mark messages as unread.

```bash
gt mail mark-unread <message-id>...
```

**Example:**

```bash
gt mail mark-unread m-003
```

---

### `gt mail peek`

Preview messages without marking them as read.

```bash
gt mail peek [message-id] [options]
```

**Description:** Shows message content without changing its read status. Without an ID, peeks at the most recent unread message.

**Options:**

| Flag | Description |
|------|-------------|
| `--count <n>` | Number of messages to peek at |

**Example:**

```bash
gt mail peek
gt mail peek m-001
gt mail peek --count 5
```

---

### `gt mail reply`

Reply to a message.

```bash
gt mail reply <message-id> [options]
```

**Description:** Sends a reply to the sender of a message, preserving the conversation thread.

**Options:**

| Flag | Description |
|------|-------------|
| `--body <text>` | Reply body |
| `--all` | Reply to all recipients in the thread |

**Example:**

```bash
gt mail reply m-001 --body "Acknowledged, restarting the witness now"
```

---

### `gt mail search`

Search messages.

```bash
gt mail search <query> [options]
```

**Description:** Full-text search across all messages in the mailbox.

**Options:**

| Flag | Description |
|------|-------------|
| `--from <agent>` | Filter by sender |
| `--since <duration>` | Search within time window |
| `--limit <n>` | Maximum results |
| `--json` | Output in JSON format |

**Example:**

```bash
gt mail search "merge conflict"
gt mail search "blocked" --from polecat --since 24h
```

---

### `gt mail thread`

View a conversation thread.

```bash
gt mail thread <message-id> [options]
```

**Description:** Shows all messages in a conversation thread, from the original message through all replies.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt mail thread m-001
```

---

### `gt mail channel`

Manage or view named channels.

```bash
gt mail channel [name] [options]
```

**Description:** Without arguments, lists available channels. With a channel name, shows messages in that channel. Channels are named mailboxes for topic-based communication.

**Options:**

| Flag | Description |
|------|-------------|
| `--create <name>` | Create a new channel |
| `--subscribe` | Subscribe to a channel |
| `--unsubscribe` | Unsubscribe from a channel |
| `--limit <n>` | Message limit |

**Example:**

```bash
# List channels
gt mail channel

# View channel messages
gt mail channel alerts

# Create a channel
gt mail channel --create deployments
```

---

### `gt mail queue`

View the outgoing message queue.

```bash
gt mail queue [options]
```

**Description:** Shows messages that are queued for delivery but have not yet been picked up by their recipients.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt mail queue
```

---

### `gt mail announces`

View or manage announcement messages.

```bash
gt mail announces [options]
```

**Description:** Shows broadcast announcements that have been sent to all agents. Announcements are high-visibility messages from the Mayor or Overseer.

**Options:**

| Flag | Description |
|------|-------------|
| `--since <duration>` | Filter by time |
| `--json` | Output in JSON format |

**Example:**

```bash
gt mail announces
gt mail announces --since 24h
```

---

### `gt mail archive`

Archive messages.

```bash
gt mail archive <message-id>...
```

**Description:** Moves messages to the archive. Archived messages are no longer shown in the inbox but are preserved for future reference.

**Example:**

```bash
gt mail archive m-001 m-002
```

---

### `gt mail delete`

Delete messages.

```bash
gt mail delete <message-id>...
```

**Description:** Permanently delete messages from the mailbox.

**Example:**

```bash
gt mail delete m-001
```

---

### `gt mail clear`

Clear all messages from an inbox.

```bash
gt mail clear
```

**Description:** Removes all messages from the current agent's inbox.

**Example:**

```bash
gt mail clear
```

---

### `gt mail hook`

Attach a mail message to your hook.

```bash
gt mail hook <message-id>
```

**Description:** Attaches a mail message to your hook as an ad-hoc work assignment. Alias for `gt hook attach`.

**Example:**

```bash
gt mail hook m-001
```

---

### `gt mail check`

Check for new mail (for hooks).

```bash
gt mail check
```

**Description:** Non-interactive check for new mail, intended for use in hooks and automation.

**Example:**

```bash
gt mail check
```

---

### `gt mail claim`

Claim a message from a queue.

```bash
gt mail claim <queue-name>
```

**Description:** Claims the next available message from a named queue, removing it from the queue for processing.

**Example:**

```bash
gt mail claim work-queue
```

---

### `gt mail release`

Release a claimed queue message.

```bash
gt mail release <message-id>
```

**Description:** Releases a previously claimed queue message back to the queue for another agent to claim.

**Example:**

```bash
gt mail release m-001
```

---

### `gt mail group`

Manage mail groups.

```bash
gt mail group [subcommand]
```

**Description:** Create and manage mail groups (distribution lists) for sending messages to multiple agents at once.

**Example:**

```bash
gt mail group
```

---

## Notifications & Broadcasts

### `gt nudge`

Send a synchronous notification to an agent.

```bash
gt nudge <target> [options]
```

**Description:** Unlike mail (async), a nudge is a synchronous message delivery that interrupts the target agent immediately. Used by supervisors to wake up or redirect agents.

**Options:**

| Flag | Description |
|------|-------------|
| `--message <text>` | Nudge message |
| `--action <action>` | Requested action: `check-hook`, `patrol`, `restart`, `status` |

**Example:**

```bash
# Nudge a polecat to check its hook
gt nudge polecat/toast --action check-hook

# Nudge with a message
gt nudge witness --message "Check polecat alpha, appears stalled" --rig myproject
```

:::warning

Use nudges sparingly. They interrupt the target agent's current activity. For non-urgent messages, use `gt mail send` instead.

:::

---

### `gt broadcast`

Send a message to all agents.

```bash
gt broadcast [options]
```

**Description:** Sends a message to all agents in the town or all agents in a specific rig. Used for system-wide announcements.

**Options:**

| Flag | Description |
|------|-------------|
| `--message <text>` | Broadcast message |
| `--rig <name>` | Broadcast to a specific rig only |
| `--priority <level>` | Priority level |

**Example:**

```bash
# Town-wide broadcast
gt broadcast --message "Maintenance window in 30 minutes, save your work"

# Rig-specific broadcast
gt broadcast --rig myproject --message "Main branch frozen for release"
```

---

### `gt dnd`

Toggle do-not-disturb mode.

```bash
gt dnd [on|off] [options]
```

**Description:** When enabled, suppresses non-critical notifications and nudges. Critical escalations still come through. Useful during focused work or maintenance.

**Options:**

| Flag | Description |
|------|-------------|
| `--duration <time>` | Auto-disable after duration |
| `--allow <agent>` | Allow messages from specific agent even in DND |

**Example:**

```bash
# Enable DND
gt dnd on

# Enable for 2 hours
gt dnd on --duration 2h

# Disable
gt dnd off
```

---

### `gt notify`

Manage notification preferences.

```bash
gt notify [options]
```

**Description:** Configure how and when you receive notifications from Gas Town agents.

**Options:**

| Flag | Description |
|------|-------------|
| `--email <address>` | Set email notification address |
| `--discord <webhook>` | Set Discord webhook URL |
| `--level <level>` | Minimum notification level: `all`, `high`, `critical` |
| `--show` | Show current notification settings |

**Example:**

```bash
# Show current settings
gt notify --show

# Set email notifications for critical alerts
gt notify --email you@example.com --level critical

# Set Discord webhook
gt notify --discord https://discord.com/api/webhooks/...
```

---

## Escalations

Escalations are priority-routed alerts for issues that need human intervention or higher-authority decisions.

### `gt escalate`

Create a new escalation.

```bash
gt escalate [options]
```

**Description:** Creates a priority-routed escalation that travels up the supervisor chain until it reaches an agent authorized to handle it. Severity levels control routing depth.

**Options:**

| Flag | Description |
|------|-------------|
| `--severity <level>` | Severity: `P0` (critical), `P1` (high), `P2` (medium), `P3` (low) |
| `--message <text>` | Escalation description |
| `--bead <id>` | Associated bead |
| `--rig <name>` | Associated rig |
| `--to <agent>` | Direct escalation to a specific agent |

**Routing by severity:**

| Severity | Route |
|----------|-------|
| P0 (Critical) | Bead -> Mail:Mayor -> Email:Human -> SMS:Human |
| P1 (High) | Bead -> Mail:Mayor -> Email:Human |
| P2 (Medium) | Bead -> Mail:Mayor |
| P3 (Low) | Bead only |

**Example:**

```bash
# Critical escalation
gt escalate --severity P0 --message "Production database migration failed" --rig myproject

# Standard escalation with associated bead
gt escalate --severity P2 --message "Need design decision for API schema" --bead gt-abc12

# Direct escalation to Mayor
gt escalate --to mayor --message "Merge conflicts accumulating faster than resolution"
```

---

### `gt escalate list`

List all active escalations.

```bash
gt escalate list [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--severity <level>` | Filter by severity |
| `--status <status>` | Filter: `open`, `acked`, `closed` |
| `--rig <name>` | Filter by rig |
| `--json` | Output in JSON format |

**Example:**

```bash
gt escalate list
gt escalate list --severity P0 --status open
```

**Sample output:**

```
ID       SEVERITY   STATUS   FROM      AGE    MESSAGE
esc-001  P0         open     witness   5m     Production DB migration failed
esc-002  P2         acked    polecat   1h     Need API schema decision
esc-003  P3         open     refinery  30m    Flaky test in auth module
```

---

### `gt escalate ack`

Acknowledge an escalation.

```bash
gt escalate ack <escalation-id> [options]
```

**Description:** Marks an escalation as acknowledged, indicating someone is looking at it. This stops further routing up the chain.

**Options:**

| Flag | Description |
|------|-------------|
| `--message <text>` | Acknowledgment message |

**Example:**

```bash
gt escalate ack esc-001 --message "Investigating, will have fix in 15 minutes"
```

---

### `gt escalate close`

Close a resolved escalation.

```bash
gt escalate close <escalation-id> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--resolution <text>` | How the escalation was resolved |

**Example:**

```bash
gt escalate close esc-001 --resolution "Rolled back migration, applied fix, re-ran successfully"
```

---

### `gt escalate stale`

Find escalations that have not been acknowledged.

```bash
gt escalate stale [options]
```

**Description:** Lists escalations that have been open without acknowledgment for longer than expected, based on their severity level.

**Options:**

| Flag | Description |
|------|-------------|
| `--age <duration>` | Override stale threshold |
| `--json` | Output in JSON format |

**Example:**

```bash
gt escalate stale
gt escalate stale --age 30m
```

:::warning

Stale P0/P1 escalations indicate that critical issues are going unaddressed. These should be triaged immediately.

:::

## See Also

- [gt nudge](nudge.md) — Synchronous agent-to-agent messaging
- [Escalations](../operations/escalations.md) — The escalation routing system
- [Agent Hierarchy](../architecture/agent-hierarchy.md) — How agents are organized for communication


========================================================================
# gt nudge
# URL: /docs/cli-reference/nudge
========================================================================

# gt nudge

Send a synchronous message to any Gas Town worker's Claude Code session.

```bash
gt nudge <target> [message] [flags]
```

## Description

`gt nudge` delivers a message directly to a running Claude Code session: polecats, crew, witness, refinery, mayor, or deacon. Unlike `gt mail` (async mailbox), nudges interrupt the target immediately for real-time coordination.

Uses a reliable delivery pattern:

1. Sends text in literal mode (`-l` flag)
2. Waits 500ms for paste to complete
3. Sends Enter as a separate command

This is the **only** recommended way to send messages to Claude sessions. Do not use raw `tmux send-keys` elsewhere.

## Flags

| Flag | Short | Description |
|------|-------|-------------|
| `--force` | `-f` | Send even if target has DND enabled |
| `--message <text>` | `-m` | Message to send (alternative to positional argument) |

## Target Resolution

### Role Shortcuts

Role shortcuts expand to the appropriate session name:

| Shortcut | Expands to |
|----------|-----------|
| `mayor` | `gt-mayor` |
| `deacon` | `gt-deacon` |
| `witness` | `gt-<rig>-witness` (uses current rig) |
| `refinery` | `gt-<rig>-refinery` (uses current rig) |

### Direct Targets

Specify polecats and other agents by their full path:

```bash
gt nudge myproject/toast "Check your mail"
gt nudge myproject/alpha "What's your status?"
```

### Channel Syntax

Nudge all members of a named channel defined in `~/gt/config/messaging.json`:

```bash
gt nudge channel:workers "New priority work available"
```

Patterns like `myproject/polecats/*` in the channel definition are expanded to all matching agents.

## DND (Do Not Disturb)

If the target has DND enabled (`gt dnd on`), the nudge is skipped. Use `--force` to override DND and send anyway.

```bash
gt nudge myproject/toast "Urgent: check your mail" --force
```

## Examples

```bash
# Nudge a polecat
gt nudge myproject/furiosa "Check your mail and start working"

# Nudge with -m flag
gt nudge myproject/alpha -m "What's your status?"

# Nudge the mayor
gt nudge mayor "Status update requested"

# Nudge the current rig's witness
gt nudge witness "Check polecat health"

# Nudge the deacon
gt nudge deacon session-started

# Nudge a channel
gt nudge channel:workers "New priority work available"

# Force nudge past DND
gt nudge myproject/toast "Urgent fix needed" --force
```

:::tip

For non-urgent communication, use `gt mail send` instead. Nudges interrupt the target's current activity and should be reserved for time-sensitive coordination.

:::

:::note

`gt nudge` is the preferred way to send messages to running sessions. The lower-level `gt session inject` exists but lacks the reliable delivery guarantees that `gt nudge` provides.

:::

## See Also

- [Communication](communication.md) — Full communication command reference including mail and escalations
- [gt sling](sling.md) — Assign work (often paired with nudge to wake agents)
- [gt session](session-commands.md) — Low-level session management


========================================================================
# gt sling
# URL: /docs/cli-reference/sling
========================================================================

# gt sling

Sling work onto an agent's hook and start working immediately. This is the primary command for assigning work in Gas Town.

```bash
gt sling <bead-or-formula> [target] [flags]
```

## Description

`gt sling` handles the full lifecycle of work assignment:

- **Existing agents** -- sling to mayor, crew, witness, or refinery
- **Auto-spawning polecats** -- when the target is a rig, a new polecat is created
- **Dog dispatch** -- route work to Deacon's helper workers
- **Formula instantiation** -- cook formulas into molecules and attach them
- **Auto-convoy creation** -- ensures all slung work appears in `gt convoy list`

## Target Resolution

The target argument is flexible and resolves based on context:

| Target | Resolves to |
|--------|------------|
| *(none)* | Self (current agent) |
| `crew` | Crew worker in current rig |
| `<rig>` | Auto-spawn polecat in rig |
| `<rig>/<polecat>` | Specific polecat |
| `mayor` | Mayor agent |
| `deacon/dogs` | Auto-dispatch to idle dog |
| `deacon/dogs/<name>` | Specific dog |

## Flags

| Flag | Short | Description |
|------|-------|-------------|
| `--account <handle>` | | Claude Code account handle to use |
| `--agent <runtime>` | | Override agent/runtime (e.g., `claude`, `gemini`, `codex`, or custom alias) |
| `--args <text>` | `-a` | Natural language instructions for the executor |
| `--create` | | Create polecat if it doesn't exist |
| `--dry-run` | `-n` | Show what would be done without executing |
| `--force` | | Force spawn even if polecat has unread mail |
| `--hook-raw-bead` | | Hook raw bead without default formula (expert mode) |
| `--message <text>` | `-m` | Context message for the work |
| `--no-convoy` | | Skip auto-convoy creation for single-issue sling |
| `--no-merge` | | Skip merge queue on completion (keep work on feature branch for review) |
| `--on <bead>` | | Apply formula to existing bead (implies wisp scaffolding) |
| `--subject <text>` | `-s` | Context subject for the work |
| `--var <key=value>` | | Formula variable (can be repeated) |

## Examples

### Basic work assignment

```bash
# Sling a bead to a rig (auto-spawns a polecat)
gt sling gt-abc myproject

# Sling to a specific polecat
gt sling gt-abc myproject/toast

# Sling to the mayor
gt sling gt-abc mayor
```

### Batch slinging

```bash
# Sling multiple beads to a rig -- each gets its own polecat
gt sling gt-abc gt-def gt-ghi myproject
```

### Natural language instructions

```bash
# Pass context to the executing agent
gt sling gt-abc myproject --args "patch release"
gt sling gt-abc myproject --args "focus on security"
```

### Formula slinging

```bash
# Cook a formula into a molecule and sling it
gt sling mol-release mayor/

# Formula with variables
gt sling towers-of-hanoi --var disks=3

# Apply a formula to an existing bead
gt sling mol-review --on gt-abc

# Apply formula, then sling to a target
gt sling shiny --on gt-abc crew
```

### Spawning options

```bash
# Create polecat if it doesn't exist
gt sling gt-abc myproject --create

# Force spawn even with unread mail
gt sling gt-abc myproject --force

# Use a specific Claude account
gt sling gt-abc myproject --account work
```

### Dry run

```bash
# Preview what would happen
gt sling gt-abc myproject --dry-run
```

## Auto-Convoy

When slinging a single issue (not a formula), `gt sling` automatically creates a convoy to track the work. This ensures all slung work appears in `gt convoy list`, even single assignments. Use `--no-convoy` to skip this.

```bash
gt sling gt-abc myproject              # Creates "Work: <issue-title>" convoy
gt sling gt-abc myproject --no-convoy  # Skip auto-convoy creation
```

## Comparison with Related Commands

| Command | Behavior |
|---------|----------|
| `gt hook <bead>` | Just attach (no action taken) |
| `gt sling <bead>` | Attach + start now (keep context) |
| `gt handoff <bead>` | Attach + restart (fresh context) |

:::tip

The `--args` string is stored in the bead and shown via `gt prime`. Since the executor is an LLM, it interprets these instructions naturally -- write them as you would explain the task to a person.

:::

## See Also

- [GUPP](../concepts/gupp.md) — The propulsion principle that drives work assignment
- [Hooks](../concepts/hooks.md) — How agents claim work via hooks
- [Work Management](work.md) — Full work lifecycle commands
- [gt nudge](nudge.md) — Wake agents after slinging work


========================================================================
# Merge Queue
# URL: /docs/cli-reference/merge-queue
========================================================================

# Merge Queue

Commands for managing the Refinery's merge queue. The Refinery processes merge requests (MRs) submitted by polecats, rebasing them onto the latest main branch, running validation, and merging clean code.

---

### `gt mq list`

List items in the merge queue.

```bash
gt mq list [options]
```

**Description:** Shows all merge requests currently in the queue, including their position, status, and associated bead.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Filter to a specific rig |
| `--status <status>` | Filter: `pending`, `processing`, `validated`, `merged`, `rejected`, `conflict` |
| `--all` | Show across all rigs |
| `--json` | Output in JSON format |

**Example:**

```bash
# List queue for current rig
gt mq list

# List across all rigs
gt mq list --all

# Show only pending items
gt mq list --status pending
```

**Sample output:**

```
POS  ID       BEAD       BRANCH                  STATUS       RIG          AGE
1    mr-001   gt-abc12   fix/login-bug           processing   myproject    5m
2    mr-002   gt-def34   feat/email-validation   pending      myproject    2m
3    mr-003   gt-ghi56   docs/update-readme      pending      docs         1m
```

---

### `gt mq next`

Show or process the next item in the merge queue.

```bash
gt mq next [options]
```

**Description:** Without options, shows what the Refinery will process next. The Refinery typically calls this automatically during its patrol cycle.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Target a specific rig |
| `--process` | Immediately process the next item |
| `--json` | Output in JSON format |

**Example:**

```bash
# Show next item
gt mq next

# Process next item now
gt mq next --process
```

---

### `gt mq submit`

Submit a merge request to the queue.

```bash
gt mq submit [options]
```

**Description:** Adds the current branch to the merge queue for processing by the Refinery. This is typically called by `gt done` automatically, but can be used manually for crew workspaces or special cases.

**Options:**

| Flag | Description |
|------|-------------|
| `--branch <name>` | Branch to submit (default: current branch) |
| `--bead <id>` | Associated bead |
| `--message <text>` | MR description |
| `--priority` | Mark as priority merge (processed before others) |
| `--rig <name>` | Target rig |
| `--no-validate` | Skip pre-submission validation |

**Example:**

```bash
# Submit current branch
gt mq submit --bead gt-abc12 --message "Fixed OAuth callback URL handling"

# Submit a specific branch with priority
gt mq submit --branch fix/critical-bug --bead gt-xyz99 --priority

# Submit from a crew workspace
gt mq submit --branch feat/new-feature --rig myproject --message "Add user profile page"
```

:::tip

The standard polecat workflow uses `gt done` which handles `gt mq submit` automatically. Use `gt mq submit` directly for crew (human developer) workflows or manual submissions.

:::

---

### `gt mq status`

Show overall merge queue status.

```bash
gt mq status [options]
```

**Description:** Displays a summary of the merge queue including queue depth, processing rate, and any current issues.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Status for a specific rig |
| `--all` | Status across all rigs |
| `--json` | Output in JSON format |

**Example:**

```bash
gt mq status
gt mq status --all
```

**Sample output:**

```
Merge Queue Status: myproject
  Queue depth: 3
  Currently processing: mr-001 (fix/login-bug)
  Merged today: 7
  Rejected today: 1
  Avg merge time: 3m 20s
  Refinery: running (PID 1250)
```

---

### `gt mq reject`

Reject a merge request.

```bash
gt mq reject <mr-id> [options]
```

**Description:** Removes a merge request from the queue and marks it as rejected. The associated bead is updated and the submitting agent is notified.

**Options:**

| Flag | Description |
|------|-------------|
| `--reason <text>` | Rejection reason |
| `--reassign` | Release the bead for reassignment |

**Example:**

```bash
gt mq reject mr-002 --reason "Fails integration tests, needs rework"
gt mq reject mr-003 --reason "Superseded by mr-005" --reassign
```

---

### `gt mq retry`

Retry a failed or rejected merge request.

```bash
gt mq retry <mr-id> [options]
```

**Description:** Re-queues a previously failed or rejected merge request for another processing attempt. Useful after the underlying issue has been resolved (e.g., flaky test fixed, conflict resolved).

**Options:**

| Flag | Description |
|------|-------------|
| `--priority` | Retry with priority processing |
| `--rebase` | Force a fresh rebase before retrying |

**Example:**

```bash
gt mq retry mr-002
gt mq retry mr-002 --rebase --priority
```

---

### `gt mq integration`

Manage integration validation for the merge queue.

```bash
gt mq integration [options]
```

**Description:** Controls what validation the Refinery runs before merging. This includes test suites, build checks, linting, and custom validation scripts.

**Options:**

| Flag | Description |
|------|-------------|
| `--show` | Show current integration configuration |
| `--add <check>` | Add a validation check |
| `--remove <check>` | Remove a validation check |
| `--enable <check>` | Enable a disabled check |
| `--disable <check>` | Disable a check without removing it |
| `--rig <name>` | Configure for a specific rig |

**Example:**

```bash
# Show current checks
gt mq integration --show

# Add a test check
gt mq integration --add "npm test" --rig myproject

# Disable linting temporarily
gt mq integration --disable lint --rig myproject
```

**Sample configuration output:**

```
Integration Checks: myproject
  [enabled]   build     npm run build
  [enabled]   test      npm test
  [disabled]  lint      npm run lint
  [enabled]   typecheck npx tsc --noEmit
```

:::note[Merge Process]

The Refinery processes each MR through these steps:

1. **Rebase** -- Rebase the branch onto latest main
2. **Validate** -- Run all enabled integration checks
3. **Merge** -- Fast-forward merge to main if all checks pass
4. **Notify** -- Update bead status and notify the submitting agent

If a merge conflict occurs during rebase, the Refinery can spawn a fresh polecat to resolve the conflict before retrying.

:::

## See Also

- [Refinery](../agents/refinery.md) — The merge queue agent
- [gt refinery](refinery-commands.md) — Refinery management commands
- [Work Management](work.md) — Submitting work to the merge queue via `gt done`


========================================================================
# Rig Management
# URL: /docs/cli-reference/rigs
========================================================================

# Rig Management

Commands for adding, configuring, starting, stopping, and managing rigs. A rig is a project container that wraps a git repository with the full Gas Town agent infrastructure.

---

### `gt rig list`

List all rigs in the town.

```bash
gt rig list [options]
```

**Description:** Shows all rigs with their current status, agent counts, and activity summary.

**Options:**

| Flag | Description |
|------|-------------|
| `--status <status>` | Filter: `active`, `parked`, `docked`, `stopped` |
| `--json` | Output in JSON format |
| `--verbose` | Show extended details |

**Example:**

```bash
gt rig list
gt rig list --status active
```

**Sample output:**

```
RIG          STATUS    POLECATS   QUEUE   OPEN BEADS   BRANCH
myproject    active    3          2       7            main
docs         active    1          0       2            main
backend      parked    0          0       4            main
```

---

### `gt rig add`

Add a new rig to the town.

```bash
gt rig add <name> <git-url> [options]
```

**Description:** Clones the repository, creates the rig directory structure, initializes beads, and sets up agent workspaces (witness, refinery, mayor, polecats).

**Options:**

| Flag | Description |
|------|-------------|
| `--branch <name>` | Check out a specific branch (default: `main`) |
| `--agent <runtime>` | Default agent runtime for this rig |
| `--no-start` | Add the rig but do not start its agents |
| `--shallow` | Shallow clone for large repositories |

**Example:**

```bash
# Add a rig
gt rig add myproject https://github.com/you/repo.git

# Add with SSH URL and specific branch
gt rig add backend git@github.com:you/backend.git --branch develop

# Add without starting agents
gt rig add docs https://github.com/you/docs.git --no-start
```

**Created structure:**

```
~/gt/myproject/
├── .beads/          # Rig-level issue tracking
├── config.json      # Rig configuration
├── refinery/rig/    # Canonical main clone
├── mayor/rig/       # Mayor's working copy
├── crew/            # Human developer workspaces
├── witness/         # Health monitor state
├── polecats/        # Ephemeral worker directories
└── plugins/         # Rig-level plugins
```

---

### `gt rig start`

Start all agents for a rig.

```bash
gt rig start <name> [options]
```

**Description:** Starts the Witness and Refinery agents for the specified rig. If polecats have hooked work, they will also be spawned.

**Options:**

| Flag | Description |
|------|-------------|
| `--agents <list>` | Start only specific agents (comma-separated) |

**Example:**

```bash
gt rig start myproject
gt rig start myproject --agents witness,refinery
```

---

### `gt rig stop`

Stop all agents for a rig.

```bash
gt rig stop <name> [options]
```

**Description:** Gracefully stops all agents running in the rig, including the Witness, Refinery, and any active polecats.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Force stop without graceful shutdown |
| `--keep-polecats` | Do not stop running polecats |

**Example:**

```bash
gt rig stop myproject
gt rig stop myproject --force
```

:::warning

Stopping a rig with active polecats may result in lost uncommitted work. Use `--keep-polecats` or ensure polecats have committed their changes first.

:::

---

### `gt rig shutdown`

Fully shut down a rig including cleanup.

```bash
gt rig shutdown <name> [options]
```

**Description:** Stops all agents, cleans up polecat worktrees, drains the merge queue, and puts the rig in a stopped state. More thorough than `gt rig stop`.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Skip confirmation and force shutdown |
| `--drain` | Wait for merge queue to empty before shutting down |

**Example:**

```bash
gt rig shutdown myproject
gt rig shutdown myproject --drain
```

---

### `gt rig status`

Show detailed status for a rig.

```bash
gt rig status <name> [options]
```

**Description:** Displays comprehensive rig information including agent status, polecat activity, merge queue depth, open beads, and resource usage.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--verbose` | Show extended details |

**Example:**

```bash
gt rig status myproject
```

**Sample output:**

```
Rig: myproject
Repository: https://github.com/you/repo.git
Branch: main
Status: active

Agents:
  Witness:  running (PID 1240)
  Refinery: running (PID 1250)

Polecats: 3 running
  toast    running   gt-abc12   fix/login-bug          15m
  alpha    running   gt-def34   feat/email-validation   10m
  bravo    running   gt-ghi56   refactor/auth-module    5m

Merge Queue: 2 pending, 1 processing
Open Beads: 7
Active Convoy: hq-cv-001 (2/3)
```

---

### `gt rig reset`

Reset a rig to a clean state.

```bash
gt rig reset <name> [options]
```

**Description:** Resets the rig by stopping all agents, removing all polecat worktrees, clearing the merge queue, and optionally resetting the beads database.

**Options:**

| Flag | Description |
|------|-------------|
| `--hard` | Also reset beads database and agent state |
| `--force` | Skip confirmation |
| `--keep-crew` | Preserve crew workspaces |

**Example:**

```bash
gt rig reset myproject
gt rig reset myproject --hard --force
```

:::danger

`gt rig reset --hard` destroys all work state including beads, hooks, and merge queue items. This cannot be undone.

:::

---

### `gt rig boot`

Boot a rig from cold state.

```bash
gt rig boot <name> [options]
```

**Description:** Initializes a rig that has been shut down or is in a cold state. Sets up worktrees, starts agents, and processes any pending hooks.

**Options:**

| Flag | Description |
|------|-------------|
| `--full` | Full boot including all optional agents |

**Example:**

```bash
gt rig boot myproject
gt rig boot myproject --full
```

---

### `gt rig reboot`

Reboot a running rig.

```bash
gt rig reboot <name> [options]
```

**Description:** Performs a stop-then-start cycle for the rig. Agents are stopped gracefully, state is preserved, and agents are restarted.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Force reboot without waiting for graceful shutdown |

**Example:**

```bash
gt rig reboot myproject
```

---

### `gt rig park`

Park a rig (suspend without removing).

```bash
gt rig park <name> [options]
```

**Description:** Stops all agents and marks the rig as parked. Parked rigs consume no resources but retain all configuration and state. Work can be resumed later with `gt rig unpark`.

**Options:**

| Flag | Description |
|------|-------------|
| `--reason <text>` | Reason for parking |

**Example:**

```bash
gt rig park backend --reason "Waiting for API spec finalization"
```

---

### `gt rig unpark`

Resume a parked rig.

```bash
gt rig unpark <name> [options]
```

**Description:** Restarts agents and resumes work in a previously parked rig.

**Example:**

```bash
gt rig unpark backend
```

---

### `gt rig dock`

Dock a rig (deep storage mode).

```bash
gt rig dock <name> [options]
```

**Description:** Places a rig in deep storage. Docking is more aggressive than parking: it cleans up worktrees, removes polecat directories, and minimizes disk usage while preserving configuration and beads history.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Skip confirmation |

**Example:**

```bash
gt rig dock backend
```

---

### `gt rig undock`

Restore a docked rig.

```bash
gt rig undock <name> [options]
```

**Description:** Restores a docked rig by recreating worktrees, agent directories, and starting agents.

**Example:**

```bash
gt rig undock backend
```

---

### `gt rig config`

View or modify rig configuration.

```bash
gt rig config <name> [key] [value] [options]
```

**Description:** Without a key, shows all configuration for the rig. With a key, shows that specific setting. With a key and value, sets the configuration.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--reset` | Reset configuration to defaults |

**Common configuration keys:**

| Key | Description | Default |
|-----|-------------|---------|
| `agent` | Default agent runtime | `claude` |
| `max_polecats` | Maximum concurrent polecats | `5` |
| `merge_strategy` | Merge strategy: `rebase`, `merge`, `squash` | `rebase` |
| `auto_witness` | Auto-start witness on rig start | `true` |
| `auto_refinery` | Auto-start refinery on rig start | `true` |
| `branch` | Main branch name | `main` |

**Example:**

```bash
# Show all config
gt rig config myproject

# Get a specific setting
gt rig config myproject max_polecats

# Set a value
gt rig config myproject max_polecats 8

# Reset to defaults
gt rig config myproject --reset
```

---

### `gt rig settings`

Manage advanced rig settings.

```bash
gt rig settings <name> [options]
```

**Description:** Access and modify advanced rig settings that are not part of the standard configuration. Includes validation rules, plugin settings, and integration configuration.

**Options:**

| Flag | Description |
|------|-------------|
| `--show` | Display all settings |
| `--set <key=value>` | Set a setting |
| `--unset <key>` | Remove a setting |
| `--json` | Output in JSON format |

**Example:**

```bash
# Show all settings
gt rig settings myproject --show

# Set a custom setting
gt rig settings myproject --set "validation.timeout=300"

# Remove a setting
gt rig settings myproject --unset "validation.timeout"
```

## See Also

- [Rigs](../concepts/rigs.md) — Conceptual overview of project containers
- [Workspace Management](workspace.md) — Town-level workspace commands
- [Configuration](configuration.md) — Town-level configuration


========================================================================
# Session & Handoff
# URL: /docs/cli-reference/sessions
========================================================================

# Session & Handoff

Commands for managing agent sessions, handoffs between sessions, molecules (multi-step workflows), and formulas (reusable workflow templates).

---

## Session Management

### `gt handoff`

Hand off work to a new session.

```bash
gt handoff [options]
```

**Description:** Performs a graceful session transition. The current session saves its state (hook, context, progress) into a handoff file, then exits. The next session picks up from where the previous one left off. This is the standard way to deal with context limits.

**Options:**

| Flag | Description |
|------|-------------|
| `--message <text>` | Handoff notes for the next session |
| `--checkpoint` | Save a full checkpoint before handing off |
| `--to <agent>` | Hand off to a specific agent role |

**Example:**

```bash
# Standard handoff
gt handoff --message "Completed 3/5 test fixes, remaining: auth_test.go and api_test.go"

# Handoff with checkpoint
gt handoff --checkpoint --message "At step 3 of molecule, next: run integration tests"
```

:::tip[Handoff Best Practice]

Always include a clear message describing what was accomplished and what remains. The next session relies on this context to continue work effectively.

:::

---

### `gt resume`

Resume from a previous session or handoff.

```bash
gt resume [options]
```

**Description:** Loads the most recent handoff state and resumes work. Reads the handoff file, restores hook state, and continues from where the previous session ended.

**Options:**

| Flag | Description |
|------|-------------|
| `--session <id>` | Resume a specific session by ID |
| `--latest` | Resume the most recent session (default) |
| `--list` | List available sessions to resume |

**Example:**

```bash
# Resume latest
gt resume

# List available sessions
gt resume --list

# Resume a specific session
gt resume --session sess-abc123
```

---

### `gt park`

Park the current session (pause without handoff).

```bash
gt park [options]
```

**Description:** Saves the current session state and exits without triggering a new session. The work stays on the hook and can be resumed later. Unlike handoff, parking does not expect an immediate successor.

**Options:**

| Flag | Description |
|------|-------------|
| `--message <text>` | Parking notes |
| `--duration <time>` | Expected park duration (informational) |

**Example:**

```bash
gt park --message "Waiting for API review feedback" --duration 4h
```

---

### `gt prime`

Initialize agent context for a new or resumed session.

```bash
gt prime [options]
```

**Description:** Loads the full agent context including role, identity, configuration, hook state, and CLAUDE.md instructions. This is the first command an agent runs in a new session.

**Options:**

| Flag | Description |
|------|-------------|
| `--role <role>` | Override the agent role |
| `--verbose` | Show detailed priming information |

**Example:**

```bash
# Standard prime (reads GT_ROLE from environment)
gt prime

# Prime with explicit role
gt prime --role witness
```

:::note

`gt prime` should be run after compaction, clear, or new session. It is the canonical way to restore agent identity and context.

:::

---

### `gt seance`

Inspect a completed or crashed session.

```bash
gt seance <session-id> [options]
```

**Description:** Examines the state and artifacts from a previous session, including its hook state, messages sent, activity log, and exit condition. Named after "communicating with the dead" -- useful for debugging crashed or failed sessions.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--verbose` | Show full session transcript excerpts |
| `--artifacts` | List all session artifacts |

**Example:**

```bash
gt seance sess-abc123
gt seance sess-abc123 --verbose
```

**Sample output:**

```
Session: sess-abc123
Agent: polecat/toast
Rig: myproject
Duration: 45m
Exit: COMPLETED
Hook: gt-abc12 (completed)
Messages sent: 3
Commits: 4
Branch: fix/login-bug
```

---

### `gt checkpoint`

Manage session checkpoints for crash recovery.

```bash
gt checkpoint <subcommand>
```

**Description:** Checkpoints capture current work state so that if a session crashes, the next session can resume. Checkpoint data includes current molecule and step, hooked bead, modified files, git branch, last commit, and timestamp. Stored in `.polecat-checkpoint.json` in the polecat directory.

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt checkpoint write` | Write a checkpoint of current session state |
| `gt checkpoint read` | Read and display the current checkpoint |
| `gt checkpoint clear` | Clear the checkpoint file |

**Example:**

```bash
# Save current state
gt checkpoint write

# View saved checkpoint
gt checkpoint read

# Clear checkpoint
gt checkpoint clear
```

---

### `gt cycle`

Cycle between related tmux sessions.

```bash
gt cycle <subcommand>
```

**Description:** Switches between related tmux sessions based on the current session type. Session groups are detected automatically.

**Session groups:**

| Group | Sessions |
|-------|----------|
| Town | Mayor ↔ Deacon |
| Crew | All crew members in the same rig |
| Rig infra | Witness ↔ Refinery (per rig) |
| Polecats | All polecats in the same rig |

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt cycle next` | Switch to next session in group |
| `gt cycle prev` | Switch to previous session in group |

**Example:**

```bash
gt cycle next
gt cycle prev
```

---

## Molecules

Molecules are multi-step workflow execution units. They break complex work into a directed acyclic graph (DAG) of steps that can be executed sequentially, in parallel, or with dependencies.

### `gt mol status`

Show molecule execution status.

```bash
gt mol status [options]
```

**Description:** Displays the status of the currently active molecule, including completed steps, current step, and remaining steps.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt mol status
```

**Sample output:**

```
Molecule: auth-refactor
Status: in_progress
Progress: 3/7 steps

STEP   STATUS       DESCRIPTION              AGENT
1      completed    Create migration script   polecat/toast
2      completed    Update data models        polecat/alpha
3      completed    Migrate endpoints         polecat/bravo
4      in_progress  Update tests              polecat/charlie
5      pending      Run integration suite     -
6      pending      Update documentation      -
7      pending      Deploy to staging         -
```

---

### `gt mol current`

Show the currently executing step.

```bash
gt mol current [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt mol current
```

---

### `gt mol progress`

Show a progress summary for the active molecule.

```bash
gt mol progress [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt mol progress
```

---

### `gt mol step done`

Mark the current molecule step as completed.

```bash
gt mol step done [options]
```

**Description:** Marks the current step as complete and advances the molecule to the next step (or triggers parallel steps if the DAG allows).

**Options:**

| Flag | Description |
|------|-------------|
| `--message <text>` | Completion notes |
| `--output <data>` | Step output data for downstream steps |
| `--skip-next` | Skip the next step |

**Example:**

```bash
gt mol step done --message "All endpoints migrated, 47 files changed"
```

---

### `gt mol attach`

Attach to a running molecule.

```bash
gt mol attach <molecule-id> [options]
```

**Description:** Attaches the current agent to an active molecule, joining the workflow execution.

**Example:**

```bash
gt mol attach mol-auth-001
```

---

### `gt mol detach`

Detach from a molecule without stopping it.

```bash
gt mol detach [options]
```

**Description:** Removes the current agent from the molecule while allowing other agents to continue. The molecule continues execution with remaining participants.

**Example:**

```bash
gt mol detach
```

---

### `gt mol burn`

Abort and discard a molecule.

```bash
gt mol burn <molecule-id> [options]
```

**Description:** Terminates molecule execution and discards all in-progress work. Completed steps are preserved but remaining steps are cancelled.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Skip confirmation |

**Example:**

```bash
gt mol burn mol-auth-001
gt mol burn mol-auth-001 --force
```

:::danger

Burning a molecule cancels all pending and in-progress steps. This cannot be undone.

:::

---

### `gt mol squash`

Squash molecule steps into a single work item.

```bash
gt mol squash <molecule-id> [options]
```

**Description:** Combines the outputs of all completed molecule steps into a single consolidated result. Useful when a multi-step workflow should produce a single merge request.

**Options:**

| Flag | Description |
|------|-------------|
| `--message <text>` | Squash commit message |

**Example:**

```bash
gt mol squash mol-auth-001 --message "Complete auth refactor"
```

---

### `gt mol dag`

Display the molecule's step dependency graph.

```bash
gt mol dag [molecule-id] [options]
```

**Description:** Shows the directed acyclic graph of steps, their dependencies, and current execution state. Helps visualize the workflow structure.

**Options:**

| Flag | Description |
|------|-------------|
| `--format <fmt>` | Output format: `text`, `mermaid`, `json` |

**Example:**

```bash
gt mol dag

# Generate Mermaid diagram
gt mol dag --format mermaid
```

**Sample output (text):**

```
1: Create migration script [completed]
├── 2: Update data models [completed]
│   ├── 3: Migrate endpoints [completed]
│   │   └── 4: Update tests [in_progress]
│   │       └── 5: Run integration suite [pending]
│   └── 6: Update documentation [pending]
└── 7: Deploy to staging [pending] (depends: 5, 6)
```

---

## Formulas

Formulas are reusable workflow templates that define molecule structures. They encode repeatable multi-step processes. See [gt formula](formula.md) for the full formula command reference.

## See Also

- [Session Cycling](../concepts/session-cycling.md) — How context refresh works
- [Molecules](../concepts/molecules.md) — Running workflow instances
- [gt formula](formula.md) — Formula template management
- [gt session](session-commands.md) — Low-level session management
### `gt formula list`

List available formulas.

```bash
gt formula list [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt formula list
```

**Sample output:**

```
NAME                 STEPS   DESCRIPTION
feature-standard     5       Standard feature development workflow
bug-fix              3       Bug fix with test and validation
refactor             7       Multi-phase refactoring pipeline
release              4       Release preparation and deployment
```

---

### `gt formula show`

Show details of a formula.

```bash
gt formula show <name> [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt formula show feature-standard
```

---

### `gt formula run`

Execute a formula as a new molecule.

```bash
gt formula run <name> [options]
```

**Description:** Instantiates a formula into a running molecule, assigning it to the current context.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Target rig |
| `--bead <id>` | Associated bead |
| `--param <key=value>` | Set formula parameters (repeatable) |
| `--dry-run` | Show what would happen without executing |

**Example:**

```bash
gt formula run feature-standard --rig myproject --bead gt-abc12

gt formula run refactor --param "target=auth-module" --param "scope=endpoints"

gt formula run release --dry-run
```

---

### `gt formula create`

Create a new formula.

```bash
gt formula create <name> [options]
```

**Description:** Creates a new formula template from a definition or interactively.

**Options:**

| Flag | Description |
|------|-------------|
| `--from <file>` | Load formula definition from a YAML/JSON file |
| `--from-molecule <id>` | Create a formula from an existing molecule's structure |
| `--description <text>` | Formula description |

**Example:**

```bash
# Create from a file
gt formula create my-workflow --from workflow.yaml

# Create from an existing molecule
gt formula create api-migration --from-molecule mol-auth-001 --description "API version migration workflow"
```


========================================================================
# gt session
# URL: /docs/cli-reference/session-commands
========================================================================

# gt session

Manage tmux sessions for polecats.

```bash
gt session [command] [flags]
```

**Alias:** `gt sess`

## Description

Sessions are tmux sessions running Claude for each polecat. Use the subcommands to start, stop, attach, and monitor sessions.

:::tip

To send messages to a running session, use `gt nudge` (not `gt session inject`). The nudge command uses reliable delivery that works correctly with Claude Code.

:::

## Subcommands

| Command | Description |
|---------|-------------|
| [`list`](#gt-session-list) | List all sessions |
| [`start`](#gt-session-start) | Start a polecat session |
| [`stop`](#gt-session-stop) | Stop a polecat session |
| [`restart`](#gt-session-restart) | Restart a polecat session |
| [`at`](#gt-session-at) | Attach to a running session |
| [`status`](#gt-session-status) | Show session status details |
| [`check`](#gt-session-check) | Check session health |
| [`capture`](#gt-session-capture) | Capture recent session output |
| [`inject`](#gt-session-inject) | Send message to session (prefer `gt nudge`) |

---

## gt session list

List all running polecat sessions.

```bash
gt session list [flags]
```

Shows session status, rig, and polecat name.

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |
| `--rig <name>` | Filter by rig name |

**Examples:**

```bash
gt session list
gt session list --rig myproject
gt session list --json
```

---

## gt session start

Start a new tmux session for a polecat.

```bash
gt session start <rig>/<polecat> [flags]
```

Creates a tmux session, navigates to the polecat's working directory, and launches Claude. Optionally inject an initial issue to work on.

**Flags:**

| Flag | Description |
|------|-------------|
| `--issue <id>` | Issue ID to work on |

**Examples:**

```bash
gt session start myproject/toast
gt session start myproject/toast --issue gt-123
```

---

## gt session stop

Stop a running polecat session.

```bash
gt session stop <rig>/<polecat> [flags]
```

Attempts graceful shutdown first (Ctrl-C), then kills the tmux session. Use `--force` to skip graceful shutdown.

**Flags:**

| Flag | Short | Description |
|------|-------|-------------|
| `--force` | `-f` | Force immediate shutdown |

**Examples:**

```bash
gt session stop myproject/toast
gt session stop myproject/toast --force
```

---

## gt session restart

Restart a polecat session (stop + start).

```bash
gt session restart <rig>/<polecat> [flags]
```

Gracefully stops the current session and starts a fresh one.

**Flags:**

| Flag | Short | Description |
|------|-------|-------------|
| `--force` | `-f` | Force immediate shutdown before restart |

**Examples:**

```bash
gt session restart myproject/toast
gt session restart myproject/toast --force
```

---

## gt session at

Attach to a running polecat session.

```bash
gt session at <rig>/<polecat> [flags]
```

**Alias:** `gt session attach`

Attaches the current terminal to the tmux session. Detach with `Ctrl-B D`.

**Examples:**

```bash
gt session at myproject/toast
```

---

## gt session status

Show detailed status for a polecat session.

```bash
gt session status <rig>/<polecat> [flags]
```

Displays running state, uptime, session info, and activity.

**Examples:**

```bash
gt session status myproject/toast
```

---

## gt session check

Check session health for polecats.

```bash
gt session check [rig] [flags]
```

Validates that:

1. Polecats with work-on-hook have running tmux sessions
2. Sessions are responsive

Use this for manual health checks or debugging session issues.

**Examples:**

```bash
gt session check              # Check all rigs
gt session check myproject    # Check specific rig
```

---

## gt session capture

Capture recent output from a polecat session.

```bash
gt session capture <rig>/<polecat> [count] [flags]
```

Returns the last N lines of terminal output. Useful for checking progress without attaching to the session.

**Flags:**

| Flag | Short | Description |
|------|-------|-------------|
| `--lines <n>` | `-n` | Number of lines to capture (default: 100) |

**Examples:**

```bash
gt session capture myproject/toast          # Last 100 lines (default)
gt session capture myproject/toast 50       # Last 50 lines
gt session capture myproject/toast -n 50    # Same as above
```

---

## gt session inject

Send a message to a polecat session (low-level).

```bash
gt session inject <rig>/<polecat> [flags]
```

:::warning

For sending messages to Claude sessions, use `gt nudge` instead. It uses reliable delivery (literal mode + timing) that works correctly with Claude Code's input handling. This command is a low-level primitive for file-based injection or cases where you need raw tmux `send-keys` behavior.

:::

**Flags:**

| Flag | Short | Description |
|------|-------|-------------|
| `--file <path>` | `-f` | File to read message from |
| `--message <text>` | `-m` | Message to inject |

**Examples:**

```bash
# Preferred: use gt nudge
gt nudge myproject/furiosa "Check your mail"

# Low-level: file injection
gt session inject myproject/toast -f prompt.txt
```

## See Also

- [Session & Handoff](sessions.md) — Higher-level session lifecycle commands
- [Session Cycling](../concepts/session-cycling.md) — How context refresh works
- [gt nudge](nudge.md) — Preferred way to message running sessions


========================================================================
# gt polecat
# URL: /docs/cli-reference/polecat-commands
========================================================================

# gt polecat

Manage polecat lifecycle in rigs.

```bash
gt polecat [command] [flags]
```

**Alias:** `gt polecats`

## Description

Polecats are **ephemeral workers**: spawned for one task, nuked when done. There is no idle state. A polecat is either:

- **Working** -- actively doing assigned work
- **Stalled** -- session crashed mid-work (needs Witness intervention)
- **Zombie** -- finished but `gt done` failed (needs cleanup)

**Self-cleaning model:** When work completes, the polecat runs `gt done`, which pushes the branch, submits to the merge queue, and exits. The Witness then nukes the sandbox. Polecats don't wait for more work.

**Session vs sandbox:** The Claude session cycles frequently (handoffs, compaction). The git worktree (sandbox) persists until nuke. Work survives session restarts.

## Subcommands

| Command | Description |
|---------|-------------|
| [`list`](#gt-polecat-list) | List polecats in a rig |
| [`status`](#gt-polecat-status) | Show detailed polecat status |
| [`nuke`](#gt-polecat-nuke) | Completely destroy a polecat |
| [`remove`](#gt-polecat-remove) | Remove polecats from a rig |
| [`stale`](#gt-polecat-stale) | Detect stale polecats |
| [`gc`](#gt-polecat-gc) | Garbage collect stale branches |
| [`check-recovery`](#gt-polecat-check-recovery) | Check if polecat needs recovery vs safe to nuke |
| [`git-state`](#gt-polecat-git-state) | Show git state for pre-kill verification |
| [`identity`](#gt-polecat-identity) | Manage polecat identities |

---

## gt polecat list

List polecats in a rig or all rigs.

```bash
gt polecat list [rig] [flags]
```

Shows all currently active polecats with their states: `working`, `done`, or `stuck`.

**Flags:**

| Flag | Description |
|------|-------------|
| `--all` | List polecats in all rigs |
| `--json` | Output as JSON |

**Examples:**

```bash
gt polecat list myproject
gt polecat list --all
gt polecat list myproject --json
```

---

## gt polecat status

Show detailed status for a polecat.

```bash
gt polecat status <rig>/<polecat> [flags]
```

Displays comprehensive information including:

- Current lifecycle state (working, done, stuck, idle)
- Assigned issue (if any)
- Session status (running/stopped, attached/detached)
- Session creation time
- Last activity time

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Examples:**

```bash
gt polecat status myproject/toast
gt polecat status myproject/toast --json
```

---

## gt polecat nuke

Completely destroy a polecat and all its artifacts.

```bash
gt polecat nuke <rig>/<polecat>... | <rig> --all [flags]
```

This is the nuclear option for post-merge cleanup. It:

1. Kills the Claude session (if running)
2. Deletes the git worktree (bypassing all safety checks)
3. Deletes the polecat branch
4. Closes the agent bead (if exists)

**Safety checks** -- the command refuses to nuke a polecat if:

- Worktree has unpushed/uncommitted changes
- Polecat has an open merge request (MR bead)
- Polecat has work on its hook

Use `--force` to bypass safety checks. Use `--dry-run` to preview.

**Flags:**

| Flag | Short | Description |
|------|-------|-------------|
| `--all` | | Nuke all polecats in the rig |
| `--dry-run` | | Show what would be nuked without doing it |
| `--force` | `-f` | Force nuke, bypassing all safety checks (LOSES WORK) |

**Examples:**

```bash
# Nuke a specific polecat
gt polecat nuke myproject/toast

# Nuke multiple polecats
gt polecat nuke myproject/toast myproject/furiosa

# Nuke all polecats in a rig
gt polecat nuke myproject --all

# Preview what would be nuked
gt polecat nuke myproject --all --dry-run

# Force nuke (bypasses safety checks)
gt polecat nuke myproject/toast --force
```

:::warning

Nuking with `--force` bypasses all safety checks and will destroy uncommitted work. Always use `--dry-run` first to verify what will be affected.

:::

---

## gt polecat remove

Remove one or more polecats from a rig.

```bash
gt polecat remove <rig>/<polecat>... | <rig> --all [flags]
```

Fails if session is running (stop first). Warns if uncommitted changes exist.

**Flags:**

| Flag | Short | Description |
|------|-------|-------------|
| `--all` | | Remove all polecats in the rig |
| `--force` | `-f` | Force removal, bypassing checks |

**Examples:**

```bash
gt polecat remove myproject/toast
gt polecat remove myproject/toast myproject/furiosa
gt polecat remove myproject --all
gt polecat remove myproject --all --force
```

---

## gt polecat stale

Detect stale polecats that are candidates for cleanup.

```bash
gt polecat stale <rig> [flags]
```

A polecat is considered stale if:

- No active tmux session
- Way behind main (>threshold commits) OR no agent bead
- Has no uncommitted work that could be lost

The default threshold is 20 commits behind main.

**Flags:**

| Flag | Description |
|------|-------------|
| `--cleanup` | Automatically nuke stale polecats |
| `--json` | Output as JSON |
| `--threshold <n>` | Commits behind main to consider stale (default: 20) |

**Examples:**

```bash
gt polecat stale myproject
gt polecat stale myproject --threshold 50
gt polecat stale myproject --json

# Auto-cleanup stale polecats
gt polecat stale myproject --cleanup

# Preview cleanup
gt polecat stale myproject --cleanup --dry-run
```

---

## gt polecat gc

Garbage collect stale polecat branches in a rig.

```bash
gt polecat gc <rig> [flags]
```

Polecats use unique timestamped branches (`polecat/<name>-<timestamp>`) to prevent drift issues. Over time, these branches accumulate when stale polecats are repaired.

This command removes orphaned branches:

- Branches for polecats that no longer exist
- Old timestamped branches (keeps only the current one per polecat)

**Flags:**

| Flag | Description |
|------|-------------|
| `--dry-run` | Show what would be deleted without deleting |

**Examples:**

```bash
gt polecat gc myproject
gt polecat gc myproject --dry-run
```

---

## gt polecat check-recovery

Check recovery status of a polecat.

```bash
gt polecat check-recovery <rig>/<polecat> [flags]
```

Used by the Witness to determine appropriate cleanup action based on `cleanup_status` in the agent bead:

- **SAFE_TO_NUKE** -- `cleanup_status` is `clean`, no work at risk
- **NEEDS_RECOVERY** -- unpushed/uncommitted work detected

The Witness should escalate `NEEDS_RECOVERY` cases to the Mayor.

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Examples:**

```bash
gt polecat check-recovery myproject/toast
gt polecat check-recovery myproject/toast --json
```

---

## gt polecat git-state

Show git state for a polecat's worktree.

```bash
gt polecat git-state <rig>/<polecat> [flags]
```

Used by the Witness for pre-kill verification to ensure no work is lost. Returns whether the worktree is clean (safe to kill) or dirty (needs cleanup).

Checks:

- **Working tree** -- uncommitted changes
- **Unpushed commits** -- commits ahead of origin/main
- **Stashes** -- stashed changes

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Examples:**

```bash
gt polecat git-state myproject/toast
gt polecat git-state myproject/toast --json
```

---

## gt polecat identity

Manage polecat identity beads in rigs.

```bash
gt polecat identity [command] [flags]
```

**Alias:** `gt polecat id`

Identity beads track polecat metadata, CV history, and lifecycle state.

**Subcommands:**

| Command | Description |
|---------|-------------|
| `add` | Create an identity bead for a polecat |
| `list` | List polecat identity beads in a rig |
| `show` | Show polecat identity with CV summary |
| `rename` | Rename a polecat identity (preserves CV) |
| `remove` | Remove a polecat identity |

**Examples:**

```bash
gt polecat identity list myproject
gt polecat identity show myproject/toast
gt polecat identity rename myproject/toast myproject/alpha
```

## See Also

- [Polecats](../agents/polecats.md) — Ephemeral worker agent documentation
- [Lifecycle](../operations/lifecycle.md) — Agent lifecycle management
- [gt warrant](warrant.md) — Death warrants for stuck polecats
- [Agent Operations](agents.md) — General agent management commands


========================================================================
# gt refinery
# URL: /docs/cli-reference/refinery-commands
========================================================================

# gt refinery

Manage the Refinery -- the per-rig merge queue processor.

```bash
gt refinery [command] [flags]
```

**Alias:** `gt ref`

## Description

The Refinery serializes all merges to main for a rig:

1. Receives MRs submitted by polecats (via `gt done`)
2. Rebases work branches onto latest main
3. Runs validation (tests, builds, checks)
4. Merges to main when clear
5. If conflict: spawns a fresh polecat to re-implement (original is already gone)

One Refinery per rig. It is a persistent agent that processes work as it arrives. The polecat is already nuked by the time the Refinery processes its MR.

**Role shortcuts:** `refinery` in mail/nudge addresses resolves to the current rig's Refinery.

## Subcommands

| Command | Description |
|---------|-------------|
| [`start`](#gt-refinery-start) | Start the Refinery |
| [`stop`](#gt-refinery-stop) | Stop the Refinery |
| [`restart`](#gt-refinery-restart) | Restart the Refinery |
| [`status`](#gt-refinery-status) | Show Refinery status |
| [`attach`](#gt-refinery-attach) | Attach to Refinery session |
| [`queue`](#gt-refinery-queue) | Show merge queue |
| [`ready`](#gt-refinery-ready) | List MRs ready for processing |
| [`blocked`](#gt-refinery-blocked) | List MRs blocked by open tasks |
| [`unclaimed`](#gt-refinery-unclaimed) | List unclaimed MRs |
| [`claim`](#gt-refinery-claim) | Claim an MR for processing |
| [`release`](#gt-refinery-release) | Release a claimed MR |

---

## gt refinery start

Start the Refinery for a rig.

```bash
gt refinery start [rig] [flags]
```

**Alias:** `gt refinery spawn`

Launches the merge queue processor which monitors for polecat work branches and merges them to the appropriate target branches. If rig is not specified, infers it from the current directory.

**Flags:**

| Flag | Description |
|------|-------------|
| `--agent <alias>` | Agent alias to run the Refinery with (overrides town default) |
| `--foreground` | Run in foreground (default: background) |

**Examples:**

```bash
gt refinery start myproject
gt refinery start myproject --foreground
gt refinery start              # infer rig from cwd
```

---

## gt refinery stop

Gracefully stop a running Refinery.

```bash
gt refinery stop [rig] [flags]
```

Completes any in-progress merge first, then stops. If rig is not specified, infers it from the current directory.

---

## gt refinery restart

Restart the Refinery for a rig.

```bash
gt refinery restart [rig] [flags]
```

Stops the current session (if running) and starts a fresh one.

**Flags:**

| Flag | Description |
|------|-------------|
| `--agent <alias>` | Agent alias to run the Refinery with (overrides town default) |

**Examples:**

```bash
gt refinery restart myproject
gt refinery restart             # infer rig from cwd
```

---

## gt refinery status

Show Refinery status for a rig.

```bash
gt refinery status [rig] [flags]
```

Displays running state, current work, queue length, and statistics.

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Examples:**

```bash
gt refinery status
gt refinery status myproject
gt refinery status --json
```

---

## gt refinery attach

Attach to a running Refinery's Claude session.

```bash
gt refinery attach [rig] [flags]
```

Allows interactive access to the Refinery agent for debugging or manual intervention. Detach with `Ctrl-B D`.

**Flags:**

| Flag | Description |
|------|-------------|
| `--agent <alias>` | Agent alias to run the Refinery with (overrides town default) |

**Examples:**

```bash
gt refinery attach myproject
gt refinery attach              # infer rig from cwd
```

---

## gt refinery queue

Show the merge queue for a rig.

```bash
gt refinery queue [rig] [flags]
```

Lists all pending merge requests waiting to be processed.

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Examples:**

```bash
gt refinery queue
gt refinery queue myproject
gt refinery queue --json
```

---

## gt refinery ready

List MRs ready for processing.

```bash
gt refinery ready [rig] [flags]
```

Shows MRs that are:

- Not currently claimed by any worker (or claim is stale)
- Not blocked by an open task (e.g., conflict resolution in progress)

This is the preferred command for finding work to process.

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Examples:**

```bash
gt refinery ready
gt refinery ready --json
```

---

## gt refinery blocked

List MRs blocked by open tasks.

```bash
gt refinery blocked [rig] [flags]
```

Shows MRs waiting for conflict resolution or other blocking tasks to complete. When the blocking task closes, the MR will appear in `ready`.

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Examples:**

```bash
gt refinery blocked
gt refinery blocked --json
```

---

## gt refinery unclaimed

List MRs available for claiming.

```bash
gt refinery unclaimed [rig] [flags]
```

Shows MRs that are not currently claimed by any worker, or have stale claims (worker may have crashed). Useful for parallel refinery workers to find work.

**Flags:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Examples:**

```bash
gt refinery unclaimed
gt refinery unclaimed --json
```

---

## gt refinery claim

Claim an MR for processing.

```bash
gt refinery claim <mr-id> [flags]
```

When running multiple refinery workers in parallel, each worker must claim an MR before processing to prevent double-processing. Claims expire after 10 minutes if not processed (for crash recovery).

The worker ID is automatically determined from the `GT_REFINERY_WORKER` environment variable, or defaults to `refinery-1`.

**Examples:**

```bash
gt refinery claim gt-abc123
GT_REFINERY_WORKER=refinery-2 gt refinery claim gt-abc123
```

---

## gt refinery release

Release a claimed MR back to the queue.

```bash
gt refinery release <mr-id> [flags]
```

Called when processing fails and the MR should be retried by another worker. Clears the claim so other workers can pick up the MR.

**Examples:**

```bash
gt refinery release gt-abc123
```

:::note

All subcommands that accept `[rig]` infer the rig from the current directory when not specified.

:::

## See Also

- [Refinery](../agents/refinery.md) — The merge queue agent
- [Merge Queue](merge-queue.md) — MQ list, submit, and status commands
- [Work Management](work.md) — Submitting work via `gt done`


========================================================================
# Diagnostics
# URL: /docs/cli-reference/diagnostics
========================================================================

# Diagnostics

Commands for monitoring, auditing, and troubleshooting Gas Town. These tools provide visibility into system health, agent activity, resource usage, and operational state.

---

## Town Status

### `gt status`

Display overall town status.

```bash
gt status [options]
```

**Description:** Shows the current state of the Gas Town workspace including registered rigs, active polecats, and agent status.

**Options:**

| Flag | Description |
|------|-------------|
| `--fast` | Skip mail lookups for faster execution |
| `--json` | Output as JSON |
| `--verbose`, `-v` | Show detailed multi-line output per agent |
| `--watch`, `-w` | Watch mode: refresh status continuously |
| `--interval`, `-n` | Refresh interval in seconds (default: `2`) |

**Aliases:** `stat`

**Example:**

```bash
# Quick status
gt status

# Watch mode with custom interval
gt status --watch --interval 5

# Fast mode (skip mail lookups)
gt status --fast
```

---

### `gt whoami`

Show the identity used for mail commands.

```bash
gt whoami
```

**Description:** Displays the identity determined by the `GT_ROLE` environment variable. If set, you are an agent session. If not, you are the overseer (human). Use the `--identity` flag with mail commands to override.

**Example:**

```bash
gt whoami
```

---

## Activity & Monitoring

### `gt activity`

Show recent system activity.

```bash
gt activity [options]
```

**Description:** Displays a chronological log of significant events across the town, including agent starts/stops, work assignments, completions, escalations, and merges.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Filter to a specific rig |
| `--agent <name>` | Filter to a specific agent |
| `--type <type>` | Filter by event type: `work`, `agent`, `merge`, `escalation`, `mail` |
| `--since <duration>` | Show events since (e.g., `1h`, `30m`, `1d`) |
| `--limit <n>` | Maximum events to show |
| `--json` | Output in JSON format |

**Example:**

```bash
# Recent activity
gt activity

# Last hour of work events
gt activity --type work --since 1h

# Activity for a specific rig
gt activity --rig myproject --limit 50
```

**Sample output:**

```
TIME       TYPE        AGENT           EVENT
14:30      work        polecat/toast   Completed gt-abc12 (fix/login-bug)
14:28      merge       refinery        Merged mr-001 to main
14:25      agent       witness         Started patrol cycle
14:20      work        polecat/alpha   Started gt-def34 (feat/email-validation)
14:15      escalation  polecat/bravo   Escalated gt-ghi56 (P2: need design input)
```

---

### `gt audit`

Generate an audit report.

```bash
gt audit [options]
```

**Description:** Produces a comprehensive audit of system state, including agent uptime, work throughput, error rates, and resource usage. Useful for periodic reviews and compliance.

**Options:**

| Flag | Description |
|------|-------------|
| `--since <duration>` | Audit period (default: `24h`) |
| `--rig <name>` | Audit a specific rig |
| `--format <fmt>` | Output format: `text`, `json`, `csv` |
| `--output <file>` | Write report to a file |

**Example:**

```bash
# Daily audit
gt audit

# Weekly audit for a specific rig
gt audit --since 7d --rig myproject --format json

# Export to file
gt audit --output audit-report.json --format json
```

---

### `gt feed`

Watch the live activity feed.

```bash
gt feed [options]
```

**Description:** Displays a real-time stream of system events. Similar to `gt activity` but shows events as they happen. Press `Ctrl+C` to exit.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Filter to a specific rig |
| `--type <type>` | Filter by event type |
| `--quiet` | Show only significant events |

**Example:**

```bash
# Watch all events
gt feed

# Watch a specific rig
gt feed --rig myproject

# Watch only work and merge events
gt feed --type work,merge
```

---

### `gt trail`

Show the execution trail for a specific bead or agent.

```bash
gt trail <target> [options]
```

**Description:** Traces the full lifecycle of a bead or agent, showing every state change, assignment, message, and action in chronological order.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--verbose` | Include full message content |

**Example:**

```bash
# Trail a bead
gt trail gt-abc12

# Trail an agent
gt trail polecat/toast
```

**Sample output (bead trail):**

```
gt-abc12: Fix login redirect loop
  14:00  created     by mayor          priority=high type=bug
  14:05  assigned    to myproject      via gt sling
  14:05  hooked      on polecat/toast  branch=fix/login-bug
  14:10  started     by polecat/toast
  14:25  committed   4 files changed
  14:28  done        by polecat/toast  exit=COMPLETED
  14:28  submitted   mr-001            to merge queue
  14:30  merged      to main           by refinery
  14:30  closed      auto
```

---

### `gt log`

View system logs.

```bash
gt log [options]
```

**Description:** Accesses the raw event log (`.events.jsonl`) with filtering and formatting options. More detailed than `gt activity` -- includes internal system events.

**Options:**

| Flag | Description |
|------|-------------|
| `--level <level>` | Filter: `debug`, `info`, `warn`, `error` |
| `--since <duration>` | Time filter |
| `--limit <n>` | Maximum entries |
| `--follow`, `-f` | Follow the log in real time |
| `--json` | Raw JSON output |

**Example:**

```bash
# Recent errors
gt log --level error

# Follow log in real time
gt log -f

# Debug output for the last 30 minutes
gt log --level debug --since 30m
```

---

### `gt doctor`

Diagnose system health issues.

```bash
gt doctor [options]
```

**Description:** Runs a comprehensive health check of the Gas Town installation, verifying dependencies, configuration, agent state, database integrity, and common issues.

**Options:**

| Flag | Description |
|------|-------------|
| `--fix` | Attempt to fix detected issues |
| `--verbose` | Show detailed check results |
| `--json` | Output in JSON format |

**Example:**

```bash
# Run health check
gt doctor

# Run and attempt fixes
gt doctor --fix
```

**Sample output:**

```
Gas Town Doctor
===============

[OK]  Go version: 1.23.4
[OK]  Git version: 2.43.0
[OK]  Beads (bd) version: 0.44.2
[OK]  SQLite3 available
[OK]  Tmux version: 3.4
[OK]  Claude Code available
[OK]  Workspace structure valid
[OK]  Beads database integrity
[WARN] Stale polecat worktree: myproject/polecats/toast (2d old)
[OK]  Daemon running (PID 1200)
[OK]  Mayor session active
[OK]  Deacon session active

11 checks passed, 1 warning, 0 errors
```

:::tip

Run `gt doctor` after installation, after upgrading Gas Town, or whenever something seems wrong. It catches most common configuration issues.

:::

---

### `gt dashboard`

Start the convoy tracking web dashboard.

```bash
gt dashboard [options]
```

**Description:** Starts a web server that displays a convoy tracking dashboard with real-time status indicators, progress tracking, and last activity indicators. Auto-refreshes every 30 seconds via htmx.

**Options:**

| Flag | Description |
|------|-------------|
| `--port <number>` | HTTP port to listen on (default: `8080`) |
| `--open` | Open browser automatically |

**Example:**

```bash
# Start on default port
gt dashboard

# Start on custom port
gt dashboard --port 3000

# Start and open browser
gt dashboard --open
```

---

### `gt costs`

Show resource usage and cost metrics.

```bash
gt costs [options]
```

**Description:** Displays token usage, API costs, agent session time, and other resource metrics. Helps track spending across agents and rigs.

**Options:**

| Flag | Description |
|------|-------------|
| `--since <duration>` | Time period (default: `24h`) |
| `--rig <name>` | Filter by rig |
| `--agent <name>` | Filter by agent |
| `--format <fmt>` | Output format: `text`, `json`, `csv` |
| `--group-by <field>` | Group by: `rig`, `agent`, `role`, `hour` |

**Example:**

```bash
# Daily costs
gt costs

# Weekly costs by rig
gt costs --since 7d --group-by rig

# Costs for a specific agent
gt costs --agent polecat/toast
```

**Sample output:**

```
Gas Town Costs (last 24h)
=========================

Total tokens: 2,450,000 (input: 1,800,000 / output: 650,000)
Estimated cost: $12.35

By rig:
  myproject    $8.20    (66%)
  docs         $2.15    (17%)
  backend      $2.00    (16%)

By role:
  polecats     $9.50    (77%)
  refinery     $1.85    (15%)
  mayor        $0.60    (5%)
  other        $0.40    (3%)
```

---

### `gt cleanup`

Clean up temporary files and stale resources.

```bash
gt cleanup [options]
```

**Description:** Removes temporary files, stale worktrees, dead session artifacts, and other accumulated debris. A more action-oriented companion to `gt stale`.

**Options:**

| Flag | Description |
|------|-------------|
| `--rig <name>` | Clean up a specific rig |
| `--all` | Clean up across all rigs |
| `--dry-run` | Show what would be cleaned without doing it |
| `--force` | Skip confirmation |
| `--age <duration>` | Only clean items older than this (default: `24h`) |

**Example:**

```bash
# Preview cleanup
gt cleanup --dry-run

# Clean up everything
gt cleanup --all --force

# Clean up a specific rig
gt cleanup --rig myproject --age 12h
```

---

### `gt patrol digest`

Generate a patrol cycle digest.

```bash
gt patrol digest [options]
```

**Description:** Summarizes the results of recent patrol cycles run by persistent agents (Deacon, Witnesses, Refinery). Shows what was detected and what actions were taken.

**Options:**

| Flag | Description |
|------|-------------|
| `--since <duration>` | Time period (default: `1h`) |
| `--agent <name>` | Filter to a specific agent's patrols |
| `--json` | Output in JSON format |

**Example:**

```bash
gt patrol digest
gt patrol digest --since 6h --agent witness
```

---

## Orphan Management

### `gt orphans`

Find orphaned resources.

```bash
gt orphans [options]
```

**Description:** Identifies orphaned resources across the town -- processes without parent agents, worktrees without polecats, branches without beads, and other disconnected artifacts.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |
| `--verbose` | Show detailed information about each orphan |

**Example:**

```bash
gt orphans
```

**Sample output:**

```
Orphaned Resources
==================

Processes (2):
  PID 4521  claude session (no parent agent)
  PID 4789  claude session (no parent agent)

Worktrees (1):
  myproject/polecats/ghost/  (no active polecat)

Branches (3):
  fix/old-bug         (no associated bead)
  feat/abandoned       (no associated bead)
  tmp/experiment       (no associated bead)
```

---

### `gt orphans procs`

List orphaned processes specifically.

```bash
gt orphans procs [options]
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt orphans procs
```

---

### `gt orphans kill`

Terminate orphaned processes.

```bash
gt orphans kill [options]
```

**Description:** Kills orphaned processes that have no parent agent managing them. These are typically leftover sessions from crashed agents.

**Options:**

| Flag | Description |
|------|-------------|
| `--force` | Kill without confirmation |
| `--dry-run` | Show what would be killed |

**Example:**

```bash
gt orphans kill --dry-run
gt orphans kill --force
```

:::warning

Verify orphans are truly orphaned before killing. Use `gt orphans procs` first to review, then `gt orphans kill --dry-run` to preview the action.

:::

---

## Peek & Sessions

:::tip

For session handoffs, molecules, and formulas, see the [Session & Handoff](sessions.md) page.

:::

### `gt peek`

Peek at an agent's current state without interrupting it.

```bash
gt peek <agent> [options]
```

**Description:** Non-invasively inspects what an agent is currently doing, including its hook state, recent activity, and resource usage. Does not send any messages to the agent.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt peek polecat/toast
gt peek witness --rig myproject
gt peek mayor
```

**Sample output:**

```
Peek: polecat/toast (myproject)
  Status: running
  Hook: gt-abc12 "Fix login redirect loop"
  Last activity: 2m ago (editing src/auth/callback.ts)
  Session: sess-xyz789
  Commits: 3 (uncommitted changes: 2 files)
  Branch: fix/login-bug
```

---

### `gt session`

Manage tmux sessions for polecats.

```bash
gt session <subcommand> [options]
```

**Aliases:** `sess`

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt session list` | List all sessions |
| `gt session status <name>` | Show session status details |
| `gt session start <name>` | Start a polecat session |
| `gt session stop <name>` | Stop a polecat session |
| `gt session restart <name>` | Restart a polecat session |
| `gt session at <name>` | Attach to a running session |
| `gt session capture <name>` | Capture recent session output |
| `gt session check <name>` | Check session health |
| `gt session inject <name>` | Send message to session (prefer `gt nudge`) |

**Example:**

```bash
# List all sessions
gt session list

# Check session health
gt session check toast

# Capture recent output
gt session capture toast

# Restart a session
gt session restart toast
```

:::tip

To send messages to a running session, use `gt nudge` instead of `gt session inject`. The nudge command uses reliable delivery that works correctly with Claude Code.

:::

## See Also

- [Monitoring](../operations/monitoring.md) — Operational monitoring patterns
- [Troubleshooting](../operations/troubleshooting.md) — Common issues and solutions
- [gt krc](krc.md) — Ephemeral data lifecycle management
- [gt patrol](patrol.md) — Patrol digest aggregation


========================================================================
# Configuration
# URL: /docs/cli-reference/configuration
========================================================================

# Configuration

Commands for configuring agent runtimes, account settings, themes, hooks, and issue integration. These settings control how Gas Town operates at the town and rig levels.

---

## Agent Configuration

### `gt config agent list`

List configured agent runtimes.

```bash
gt config agent list [options]
```

**Description:** Shows all configured agent runtimes and their command mappings. Gas Town supports multiple AI coding agent runtimes.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

**Example:**

```bash
gt config agent list
```

**Sample output:**

```
AGENT      COMMAND     STATUS      DEFAULT
claude     claude      available   *
gemini     gemini      available
codex      codex       not found
cursor     cursor      available
auggie     auggie      not found
amp        amp         not found
```

---

### `gt config agent get`

Get a specific agent runtime configuration value.

```bash
gt config agent get <agent> [key]
```

**Description:** Without a key, shows all configuration for the specified agent runtime. With a key, shows that specific setting.

**Example:**

```bash
# Show all config for claude
gt config agent get claude

# Get a specific setting
gt config agent get claude model
```

---

### `gt config agent set`

Set an agent runtime configuration value.

```bash
gt config agent set <agent> <key> <value>
```

**Description:** Configures a specific setting for an agent runtime. Use this to set command paths, model preferences, and other runtime-specific options.

**Common keys:**

| Key | Description | Example |
|-----|-------------|---------|
| `command` | Command to invoke the agent | `claude` |
| `model` | Preferred model | `claude-opus-4-5-20251101` |
| `args` | Additional arguments | `--verbose` |
| `timeout` | Session timeout | `3600` |
| `max_tokens` | Maximum token limit | `200000` |

**Example:**

```bash
# Set command for gemini
gt config agent set gemini command "gemini"

# Set model preference
gt config agent set claude model "claude-opus-4-5-20251101"

# Set custom args
gt config agent set cursor args "--no-telemetry"
```

---

### `gt config default-agent`

Get or set the default agent runtime.

```bash
gt config default-agent [agent]
```

**Description:** Without an argument, shows the current default agent. With an argument, sets the default agent runtime used when no `--agent` flag is specified.

**Example:**

```bash
# Show default
gt config default-agent
# Output: claude

# Set default to gemini
gt config default-agent gemini
```

:::tip

The default agent can be overridden at the rig level with `gt rig config <rig> agent <runtime>` or per-command with the `--agent` flag.

:::

---

## Account Management

### `gt account`

Manage multiple Claude Code accounts for Gas Town.

```bash
gt account <subcommand>
```

**Description:** Enables switching between Claude Code accounts (e.g., personal vs work) with easy account selection per spawn or globally.

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt account list` | List registered accounts |
| `gt account add <handle>` | Add a new account |
| `gt account default <handle>` | Set the default account |
| `gt account status` | Show current account info |
| `gt account switch <handle>` | Switch to a different account |

**Example:**

```bash
# List accounts
gt account list

# Add a new account
gt account add work

# Set default
gt account default work

# Show current status
gt account status

# Switch accounts
gt account switch personal
```

---

## Appearance

### `gt theme`

Manage tmux status bar themes for Gas Town sessions.

```bash
gt theme [name] [options]
```

**Description:** Without arguments, shows the current theme assignment for the rig. With a name, sets the active tmux status bar theme.

**Options:**

| Flag | Description |
|------|-------------|
| `--list`, `-l` | List available themes |

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt theme apply` | Apply theme to all running sessions in this rig |
| `gt theme cli` | View or set CLI color scheme (`dark`/`light`/`auto`) |

**Example:**

```bash
# Show current theme
gt theme

# List available themes
gt theme --list

# Set theme
gt theme forest

# Apply to running sessions
gt theme apply

# Set CLI color scheme
gt theme cli dark
```

---

## Claude Code Hooks

### `gt hooks`

List all Claude Code hooks configured in the workspace.

```bash
gt hooks [options]
```

**Description:** Scans for `.claude/settings.json` files across the workspace and displays all configured Claude Code hooks, organized by type.

**Options:**

| Flag | Description |
|------|-------------|
| `--verbose`, `-v` | Show hook commands |
| `--json` | Output as JSON |

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt hooks install` | Install a hook from the registry |
| `gt hooks list` | List available hooks from the registry |

**Hook types:**

| Type | Description |
|------|-------------|
| `SessionStart` | Runs when Claude session starts |
| `PreCompact` | Runs before context compaction |
| `UserPromptSubmit` | Runs before user prompt is submitted |
| `PreToolUse` | Runs before tool execution |
| `PostToolUse` | Runs after tool execution |
| `Stop` | Runs when Claude session stops |

**Example:**

```bash
# List all hooks
gt hooks

# Show with commands
gt hooks --verbose

# JSON output
gt hooks --json

# Install from registry
gt hooks install
```

---

## Status Line

### `gt issue`

Manage current issue displayed in the tmux status line.

```bash
gt issue <subcommand>
```

**Description:** Controls which issue/bead ID is shown in the tmux status bar for the current session. Useful for quick visual identification of what you are working on.

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt issue set <bead-id>` | Set the current issue (shown in tmux status line) |
| `gt issue show` | Show the current issue |
| `gt issue clear` | Clear the current issue from status line |

**Example:**

```bash
# Set the current issue
gt issue set gt-abc12

# Show current issue
gt issue show

# Clear the status line
gt issue clear
```

---

## Plugin Management

### `gt plugin`

Manage plugins that run during Deacon patrol cycles.

```bash
gt plugin <subcommand> [options]
```

**Description:** Plugins are periodic automation tasks defined by `plugin.md` files with TOML frontmatter. They can be installed at town level (`~/gt/plugins/`) or rig level (`<rig>/plugins/`).

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `gt plugin list` | List all discovered plugins |
| `gt plugin show <name>` | Show plugin details |
| `gt plugin run <name>` | Manually trigger plugin execution |
| `gt plugin history <name>` | Show plugin execution history |

**Gate types:**

| Type | Description |
|------|-------------|
| `cooldown` | Run if enough time has passed (e.g., `1h`) |
| `cron` | Run on a schedule (e.g., `"0 9 * * *"`) |
| `condition` | Run if a check command returns exit 0 |
| `event` | Run on events (e.g., `startup`) |
| `manual` | Never auto-run, trigger explicitly |

**Example:**

```bash
# List all plugins
gt plugin list

# Show plugin details
gt plugin show my-plugin

# Manually run a plugin
gt plugin run my-plugin

# List as JSON
gt plugin list --json
```

## See Also

- [Plugins](../operations/plugins.md) — Plugin system documentation
- [Workspace Management](workspace.md) — Town-level setup commands
- [Rig Management](rigs.md) — Per-rig settings and configuration


========================================================================
# gt formula
# URL: /docs/cli-reference/formula
========================================================================

# gt formula

Manage workflow formulas — reusable templates that define multi-step molecules.

Formulas are TOML/JSON files that define workflows with steps, variables, and composition rules. They can be "poured" to create [molecules](../concepts/molecules.md) or "wisped" for ephemeral patrol cycles.

## Search Paths

Formulas are loaded from these locations (in priority order):

1. `.beads/formulas/` — Project-level formulas
2. `~/.beads/formulas/` — User-level formulas
3. `$GT_ROOT/.beads/formulas/` — Orchestrator-level (shared across all rigs)

When multiple formulas share the same name, the highest-priority path wins. This allows project-level overrides of shared formulas.

## Commands

### `gt formula list`

List all available formulas from all search paths.

```bash
gt formula list            # List all formulas
gt formula list --json     # JSON output for scripting
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

### `gt formula show <name>`

Display formula details including steps, variables, and composition rules.

```bash
gt formula show shiny              # Show the canonical workflow
gt formula show witness-patrol     # Show a patrol formula
gt formula show shiny --json       # Machine-readable output
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output as JSON |

**Output includes:**

- Formula metadata (name, type, description)
- Variables with defaults and constraints
- Steps with dependency relationships
- Composition rules (extends, aspects)

### `gt formula run <name>`

Execute a formula by pouring it into a new molecule and dispatching it. Variables can be passed as flags.

```bash
# Run the canonical shiny workflow
gt formula run shiny --var feature="Add notifications"

# Run on a specific PR
gt formula run shiny --pr=123

# Run in a specific rig
gt formula run security-audit --rig=beads

# Preview what would happen without executing
gt formula run release --dry-run

# Run default formula (from rig config)
gt formula run
```

**Options:**

| Flag | Description |
|------|-------------|
| `--pr <N>` | GitHub PR number to run the formula against |
| `--rig <name>` | Target a specific rig (default: current or gastown) |
| `--dry-run` | Preview execution without actually running |

If no formula name is provided, uses the default formula configured in the rig's `settings/config.json` under `workflow.default_formula`.

### `gt formula create <name>`

Create a new formula template in the project's `.beads/formulas/` directory.

```bash
gt formula create my-task                    # Create task formula (default)
gt formula create my-workflow --type=workflow # Multi-step workflow with dependencies
gt formula create nightly-check --type=patrol # Repeating patrol cycle
```

**Options:**

| Flag | Description |
|------|-------------|
| `--type <type>` | Formula type: `task`, `workflow`, or `patrol` (default: `task`) |

## Formula Types

| Type | Purpose | Example |
|------|---------|---------|
| `task` | Single-step, one-shot work | A quick bugfix or one-off task |
| `workflow` | Multi-step sequence with dependencies | `shiny` — design, implement, review, test, submit |
| `patrol` | Repeating cycle (used for wisps) | `mol-witness-patrol` — continuous health monitoring |

## How Formulas Relate to Molecules

- A **formula** is a blueprint (static template)
- A **[molecule](../concepts/molecules.md)** is a live instance created from a formula
- "Pouring" a formula creates a molecule with concrete variable bindings
- Multiple molecules can be poured from the same formula simultaneously
- "Wisping" creates an ephemeral patrol molecule that repeats its cycle

## Example: The `shiny` Formula

The canonical Gas Town workflow formula:

```toml
description = "Engineer in a Box - design before you code."
formula = "shiny"
type = "workflow"
version = 1

[[steps]]
id = "design"
title = "Design {{feature}}"

[[steps]]
id = "implement"
needs = ["design"]
title = "Implement {{feature}}"

[[steps]]
id = "review"
needs = ["implement"]
title = "Review implementation"

[[steps]]
id = "test"
needs = ["review"]
title = "Test {{feature}}"

[[steps]]
id = "submit"
needs = ["test"]
title = "Submit for merge"

[vars.feature]
description = "The feature being implemented"
required = true
```

See [Formula Workflow](../workflows/formula-workflow.md) for a step-by-step guide to creating and using formulas.

## See Also

- [Molecules](../concepts/molecules.md) — Running workflow instances
- [MEOW Stack](../concepts/meow-stack.md) — The layered workflow abstraction model
- [Formula Workflow](../workflows/formula-workflow.md) — End-to-end formula usage guide
- [Convoys](convoys.md) — Parallel multi-agent workflows (convoy-type formulas)


========================================================================
# gt dolt
# URL: /docs/cli-reference/dolt
========================================================================

# gt dolt

Manage the Dolt SQL server for Gas Town beads storage.

The Dolt server provides multi-client access to all rig databases, avoiding the single-writer limitation of embedded Dolt mode. Each rig (hq, gastown, beads, etc.) has its own database within the centralized data directory.

## Server Configuration

| Setting | Value |
|---------|-------|
| Port | 3307 (avoids conflict with MySQL on 3306) |
| User | root (default Dolt user, no password for localhost) |
| Data directory | `.dolt-data/` (contains all rig databases) |

## Commands

### Lifecycle

```bash
gt dolt start          # Start the Dolt server
gt dolt stop           # Stop the Dolt server
gt dolt status         # Show server status (running, port, connections)
gt dolt logs           # View Dolt server logs
```

### Database Management

```bash
gt dolt init           # Initialize and repair workspace configuration
gt dolt init-rig       # Initialize a new rig database
gt dolt list           # List available rig databases
gt dolt sql            # Open interactive Dolt SQL shell
```

### Migration & Recovery

```bash
gt dolt migrate        # Migrate existing databases to centralized data directory
gt dolt rollback       # Restore .beads directories from a migration backup
gt dolt recover        # Detect and recover from Dolt read-only state
gt dolt fix-metadata   # Update metadata.json in all rig .beads directories
```

### Remote Sync

```bash
gt dolt sync           # Push Dolt databases to DoltHub remotes
```

## Common Workflows

### Initial Setup

```bash
gt dolt init           # Set up centralized Dolt data directory
gt dolt start          # Start the server
gt dolt status         # Verify it's running
```

### Recovering from Read-Only State

If beads operations fail with "database is read-only" errors:

```bash
gt dolt recover        # Auto-detect and fix read-only state
gt dolt status         # Verify recovery
```

This typically happens when the Dolt server shuts down uncleanly or when multiple writers conflict. The `recover` command detects the issue and restarts the server with a clean lock state.

### Adding a New Rig

When a new rig is created, it needs its own database:

```bash
gt dolt init-rig       # Creates database for the new rig
gt dolt list           # Verify it appears in the database list
```

### Inspecting Data with SQL

The `sql` subcommand opens an interactive SQL shell connected to the running Dolt server. Useful for debugging beads state:

```bash
gt dolt sql
# Then in the SQL shell:
# USE gastowndocs;
# SELECT * FROM beads WHERE status = 'in_progress';
# SHOW TABLES;
```

### Migration from Embedded Mode

If upgrading from embedded Dolt (per-rig `.beads/` databases) to the centralized server:

```bash
gt dolt migrate        # Move databases to centralized .dolt-data/
gt dolt start          # Start the server against the new location
gt dolt fix-metadata   # Update rig metadata to point to the server
```

If migration goes wrong, roll back:

```bash
gt dolt rollback       # Restore from the pre-migration backup
```

## Architecture

```
$GT_ROOT/
├── .dolt-data/              ← Centralized data directory
│   ├── hq/                  ← HQ (town-level) database
│   ├── gastowndocs/         ← Per-rig database
│   ├── beads/               ← Per-rig database
│   └── ...
└── gt/
    └── gastowndocs/
        └── .beads/
            └── metadata.json  ← Points to Dolt server on port 3307
```

All `bd` commands route through the Dolt server automatically when it's running. If the server is down, commands fall back to embedded mode (single-writer).

## See Also

- [Beads](../concepts/beads.md) — The work tracking system backed by Dolt
- [Configuration](configuration.md) — Town-level settings
- [gt krc](krc.md) — TTL-based lifecycle for ephemeral data stored in Dolt
- [Monitoring](../operations/monitoring.md) — Operational monitoring patterns


========================================================================
# gt warrant
# URL: /docs/cli-reference/warrant
========================================================================

# gt warrant

Manage death warrants for agents that need termination.

Death warrants provide a controlled way to terminate agents that are stuck, unresponsive, or otherwise need forced termination. The warrant system ensures proper cleanup and work recovery before killing an agent.

## Warrant Lifecycle

```mermaid
flowchart LR
    Detect["Detect Problem"]
    File["File Warrant"]
    Triage["Boot Triage"]
    Recover["Recover Work"]
    Terminate["Terminate Agent"]
    Archive["Archive Warrant"]

    Detect --> File --> Triage --> Recover --> Terminate --> Archive
```

1. **Deacon/Witness files a warrant** with a reason (e.g., "agent stuck in loop for 30 minutes")
2. **Boot picks up the warrant** during its triage cycle
3. **Boot recovers in-progress work** — commits, pushes, or releases hooked beads
4. **Boot terminates the session** — kills the tmux session and cleans up worktree
5. **Warrant is marked as executed** and archived

Warrants are stored as JSON files in `~/gt/warrants/`.

## Commands

### `gt warrant file`

File a death warrant for an agent.

```bash
gt warrant file --agent gastowndocs/polecats/alpha --reason "Stuck in infinite loop"
gt warrant file --agent gastowndocs/polecats/bravo --reason "Zombie: session gone but state shows running"
```

**Options:**

| Flag | Description |
|------|-------------|
| `--agent <path>` | Full agent path (e.g., `gastowndocs/polecats/alpha`) |
| `--reason <text>` | Human-readable reason for the warrant |
| `--priority <level>` | Urgency: `normal` (default), `urgent` |

### `gt warrant list`

List pending warrants that haven't been executed yet.

```bash
gt warrant list
gt warrant list --all         # Include executed warrants
```

**Example output:**

```
WARRANT ID   AGENT                           REASON                    FILED
w-abc123     gastowndocs/polecats/alpha      Stuck in infinite loop    14:30
w-def456     gastowndocs/polecats/bravo      Zombie: session gone      14:35
```

### `gt warrant execute`

Execute a warrant — recover work, terminate the agent, clean up.

```bash
gt warrant execute <warrant-id>
gt warrant execute w-abc123
```

This is normally done by Boot automatically, but can be run manually for immediate intervention.

### `gt warrant cancel`

Cancel a pending warrant before it is executed.

```bash
gt warrant cancel <warrant-id>
```

Use this if the agent recovered on its own before Boot processed the warrant.

## When Warrants Are Filed

Warrants are typically filed automatically by [patrol agents](../concepts/patrol-cycles.md) when they detect:

| Condition | Filed By | Typical Threshold |
|-----------|----------|-------------------|
| **Zombie processes** | Deacon | Session gone, state still "running" |
| **Infinite loops** | Witness | Same state for 30+ minutes after nudge |
| **Resource exhaustion** | Witness | Excessive tokens without progress |
| **Stale sessions** | Deacon | Exceeds maximum age without cycling |
| **Unresponsive after nudge** | Witness | No response within 2 patrol cycles |

### Manual Warrants

You can also file warrants manually for agents that the automated system hasn't caught:

```bash
# Polecat that's clearly stuck
gt warrant file --agent gastowndocs/polecats/alpha --reason "Stuck retrying failed API call"

# Check pending warrants
gt warrant list

# If you need immediate action, execute directly
gt warrant execute w-abc123
```

## Work Recovery

Before terminating an agent, Boot attempts to recover any in-progress work:

1. **Check for uncommitted changes** — commit and push if possible
2. **Release hooked beads** — `bd release` returns beads to the ready queue
3. **Update molecule state** — mark current step as `failed` so the next agent knows
4. **Clean up worktree** — remove the polecat's sandbox directory
5. **Send notification** — mail the Witness that the warrant was executed

If work recovery fails (e.g., merge conflicts), Boot files a follow-up bead for manual intervention.

## See Also

- **[Lifecycle](../operations/lifecycle.md)** -- Agent lifecycle management including death warrants
- **[Boot](../agents/boot.md)** -- The triage agent that processes warrants
- **[Deacon](../agents/deacon.md)** -- Files warrants when agents need termination
- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- The monitoring pattern that detects warrant-worthy conditions
- **[Polecat Commands](polecat-commands.md)** -- Manual polecat management as an alternative to warrants


========================================================================
# gt patrol
# URL: /docs/cli-reference/patrol
========================================================================

# gt patrol

Manage patrol cycle digests.

Patrol cycles ([Deacon](../agents/deacon.md), [Witness](../agents/witness.md), [Refinery](../agents/refinery.md)) create ephemeral per-cycle digests to avoid JSONL pollution. This command aggregates them into concise daily summaries.

## Commands

### `gt patrol digest`

Aggregate patrol cycle digests into a daily summary bead.

```bash
gt patrol digest              # Aggregate today's patrol digests
gt patrol digest --yesterday  # Aggregate yesterday's patrol digests
gt patrol digest --dry-run    # Preview what would be aggregated
```

**Options:**

| Flag | Description |
|------|-------------|
| `--yesterday` | Aggregate yesterday's digests instead of today's |
| `--dry-run` | Preview what would be aggregated without writing |
| `--agent <type>` | Filter to a specific agent type (witness, refinery, deacon) |
| `--rig <name>` | Filter to a specific rig |

**Examples:**

```bash
# Aggregate all of today's patrol data
gt patrol digest

# See what a specific Witness reported
gt patrol digest --agent witness --rig myproject

# Preview without writing
gt patrol digest --dry-run

# Aggregate yesterday's data (useful for morning reviews)
gt patrol digest --yesterday
```

### `gt patrol start`

Request a fresh patrol cycle from all patrol agents.

```bash
gt patrol start               # Trigger patrol across all agents
gt patrol start --rig myapp   # Trigger patrol for a specific rig
```

This nudges patrol agents to run their check cycle immediately rather than waiting for the next scheduled tick.

## What Are Patrol Cycles?

Patrol is the core monitoring pattern in Gas Town. Three agent types run continuous [patrol molecules](../concepts/patrol-cycles.md):

| Agent | Interval | Patrol Focus |
|-------|----------|-------------|
| **[Witness](../agents/witness.md)** | 5 min | Monitors polecats — checks health, detects stalls, cleans up zombies |
| **[Refinery](../agents/refinery.md)** | 5 min | Monitors merge queue — processes MRs, rebases, merges to main |
| **[Deacon](../agents/deacon.md)** | 5 min | Monitors all agents — checks Witnesses, handles escalations, files death warrants |

Each patrol cycle generates ephemeral data (heartbeats, status checks, health reports). Without aggregation, this data accumulates rapidly. `gt patrol digest` compresses it into concise daily summaries.

## How Digest Aggregation Works

```
Per-cycle data:
  witness-patrol-14:00 → "3 polecats healthy, 0 stalled"
  witness-patrol-14:05 → "3 polecats healthy, 1 nudged"
  witness-patrol-14:10 → "2 polecats healthy, 1 escalated"
  ...

After gt patrol digest:
  patrol-daily-2026-02-13 → "Witness: 288 cycles, 285 healthy, 2 nudged, 1 escalated"
```

The per-cycle ephemeral data is removed after aggregation, keeping the beads database clean while preserving the audit trail.

## When to Run Digests

- **Automatically**: The Deacon can be configured to run digests at end of day
- **Manually**: Run `gt patrol digest` during morning reviews
- **On demand**: Use `--dry-run` to check patrol health without writing

## See Also

- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- Deep dive on the patrol pattern and discovery over tracking
- **[Monitoring](../operations/monitoring.md)** -- Operational monitoring patterns
- **[gt krc](krc.md)** -- TTL-based lifecycle for ephemeral patrol data
- **[Witness](../agents/witness.md)** -- Per-rig patrol agent
- **[Deacon](../agents/deacon.md)** -- Town-wide health coordinator
- **[Diagnostics](diagnostics.md)** -- Other diagnostic and observability commands


========================================================================
# gt krc
# URL: /docs/cli-reference/krc
========================================================================

# gt krc

Key Record Chronicle (KRC) manages TTL-based lifecycle for ephemeral data.

Operational data like patrol heartbeats, status checks, and health reports decays in forensic value over time. KRC provides configurable TTLs to automatically prune expired events, keeping the beads database lean.

## Commands

### `gt krc stats`

Show statistics about ephemeral data — counts by event type, age distribution, storage usage.

```bash
gt krc stats              # Human-readable summary
gt krc stats --json       # Machine-readable output
```

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

### `gt krc prune`

Remove expired events based on configured TTLs. Events are removed from both `.events.jsonl` and `.feed.jsonl`. The operation is atomic (uses temp files and rename).

```bash
gt krc prune              # Remove expired events
gt krc prune --dry-run    # Preview what would be pruned (no changes)
gt krc prune --auto       # Daemon mode: only prune if PruneInterval has elapsed
```

**Options:**

| Flag | Description |
|------|-------------|
| `--dry-run` | Preview changes without modifying files |
| `--auto` | Daemon mode: only prune if PruneInterval has elapsed since last run |

### `gt krc decay`

Show forensic value decay report — visualize how data value decreases over time.

```bash
gt krc decay              # Human-readable decay report
gt krc decay --json       # Machine-readable output
```

Each event type follows one of four decay curves:

| Curve | Behavior | Typical Events |
|-------|----------|---------------|
| `rapid` | Value drops quickly | Heartbeats, pings |
| `steady` | Linear decay over time | Session events, patrol cycles |
| `slow` | Value persists longer | Errors, escalations |
| `flat` | Full value until near TTL | Audit events, death warrants |

Events with low forensic scores are candidates for aggressive pruning.

**Options:**

| Flag | Description |
|------|-------------|
| `--json` | Output in JSON format |

### `gt krc config`

View or modify TTL configuration for event types.

```bash
gt krc config                      # Show current TTL settings
gt krc config set patrol_* 12h     # Set TTL for patrol events to 12 hours
gt krc config set heartbeat_* 6h   # Set TTL for heartbeat events to 6 hours
gt krc config set default 3d       # Set default TTL to 3 days
gt krc config reset                # Reset to default configuration
```

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `set <pattern> <ttl>` | Set TTL for event type matching glob pattern |
| `reset` | Reset all TTLs to default values |

TTL durations use Go-style shorthand: `6h` (hours), `3d` (days), `1w` (weeks).

### `gt krc auto-prune-status`

Show the auto-prune scheduling state — whether automatic pruning is active and when it last ran.

```bash
gt krc auto-prune-status
```

## Data Levels

KRC manages **Level 0** (ephemeral) data as defined in the Dolt storage design:

| Level | Examples | Default TTL | Forensic Value |
|-------|----------|-------------|----------------|
| 0 (Ephemeral) | Patrol heartbeats, status checks, health pings | Hours | Decays rapidly |
| 1 (Operational) | Work assignments, progress updates | Days-weeks | Moderate |
| 2 (Archival) | Completed beads, audit trails | Permanent | High |

KRC only manages Level 0. Higher levels are managed by Dolt directly.

## How Auto-Pruning Works

When configured, KRC can run pruning automatically at a set interval:

1. **PruneInterval** defines how often pruning runs (e.g., every 6 hours)
2. The Deacon's patrol cycle triggers `gt krc prune --auto`
3. The `--auto` flag checks whether the interval has elapsed since the last prune
4. If the interval has passed, expired events are pruned atomically
5. Use `gt krc auto-prune-status` to verify the schedule is active

This keeps the beads database lean without manual intervention.

## Relationship to Patrol

Patrol agents are the primary producers of Level 0 data. Each 5-minute patrol cycle generates heartbeats, status checks, and health reports. Without KRC pruning, this data accumulates rapidly — hundreds of events per day per rig.

The typical flow:

```
Patrol agents → generate ephemeral events → KRC prunes expired → gt patrol digest aggregates
```

Use `gt krc stats` to monitor accumulation rates and tune TTLs accordingly.

## See Also

- [gt patrol](patrol.md) — Patrol digest aggregation
- [gt dolt](dolt.md) — Dolt SQL server management
- [Patrol Cycles](../concepts/patrol-cycles.md) — The patrol monitoring pattern
- [Monitoring](../operations/monitoring.md) — Operational monitoring patterns


========================================================================
# Agents
# URL: /docs/agents
========================================================================

# Agents

Gas Town's agent hierarchy is a supervisor tree inspired by Erlang/OTP. Each role has a well-defined scope, lifecycle, and set of responsibilities -- ensuring reliable, self-healing operation from a single worker to 30 concurrent agents.

---

## The Agent Roster

| Agent | Tagline | Scope | Lifecycle | Count |
|-------|---------|-------|-----------|-------|
| [Mayor](mayor.md) | Global Coordinator | Town | Persistent | 1 |
| [Deacon](deacon.md) | Town-Level Watchdog | Town | Persistent | 1 |
| [Witness](witness.md) | Per-Rig Health Monitor | Rig | Persistent | 1 per rig |
| [Refinery](refinery.md) | Merge Queue Processor | Rig | Persistent | 1 per rig |
| [Polecats](polecats.md) | Ephemeral Workers | Rig | Ephemeral | Many per rig |
| [Dogs](dogs.md) | Infrastructure Workers | Town | Reusable | As needed |
| [Crew](crew.md) | Human Workspaces | Rig | Persistent | Named members |
| [Boot](boot.md) | Daemon Watchdog Dog | Town | Per-tick | 1 |

## Hierarchy

```mermaid
graph TD
    Human["Human / Overseer"]
    Mayor["Mayor<br/>Global Coordinator"]
    Deacon["Deacon<br/>Town Watchdog"]
    Daemon["Daemon<br/>(Go process)"]
    Boot["Boot<br/>Triage Dog"]
    Dogs["Dogs<br/>Infrastructure"]

    Human --> Mayor
    Daemon -->|heartbeat| Deacon
    Mayor -->|strategic direction| Deacon
    Deacon -->|manages| Boot
    Deacon -->|manages| Dogs

    subgraph "Rig: myproject"
        W1["Witness"]
        R1["Refinery"]
        P1["Polecats"]
        C1["Crew"]
    end

    subgraph "Rig: docs"
        W2["Witness"]
        R2["Refinery"]
        P2["Polecats"]
    end

    Deacon -->|monitors| W1
    Deacon -->|monitors| W2
    W1 -->|supervises| P1
    W2 -->|supervises| P2
    P1 -->|submit MRs| R1
    P2 -->|submit MRs| R2
```

## Comparison Table

### Lifecycle and Persistence

| Property | Mayor | Deacon | Witness | Refinery | Polecats | Dogs | Crew | Boot |
|----------|-------|--------|---------|----------|----------|------|------|------|
| **Lifecycle** | Persistent | Persistent | Persistent | Persistent | Ephemeral | Reusable | Persistent | Per-tick |
| **Session type** | Long-running | Long-running | Long-running | Long-running | Single-task | Multi-task | User-managed | Fresh each run |
| **Survives restart** | Yes | Yes | Yes | Yes | No | Yes | Yes | N/A |
| **Patrol cycle** | On-demand | 5 min | 5 min | 5 min | None | None | None | Each daemon tick |

### Scope and Multiplicity

| Property | Mayor | Deacon | Witness | Refinery | Polecats | Dogs | Crew | Boot |
|----------|-------|--------|---------|----------|----------|------|------|------|
| **Scope** | Town | Town | Per-rig | Per-rig | Per-rig | Cross-rig | Per-rig | Town |
| **Instance count** | 1 | 1 | 1 per rig | 1 per rig | Many | As needed | Named | 1 |
| **Works on code** | Read-only | No | No | Merge only | Yes | Infra only | Yes | No |
| **Has git identity** | Yes | No | No | Yes | Yes | No | Yes | No |

### Communication

| Property | Mayor | Deacon | Witness | Refinery | Polecats | Dogs | Crew | Boot |
|----------|-------|--------|---------|----------|----------|------|------|------|
| **Receives mail** | Yes | Yes | Yes | Yes | Yes | No | Yes | No |
| **Receives nudges** | Yes | Yes | Yes | Yes | Yes | No | Yes | No |
| **Sends escalations** | To human | To Mayor | To Deacon | To Witness | To Witness | To Deacon | N/A | To Deacon |
| **Has mailbox** | Yes | Yes | Yes | Yes | Yes | No | Yes | No |

## Supervision Chain

The monitoring chain ensures no agent runs unsupervised:

```
Daemon --[heartbeat]--> Deacon
Deacon --[monitors]---> Witnesses (all rigs)
Witness --[watches]---> Polecats (in its rig)
Witness --[watches]---> Refinery (in its rig)
Mayor  --[strategy]---> Deacon
```

When something goes wrong, escalations flow upward:

```
Polecat (stuck)
  --> Witness detects stall
    --> Witness nudges polecat
      --> If still stuck: Witness escalates to Deacon
        --> Deacon escalates to Mayor
          --> Mayor escalates to Human/Overseer
```

## Role Separation

Each role has clear, non-overlapping responsibilities:

| Role | Does | Does NOT |
|------|------|----------|
| **Mayor** | Coordinate strategy, assign work | Monitor health |
| **Deacon** | Monitor health, manage lifecycle | Assign features |
| **Witness** | Watch polecats in its rig | Process merges |
| **Refinery** | Merge code to main | Write features |
| **Polecat** | Implement features | Monitor others |
| **Dog** | Infrastructure and cleanup tasks | Feature work |
| **Crew** | Human development workspace | Automated tasks |
| **Boot** | Triage system state | Direct action |

:::tip[Choosing the Right Agent]

- Need to **build a feature**? That is a [Polecat](polecats.md).
- Need to **merge code**? That is the [Refinery](refinery.md).
- Need to **clean up infrastructure**? That is a [Dog](dogs.md).
- Need to **monitor health**? That is the [Witness](witness.md) (per-rig) or [Deacon](deacon.md) (town-wide).
- Need to **coordinate work**? That is the [Mayor](mayor.md).
- Need a **human workspace**? That is [Crew](crew.md).

:::

## Key Concepts

The agent hierarchy depends on several core Gas Town concepts:

- **[GUPP](../concepts/gupp.md)** (Gas Town Universal Propulsion Principle) -- Work on a hook is an immediate assignment. No confirmation needed. This is what makes polecats fire instantly when spawned.
- **[Hooks](../concepts/hooks.md)** -- The mechanism for attaching work to an agent. Every polecat checks its hook on startup.
- **[Molecules](../concepts/molecules.md)** -- Workflow templates that define step-by-step execution plans. Every agent role has a formula (e.g., `mol-polecat-work`, `mol-witness-patrol`).
- **[Beads](../concepts/beads.md)** -- The issue tracking system all agents use to create, update, and close work items.

## Common Patterns

### Checking Agent Health Across the System

```bash
gt doctor                    # Full system health check
gt rig status <rig>          # Per-rig agent status
gt polecat list              # All polecats in current rig
gt deacon status             # Deacon health
```

### Tracing the Escalation Chain

When something goes wrong, follow the escalation path:

1. Check the polecat: `gt polecat status <name>`
2. Check the Witness: Did it detect the problem?
3. Check the Deacon: `gt deacon status` -- Did the Witness escalate?
4. Check Mayor mail: `gt mail inbox` (from Mayor session)

### Verifying the Supervision Chain

```bash
# Is the Daemon running? (sends heartbeats)
gt daemon status

# Is Boot triaging? (wakes the Deacon)
# Check the log at ~/gt/deacon/dogs/boot/triage-log.jsonl

# Is the Deacon awake? (monitors Witnesses)
gt deacon status

# Are Witnesses patrolling? (monitor polecats)
gt rig status <rig>
```

## Troubleshooting

### No Agents Are Running

```bash
gt up                        # Bring up all services
gt doctor --fix              # Auto-fix common issues
```

### Agent Is Stuck or Unresponsive

```bash
gt nudge <agent> "status?"   # Send a nudge
gt polecat nuke <name>       # Last resort: destroy polecat
```

### Escalation Chain Is Broken

If agents are not escalating properly, verify the chain from the bottom up: Daemon -> Boot -> Deacon -> Witnesses. A broken link anywhere stops escalations from flowing.


========================================================================
# Mayor -- Global Coordinator
# URL: /docs/agents/mayor
========================================================================

# Mayor -- Global Coordinator

> The Mayor is the brain of Gas Town. It receives instructions from the human overseer, decomposes them into actionable work, and orchestrates the entire agent hierarchy to deliver results.

---

## Overview

The Mayor is the primary human-facing agent in Gas Town. When you run `gt mayor attach`, you are talking directly to the Mayor. It understands your goals, creates issues (beads), bundles them into convoys, assigns work to rigs, and tracks progress through completion. The Mayor is the only agent that bridges the gap between natural language instructions and the structured work system.

## Key Characteristics

| Property | Value |
|----------|-------|
| **Scope** | Town-level (all rigs) |
| **Lifecycle** | Persistent |
| **Instance count** | 1 per town |
| **Session type** | Long-running Claude Code session |
| **Patrol cycle** | On-demand (not periodic) |
| **Location** | `~/gt/mayor/` and `~/gt/<rig>/mayor/rig/` |
| **Git identity** | Yes |
| **Mailbox** | Yes |

## Responsibilities

### 1. Receive Human Instructions

The Mayor is your interface to Gas Town. You describe what you want built, fixed, or changed in natural language:

```
> Fix the 5 failing tests in auth and add input validation to registration.
```

The Mayor parses this into discrete work items.

### 2. Create Issues and Convoys

For each piece of work, the Mayor creates a bead (issue) in the tracking system, then bundles related beads into a convoy for batch tracking:

```bash
# The Mayor does this internally:
bd create --title "Fix auth test: login_expired" --type bug --priority 1
bd create --title "Add email validation to registration" --type feature --priority 2
gt convoy create "Auth Improvements" gt-a1b2c gt-d3e4f
```

### 3. Assign Work to Rigs

The Mayor uses `gt sling` to assign beads to rigs, which triggers polecat spawning:

```bash
gt sling gt-a1b2c myproject    # Assigns issue to rig, spawns polecat
gt sling gt-d3e4f myproject    # Another issue, another polecat
```

### 4. Route Escalations

When agents encounter problems they cannot solve, escalations flow upward to the Mayor. The Mayor either resolves them, reassigns the work, or escalates to the human overseer.

### 5. Track Convoy Progress

The Mayor monitors convoy completion, ensuring all assigned work reaches the finish line:

```bash
gt convoy list
gt convoy show hq-cv-001
```

### 6. Strategic Direction to Deacon

The Mayor provides high-level direction to the Deacon for health monitoring priorities and lifecycle decisions.

## Mayor Workflow (MEOW)

The Mayor Execution and Orchestration Workflow (MEOW) is the standard operating procedure for the Mayor agent:

```mermaid
flowchart TD
    Start["Receive Instructions"]
    Analyze["Analyze & Decompose"]
    Create["Create Beads"]
    Convoy["Bundle into Convoy"]
    Assign["Sling to Rigs"]
    Monitor["Monitor Progress"]
    Escalation{"Escalation?"}
    Resolve["Resolve or Reassign"]
    Complete{"All Done?"}
    Report["Report to Overseer"]

    Start --> Analyze
    Analyze --> Create
    Create --> Convoy
    Convoy --> Assign
    Assign --> Monitor
    Monitor --> Escalation
    Escalation -->|Yes| Resolve
    Resolve --> Monitor
    Escalation -->|No| Complete
    Complete -->|No| Monitor
    Complete -->|Yes| Report
```

**MEOW Steps:**

1. **Receive** -- Accept instructions from human or mail
2. **Analyze** -- Break down into discrete, parallelizable tasks
3. **Create** -- Generate beads with clear descriptions and acceptance criteria
4. **Bundle** -- Group related beads into a convoy
5. **Assign** -- Sling beads to target rigs
6. **Monitor** -- Track convoy progress, handle escalations
7. **Report** -- Summarize results to overseer

## Commands

### Primary Commands

| Command | Description |
|---------|-------------|
| `gt mayor attach` | Open an interactive session with the Mayor |
| `gt mayor start` | Start the Mayor session in the background |
| `gt mayor stop` | Stop the Mayor session |
| `gt mayor status` | Check if the Mayor is running and view session info |

### Commands the Mayor Uses

| Command | Purpose |
|---------|---------|
| `gt sling <bead> <rig>` | Assign work to a rig |
| `gt convoy create` | Bundle beads into a convoy |
| `gt convoy list` | View convoy status |
| `gt mail inbox` | Check incoming messages |
| `gt mail send` | Send messages to agents |
| `gt escalate` | Escalate issues to overseer |
| `gt rig list` | View all rigs and their status |
| `gt feed` | View the activity feed |
| `gt broadcast` | Send message to all agents |

## Context Files

The Mayor maintains state through several context files:

| File | Purpose |
|------|---------|
| `~/gt/mayor/town.json` | Town metadata -- rig list, global config |
| `~/gt/mayor/rigs.json` | Rig status and configuration summary |
| `~/gt/mayor/overseer.json` | Overseer preferences and escalation rules |
| `~/gt/mayor/CLAUDE.md` | Mayor agent context and instructions |
| `~/gt/<rig>/mayor/rig/CLAUDE.md` | Per-rig Mayor context |

## Interaction Diagram

```mermaid
sequenceDiagram
    participant H as Human
    participant M as Mayor
    participant D as Deacon
    participant P as Polecat
    participant R as Refinery

    H->>M: "Fix auth tests and add validation"
    M->>M: Decompose into beads
    M->>M: Create convoy
    M->>P: gt sling gt-a1 myproject
    M->>P: gt sling gt-b2 myproject
    P->>P: Work on task
    P->>R: gt done (submit MR)
    R->>R: Rebase + validate + merge
    P-->>M: Escalation (if blocked)
    M-->>H: Escalation (if unresolvable)
    M->>H: Convoy complete report
```

## Tips and Best Practices

:::tip[Be Specific with Instructions]

The more specific your instructions, the better the Mayor decomposes them. Include acceptance criteria, edge cases, and constraints when possible.

:::

:::tip[Check Convoy Progress]

Use `gt convoy list` and `gt convoy show` regularly to track batch progress. The Mayor monitors automatically, but you can intervene at any time.

:::

:::tip[Use Escalations]

If you see something the Mayor should know about, use `gt mail send mayor "..."` to communicate directly. The Mayor checks its inbox as part of its workflow.

:::

:::warning[One Mayor Per Town]

Gas Town supports exactly one Mayor per town. Running multiple Mayor sessions will cause coordination conflicts.

:::

## Common Patterns

### Assigning a Batch of Related Work

The Mayor frequently needs to assign multiple related tasks. The pattern is: create beads, bundle into a convoy, sling to rigs.

```bash
# Create individual issues
bd create --title "Fix auth token expiry" --type bug --priority 1
bd create --title "Add rate limiting to API" --type feature --priority 2

# Bundle into a convoy for tracking
gt convoy create gt-abc gt-def

# Sling to target rig (auto-spawns polecats)
gt sling gt-abc myproject
gt sling gt-def myproject

# Monitor batch progress
gt convoy list
```

### Handling an Escalation

When a polecat escalates, the Mayor receives mail. The typical flow:

```bash
gt mail inbox                            # Check for escalations
gt mail read <id>                        # Read the details
# Decide: reassign, provide guidance, or escalate to human
gt mail send <rig>/witness -s "RE: ..." -m "Guidance: ..."
```

### Dispatching Cross-Rig Work

When work spans multiple rigs, the Mayor coordinates:

```bash
gt sling gt-frontend frontend-rig
gt sling gt-backend backend-rig
gt convoy create gt-frontend gt-backend  # Track together
```

## Troubleshooting

### Mayor Session Is Unresponsive

```bash
gt mayor status              # Check if session is alive
gt nudge mayor "status?"     # Send a nudge
gt mayor stop && gt mayor start  # Restart if needed
```

### Convoy Shows Incomplete But Work Is Done

Check if all beads in the convoy are closed. A convoy auto-closes only when every tracked bead reaches a terminal state:

```bash
gt convoy show <convoy-id>   # See which beads are still open
bd show <open-bead-id>       # Check why it is still open
```

### Polecats Not Being Spawned After Sling

Verify the rig is started and the Witness is running:

```bash
gt rig status <rig>          # Check rig health
gt rig start <rig>           # Start rig agents if needed
```

## See Also

- **[MEOW Stack](../concepts/meow-stack.md)** -- The Mayor's orchestration workflow
- **[Convoys](../concepts/convoys.md)** -- Batch tracking for related work
- **[Beads](../concepts/beads.md)** -- Issue tracking system the Mayor uses
- **[Deacon](deacon.md)** -- Receives strategic direction from the Mayor
- **[Polecats](polecats.md)** -- Workers the Mayor assigns tasks to
- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- Periodic monitoring cadence used by all persistent agents
- **[Handoff Ceremony](../workflows/handoff-ceremony.md)** -- Session cycling protocol for long-running agents
- **[Agent Commands](../cli-reference/agents.md)** -- CLI commands for Mayor lifecycle and operations


========================================================================
# Deacon -- Town-Level Watchdog
# URL: /docs/agents/deacon
========================================================================

# Deacon -- Town-Level Watchdog

> The Deacon is the immune system of Gas Town. It monitors the health of every agent, manages lifecycles, and ensures the system self-heals when things go wrong.

---

## Overview

The Deacon is a persistent, town-level agent responsible for health monitoring and lifecycle management across the entire Gas Town installation. It receives periodic heartbeats from the Daemon, monitors all Witnesses, manages Dogs for cross-rig infrastructure work, and performs boot triage when the system starts up. If the Mayor is the brain, the Deacon is the nervous system -- always watching, always ready to respond.

## Key Characteristics

| Property | Value |
|----------|-------|
| **Scope** | Town-level (all rigs) |
| **Lifecycle** | Persistent |
| **Instance count** | 1 per town |
| **Session type** | Long-running Claude Code session |
| **Patrol cycle** | 5 minutes |
| **Location** | `~/gt/deacon/` |
| **Git identity** | No |
| **Mailbox** | Yes |

## Responsibilities

### 1. Receive Daemon Heartbeats

The Daemon (a Go process) sends heartbeats to the Deacon on a 3-minute interval. These heartbeats trigger the Deacon's patrol cycle and carry system state information.

```mermaid
sequenceDiagram
    participant Dm as Daemon
    participant Dc as Deacon
    participant W as Witnesses

    loop Every 3 minutes
        Dm->>Dc: Heartbeat + system state
        Dc->>Dc: Process patrol cycle
        Dc->>W: Check all Witnesses
    end
```

### 2. Monitor All Witnesses

The Deacon supervises every Witness in every rig. If a Witness becomes unresponsive or dies, the Deacon restarts it:

| Witness State | Deacon Action |
|---------------|---------------|
| Healthy | No action |
| Slow to respond | Nudge |
| Unresponsive | Restart session |
| Dead | Spawn new Witness |

### 3. Manage Dogs

Dogs are reusable infrastructure workers managed by the Deacon for cross-rig tasks. The Deacon spawns, assigns, and monitors Dogs:

```bash
gt dog list          # List active dogs
gt dog add <name>    # Create a new dog
```

### 4. Patrol Cycle

Every 5 minutes, the Deacon runs a patrol cycle:

```mermaid
flowchart TD
    Start["Patrol Tick"]
    Check["Check All Witnesses"]
    Stale{"Any Stale?"}
    Nudge["Nudge Stale Witness"]
    Response{"Responded?"}
    Restart["Restart Witness"]
    Hooks["Check Stale Hooks"]
    Orphans["Cleanup Orphans"]
    Zombies["Scan for Zombies"]
    Done["Patrol Complete"]

    Start --> Check
    Check --> Stale
    Stale -->|Yes| Nudge
    Nudge --> Response
    Response -->|No| Restart
    Response -->|Yes| Hooks
    Stale -->|No| Hooks
    Restart --> Hooks
    Hooks --> Orphans
    Orphans --> Zombies
    Zombies --> Done
```

### 5. Boot Triage Process

When the system starts up (or after a crash), the Deacon runs boot triage to assess and recover state:

```mermaid
flowchart LR
    Observe["Observe<br/>System State"]
    Decide["Decide<br/>Actions Needed"]
    Act["Act<br/>Execute Recovery"]

    Observe --> Decide --> Act
```

**Boot triage steps:**

1. **Observe** -- Inventory all rigs, check agent sessions, scan for orphaned work
2. **Decide** -- Determine which agents need starting, which work needs reassignment
3. **Act** -- Start Witnesses, restart stalled agents, re-sling orphaned work

### 6. Handle Escalations from Witnesses

When a Witness encounters a problem it cannot resolve (e.g., a rig-level failure), it escalates to the Deacon. The Deacon either handles it directly or escalates further to the Mayor.

## Commands

### Deacon Management

| Command | Description |
|---------|-------------|
| `gt deacon start` | Start the Deacon session |
| `gt deacon stop` | Stop the Deacon session |
| `gt deacon status` | Check Deacon health and patrol state |

### Maintenance Commands

| Command | Description |
|---------|-------------|
| `gt deacon stale-hooks` | Find and report hooks with no active agent |
| `gt deacon cleanup-orphans` | Remove orphaned worktrees and temp files |
| `gt deacon zombie-scan` | Detect and report zombie agent sessions |

## Configuration

The Deacon's behavior is configured through town-level settings:

| Setting | Default | Description |
|---------|---------|-------------|
| Patrol interval | 5 min | Time between patrol cycles |
| Heartbeat timeout | 10 min | Time before Daemon heartbeat considered missed |
| Witness restart threshold | 3 missed patrols | Missed patrols before restarting a Witness |
| Zombie age threshold | 30 min | Idle time before a session is considered zombie |

## Deacon State

The Deacon tracks the following state:

| State File | Purpose |
|------------|---------|
| `~/gt/deacon/state.json` | Current patrol state and agent inventory |
| `~/gt/deacon/dogs/` | Dog workspace directory |
| `~/gt/deacon/CLAUDE.md` | Deacon agent context and instructions |

## Interaction Diagram

```mermaid
graph TD
    Daemon["Daemon<br/>(Go process)"]
    Deacon["Deacon"]
    Mayor["Mayor"]
    Boot["Boot Dog"]
    Dogs["Dogs"]
    W1["Witness<br/>Rig 1"]
    W2["Witness<br/>Rig 2"]
    W3["Witness<br/>Rig 3"]

    Daemon -->|heartbeat 3m| Deacon
    Mayor -->|strategic direction| Deacon
    Deacon -->|manages| Boot
    Deacon -->|manages| Dogs
    Deacon -->|monitors| W1
    Deacon -->|monitors| W2
    Deacon -->|monitors| W3
    W1 -->|escalates| Deacon
    W2 -->|escalates| Deacon
    W3 -->|escalates| Deacon
    Deacon -->|escalates| Mayor
```

## Tips and Best Practices

:::tip[Check Deacon Status After Restarts]

After restarting the Daemon or recovering from a crash, run `gt deacon status` to verify the Deacon has completed boot triage and all Witnesses are healthy.

:::

:::tip[Use Zombie Scan Proactively]

Run `gt deacon zombie-scan` periodically if you suspect stuck sessions. The Deacon does this automatically, but manual scans give you immediate visibility.

:::

:::warning[Do Not Run Multiple Deacons]

Like the Mayor, the Deacon is a singleton. Running multiple instances will cause conflicting health decisions and potential data corruption.

:::

:::info[Deacon vs Mayor]

The Deacon handles **operational health** (is everything running?). The Mayor handles **strategic coordination** (what should we build?). They communicate but have non-overlapping responsibilities.

:::

## Common Patterns

### Idle Town Protocol

When no active work exists across rigs, the Deacon enters idle monitoring mode. It skips health nudges for docked rigs and uses exponential backoff to reduce resource usage:

```
Active rigs with polecats → Full patrol cycle
No active work anywhere → Idle monitoring (reduced frequency)
```

This prevents unnecessary Witness nudges and resource consumption when the system is quiet.

### Orphan Process Cleanup

The Deacon detects and cleans up orphaned Claude subagent processes (those with TTY = "?") that were left behind by crashed sessions. This happens automatically during the patrol cycle.

### Gate Evaluation and Dispatch

The Deacon checks timer gates for expiration, closes elapsed gates, and dispatches molecules that were blocked on those gates to available polecats:

```bash
bd gate list                 # See open gates
# Deacon auto-closes expired timer gates
# Deacon auto-dispatches unblocked molecules
```

### Second-Order Monitoring

The "who watches the watchers" problem is solved by Witness pings. Each Witness sends a `WITNESS_PING` to the Deacon during its patrol. If the Deacon stops receiving pings, it knows a Witness has died:

```
Witness patrol → sends WITNESS_PING → Deacon archives it
No ping for 3+ cycles → Deacon restarts the Witness
No ping for 5+ cycles → Deacon escalates to Mayor
```

## Troubleshooting

### Deacon Is Not Starting

The Daemon spawns Boot, and Boot wakes the Deacon. Verify the chain:

```bash
gt daemon status             # Is the Daemon running?
# Check Boot's triage log:
# ~/gt/deacon/dogs/boot/triage-log.jsonl
gt deacon status             # Check Deacon state
```

### Witnesses Are Not Being Monitored

Verify the Deacon is receiving `WITNESS_PING` messages. If Witnesses are running but not pinging, they may be stuck in their own patrol cycle:

```bash
gt deacon status             # Check last patrol time
gt rig status <rig>          # Check Witness state per rig
```

### Dogs Are Not Being Dispatched

The Deacon maintains a minimum pool of idle dogs. If dogs are not spawning:

```bash
gt dog list                  # Check current dog pool
gt deacon status             # Verify Deacon patrol is running
```

### Zombie Polecats Accumulating

The Deacon detects zombies during its patrol but only files "death warrants" -- Boot handles the actual process cleanup. If zombies are accumulating, check that Boot is running:

```bash
gt daemon status             # Boot runs on daemon ticks
gt polecat list              # See zombie states
```

## See Also

- **[Boot](boot.md)** -- The Daemon's watchdog that wakes the Deacon
- **[Dogs](dogs.md)** -- Infrastructure workers managed by the Deacon
- **[Witness](witness.md)** -- Per-rig monitors that the Deacon supervises
- **[Mayor](mayor.md)** -- Provides strategic direction to the Deacon
- **[Gates](../concepts/gates.md)** -- Async coordination primitives the Deacon evaluates
- **[Molecules](../concepts/molecules.md)** -- Workflows the Deacon dispatches when gates open
- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- Periodic monitoring cadence the Deacon follows
- **[Escalations](../operations/escalations.md)** -- How problems are routed up from Witnesses to the Deacon
- **[Lifecycle](../operations/lifecycle.md)** -- Agent lifecycle management the Deacon oversees


========================================================================
# Witness -- Per-Rig Health Monitor
# URL: /docs/agents/witness
========================================================================

# Witness -- Per-Rig Health Monitor

> The Witness is the local guardian of a rig. It watches every polecat in its domain, detects problems before they spread, and keeps the workspace clean.

---

## Overview

Each rig in Gas Town has exactly one Witness -- a persistent agent whose sole job is to supervise the polecats working in that rig. The Witness detects stalled workers, nudges unresponsive sessions, cleans up zombies, and nukes sandboxes when polecats complete their work. It is the first line of defense against runaway or stuck agents.

## Key Characteristics

| Property | Value |
|----------|-------|
| **Scope** | Per-rig |
| **Lifecycle** | Persistent |
| **Instance count** | 1 per rig |
| **Session type** | Long-running Claude Code session |
| **Patrol cycle** | 5 minutes |
| **Location** | `~/gt/<rig>/witness/` |
| **Git identity** | No |
| **Mailbox** | Yes |

## Responsibilities

### 1. Supervise Polecats

The Witness monitors all polecats in its rig, tracking their state and activity:

| Polecat State | Witness Action |
|---------------|----------------|
| **Working** | No action -- healthy |
| **Stalled** | Nudge to resume |
| **Unresponsive** | Escalate, then nuke |
| **Zombie** | Clean up immediately |
| **Completed** | Nuke sandbox |

### 2. Detect Stalled Polecats

A polecat is considered stalled when it has not produced output or made progress within a threshold period. The Witness detects stalls through:

- Session activity monitoring (last output timestamp)
- Git activity monitoring (last commit timestamp)
- Process state checks (CPU/memory usage)

### 3. Nudge Unresponsive Sessions

When a polecat appears stalled, the Witness sends a nudge -- a synchronous message injected into the agent's session:

```bash
gt nudge <polecat-name> "Are you stuck? Check your current task and report status."
```

If the polecat responds and resumes work, no further action is needed. If it remains unresponsive after nudging, the Witness escalates.

### 4. Clean Up Zombies

Zombie polecats are sessions that have crashed or exited without completing the `gt done` workflow. The Witness detects and cleans these up:

```mermaid
flowchart TD
    Detect["Detect Zombie"]
    Check["Check for Unsaved Work"]
    Save{"Work Recoverable?"}
    Recover["Recover Work to Hook"]
    Nuke["Nuke Sandbox"]
    Report["Report to Deacon"]

    Detect --> Check
    Check --> Save
    Save -->|Yes| Recover
    Save -->|No| Nuke
    Recover --> Nuke
    Nuke --> Report
```

### 5. Nuke Sandboxes on Completion

When a polecat finishes its work and runs `gt done`, the Witness cleans up the polecat's sandbox (worktree directory). This prevents disk space accumulation from completed workers.

### 6. Patrol Cycle

Every 5 minutes, the Witness runs a patrol:

```mermaid
flowchart TD
    Start["Patrol Tick"]
    List["List All Polecats"]
    Each{"For Each Polecat"}
    Active["Check Activity"]
    Stale{"Stale?"}
    Nudge["Nudge Session"]
    Responded{"Responded?"}
    Escalate["Escalate to Deacon"]
    Zombie{"Zombie?"}
    Clean["Clean Up Zombie"]
    Next["Next Polecat"]
    Refinery["Check Refinery Health"]
    Done["Patrol Complete"]

    Start --> List
    List --> Each
    Each --> Active
    Active --> Stale
    Stale -->|Yes| Nudge
    Nudge --> Responded
    Responded -->|No| Escalate
    Responded -->|Yes| Next
    Stale -->|No| Zombie
    Zombie -->|Yes| Clean
    Clean --> Next
    Zombie -->|No| Next
    Escalate --> Next
    Next --> Each
    Each -->|All checked| Refinery
    Refinery --> Done
```

## Commands

### Witness Management

| Command | Description |
|---------|-------------|
| `gt witness start` | Start the Witness for the current rig |
| `gt witness stop` | Stop the Witness |
| `gt witness status` | Check Witness session status |
| `gt witness attach` | Attach to the Witness session |
| `gt witness restart` | Restart the Witness session |

### Polecat Commands (Witness-Monitored)

The following commands interact with Witness-monitored resources:

| Command | Description |
|---------|-------------|
| `gt polecat list` | List polecats in the current rig (Witness-tracked) |
| `gt polecat status <name>` | Check a specific polecat's status |
| `gt polecat nuke <name>` | Manually destroy a polecat sandbox |
| `gt polecat gc` | Garbage collect completed polecat directories |
| `gt polecat stale` | List polecats that appear stalled |

## Configuration

Witness behavior is configured per-rig:

| Setting | Default | Description |
|---------|---------|-------------|
| Patrol interval | 5 min | Time between patrol cycles |
| Stall threshold | 15 min | Idle time before a polecat is considered stalled |
| Nudge timeout | 5 min | Time to wait for nudge response |
| Max nudges | 2 | Nudges before escalating |
| Zombie threshold | 30 min | Time before a dead session is classified as zombie |

## Interaction Diagram

```mermaid
graph TD
    Deacon["Deacon"]
    Witness["Witness"]
    P1["Polecat: Toast"]
    P2["Polecat: Alpha"]
    P3["Polecat: Bravo"]
    Refinery["Refinery"]

    Deacon -->|monitors| Witness
    Witness -->|watches| P1
    Witness -->|watches| P2
    Witness -->|watches| P3
    Witness -->|watches| Refinery
    P1 -->|submits MR| Refinery
    P2 -->|submits MR| Refinery
    P3 -->|submits MR| Refinery
    Witness -->|escalates| Deacon
```

## Tips and Best Practices

:::tip[Check Stale Polecats]

Run `gt polecat stale` to see what the Witness considers stalled. This is useful for diagnosing slow progress before the Witness takes automatic action.

:::

:::tip[Manual Nuke for Stuck Workers]

If you know a polecat is hopelessly stuck, use `gt polecat nuke <name>` to clean it up immediately rather than waiting for the Witness patrol cycle.

:::

:::info[One Witness Per Rig]

Each rig has exactly one Witness. The Witness only monitors polecats within its own rig -- it has no visibility into other rigs. Cross-rig monitoring is the Deacon's job.

:::

:::warning[Do Not Kill the Witness]

Stopping a Witness leaves polecats in that rig unsupervised. If you must stop a Witness, ensure no polecats are running, or the Deacon will detect the missing Witness and restart it.

:::

## Common Patterns

### The Ephemeral Polecat Model

The Witness follows an ephemeral cleanup model for polecats. When a polecat sends `POLECAT_DONE`:

- **Clean exit** (branch pushed, MR submitted, git clean): Auto-nuke immediately. No cleanup wisp needed.
- **Dirty exit** (uncommitted changes, unpushed commits): Create a cleanup wisp. Attempt to recover work, then nuke.

This means the common case (clean `gt done`) is fast and automatic. Cleanup wisps are the exception, not the rule.

### Processing Polecat Mail

The Witness handles several mail types during its inbox check:

| Mail Type | Action |
|-----------|--------|
| `POLECAT_STARTED` | Register new polecat, begin monitoring |
| `POLECAT_DONE` | Auto-nuke if clean, cleanup wisp if dirty |
| `MERGED` | Informational -- polecat already nuked |
| `HELP` | Assess and respond to polecat request |
| `HANDOFF` | Process context handoff for session cycling |

### Swarm Tracking

When the Mayor dispatches multiple polecats as a batch (swarm), the Witness tracks completion:

```
Swarm started (4 polecats) → Monitor all 4
Polecat 1 done → 3 remaining
Polecat 2 done → 2 remaining
Polecat 3 done → 1 remaining
Polecat 4 done → Swarm complete → Notify Mayor
```

### Refinery Health Check

During each patrol, the Witness verifies the Refinery is alive and processing MRs. If the Refinery is stuck, the Witness nudges it. If it remains unresponsive, the Witness escalates to the Deacon.

## Troubleshooting

### Witness Patrol Is Not Running

The Witness runs on a 5-minute patrol cycle. If patrols have stopped:

```bash
gt rig status <rig>          # Check Witness session state
gt deacon status             # Is the Deacon monitoring this Witness?
```

The Deacon detects missing Witness pings and restarts the Witness automatically.

### Polecats Are Not Being Cleaned Up

If completed polecats are accumulating, check that:

1. The Witness is receiving `POLECAT_DONE` mail
2. The Refinery is sending `MERGED` signals after merging
3. The Witness patrol cycle is running

```bash
gt polecat list              # See polecat states
gt mq list                   # Check if MRs are stuck in the queue
```

### Stalled Polecat Not Being Nudged

The Witness only nudges polecats during its 5-minute patrol cycle. A polecat must be stalled for the stall threshold (default 15 minutes) before the Witness takes action. Check:

```bash
gt polecat stale             # See what the Witness considers stalled
gt polecat status <name>     # Check specific polecat activity
```

### Witness Context Is Filling Up

Long-running Witness sessions accumulate context. The Witness handles this automatically: when context is HIGH, it hands off to a fresh session via `gt handoff`. If this is not happening, the Witness may be stuck:

```bash
gt nudge <rig>/witness "context check"
```

## See Also

- **[Deacon](deacon.md)** -- Supervises the Witness and handles escalations from it
- **[Polecats](polecats.md)** -- Workers the Witness monitors
- **[Refinery](refinery.md)** -- Merge processor the Witness health-checks
- **[Hooks](../concepts/hooks.md)** -- Mechanism for attaching work to polecats
- **[GUPP](../concepts/gupp.md)** -- The principle that drives polecat execution
- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- Periodic monitoring cadence the Witness follows
- **[Session Cycling](../concepts/session-cycling.md)** -- How the Witness manages context limits via handoff
- **[Agent Commands](../cli-reference/agents.md)** -- CLI commands for Witness lifecycle and operations


========================================================================
# Refinery -- Merge Queue Processor
# URL: /docs/agents/refinery
========================================================================

# Refinery -- Merge Queue Processor

> The Refinery is the gatekeeper of `main`. It serializes merges from concurrent polecats, ensuring every change is rebased, validated, and cleanly integrated.

---

## Overview

Every rig has a Refinery -- a persistent agent that manages the merge queue (MQ). When polecats finish their work and run `gt done`, they submit a merge request (MR) to the Refinery. The Refinery processes MRs one at a time: rebasing onto the latest `main`, running validation, and merging if everything passes. This serialization prevents the chaos that would result from multiple agents pushing to `main` simultaneously.

The name comes from the Mad Max universe -- Gas Town's refinery is where raw fuel becomes usable. Here, raw feature branches become clean commits on `main`.

## Key Characteristics

| Property | Value |
|----------|-------|
| **Scope** | Per-rig |
| **Lifecycle** | Persistent |
| **Instance count** | 1 per rig |
| **Session type** | Long-running Claude Code session |
| **Patrol cycle** | 5 minutes |
| **Location** | `~/gt/<rig>/refinery/rig/` |
| **Git identity** | Yes (canonical clone) |
| **Mailbox** | Yes |

## Responsibilities

### 1. Receive Merge Requests

When a polecat runs `gt done`, it:

1. Pushes its feature branch
2. Submits an MR to the rig's Refinery
3. Exits (sandbox later nuked by Witness)

The MR enters the merge queue for processing.

### 2. Serialize Merges

The Refinery processes MRs strictly one at a time. This prevents race conditions and ensures each merge sees the latest state of `main`:

```mermaid
sequenceDiagram
    participant P1 as Polecat: Toast
    participant P2 as Polecat: Alpha
    participant R as Refinery
    participant M as main

    P1->>R: Submit MR (feature-a)
    P2->>R: Submit MR (feature-b)
    R->>R: Process MR 1 (feature-a)
    R->>M: Rebase + validate + merge
    R->>R: Process MR 2 (feature-b)
    R->>M: Rebase + validate + merge
```

### 3. Rebase, Validate, Merge Workflow

For each MR, the Refinery follows a strict workflow:

```mermaid
flowchart TD
    Receive["Receive MR"]
    Rebase["Rebase onto latest main"]
    Conflict{"Conflicts?"}
    Validate["Run Validation"]
    Pass{"Passes?"}
    Merge["Merge to main"]
    Done["Mark Bead Complete"]
    SpawnCat["Spawn Fresh Polecat<br/>for Conflict Resolution"]
    Reject["Reject MR"]
    Retry["Re-queue for Retry"]

    Receive --> Rebase
    Rebase --> Conflict
    Conflict -->|No| Validate
    Conflict -->|Yes| SpawnCat
    SpawnCat --> Retry
    Validate --> Pass
    Pass -->|Yes| Merge
    Pass -->|No| Reject
    Merge --> Done
```

**Steps:**

1. **Rebase** -- Rebase the feature branch onto the latest `main`
2. **Conflict check** -- If conflicts exist, spawn a fresh polecat to resolve them
3. **Validate** -- Run tests, linting, build checks (configurable per rig)
4. **Merge** -- Fast-forward merge to `main`
5. **Mark complete** -- Update the bead status to done

### 4. Conflict Resolution

When a rebase produces conflicts, the Refinery does not attempt to resolve them itself. Instead, it spawns a fresh polecat with the conflict context:

```mermaid
flowchart LR
    Conflict["Merge Conflict<br/>Detected"]
    Spawn["Spawn Polecat<br/>with Context"]
    Resolve["Polecat Resolves<br/>Conflicts"]
    Resubmit["Polecat Submits<br/>New MR"]

    Conflict --> Spawn --> Resolve --> Resubmit
```

This separation of concerns keeps the Refinery focused on queue management while leveraging polecats for creative problem-solving.

### 5. Queue Management

The merge queue maintains ordering and handles retries:

| MR State | Description |
|----------|-------------|
| `queued` | Waiting to be processed |
| `processing` | Currently being rebased/validated |
| `merged` | Successfully merged to main |
| `rejected` | Failed validation, removed from queue |
| `conflict` | Conflicts detected, polecat spawned |
| `retry` | Re-queued after conflict resolution |

## Commands

| Command | Description |
|---------|-------------|
| `gt mq list` | List all MRs in the merge queue |
| `gt mq next` | Show the next MR to be processed |
| `gt mq submit` | Manually submit an MR to the queue |
| `gt mq status` | View merge queue status and metrics |
| `gt mq reject <id>` | Manually reject an MR |
| `gt mq retry <id>` | Re-queue a failed MR for retry |
| `gt mq integration` | Run integration validation across recent merges |

## Configuration

Refinery behavior is configured per-rig:

| Setting | Default | Description |
|---------|---------|-------------|
| Patrol interval | 5 min | Time between queue processing cycles |
| Validation command | `make test` | Command to validate before merge |
| Max retries | 2 | Maximum retry attempts for failed MRs |
| Conflict strategy | `spawn-polecat` | How to handle merge conflicts |
| Auto-merge | `true` | Whether to auto-merge passing MRs |
| Branch cleanup | `true` | Delete feature branches after merge |

## Refinery Location

The Refinery holds the **canonical clone** of the repository -- the authoritative copy from which merges to `main` happen:

```
~/gt/<rig>/refinery/rig/     # Canonical git clone
~/gt/<rig>/refinery/CLAUDE.md # Refinery agent context
```

All other clones (mayor, polecats, crew) are worktrees or separate clones that ultimately merge through the Refinery.

## Interaction Diagram

```mermaid
graph TD
    P1["Polecat: Toast"]
    P2["Polecat: Alpha"]
    P3["Polecat: Bravo"]
    R["Refinery"]
    Main["main branch"]
    W["Witness"]
    Conflict["Conflict Polecat"]

    P1 -->|"gt done (MR)"| R
    P2 -->|"gt done (MR)"| R
    P3 -->|"gt done (MR)"| R
    R -->|"rebase + validate"| Main
    R -->|"conflict"| Conflict
    Conflict -->|"resolved MR"| R
    W -->|monitors| R
```

## Tips and Best Practices

:::tip[Monitor the Merge Queue]

Use `gt mq list` and `gt mq status` to keep an eye on merge throughput. A growing queue may indicate validation failures or frequent conflicts.

:::

:::tip[Tune Validation]

Configure the validation command to run only the most critical checks. Full test suites can slow the queue significantly when many polecats are submitting simultaneously.

:::

:::tip[Use Integration Checks]

Run `gt mq integration` periodically to validate that recent merges interact correctly. This catches integration issues that per-MR validation might miss.

:::

:::warning[Do Not Push Directly to Main]

All changes should flow through the Refinery. Pushing directly to `main` bypasses validation and can cause rebase failures for queued MRs.

:::

:::info[One Refinery Per Rig]

Each rig has exactly one Refinery. Cross-rig merges are coordinated by Dogs and the Deacon, not by Refineries.

:::

## Common Patterns

### The MERGED Mail Contract

After merging a branch to `main`, the Refinery **must immediately** send a `MERGED` mail to the Witness. This is the critical signal that triggers polecat sandbox cleanup:

```
Merge to main → Push → Send MERGED mail → Close MR bead → Archive mail → Cleanup branches
```

If the `MERGED` mail is delayed or lost, polecat worktrees accumulate indefinitely. This is why the formula enforces immediate send before any cleanup steps.

### The Scotty Principle

Named after the Star Trek engineer who would never walk past a warp core leak: the Refinery does not proceed past failures. If tests fail, the Refinery stops and diagnoses whether the failure is:

- **Branch-caused**: The polecat's changes broke something. Abort merge, notify the polecat to fix.
- **Pre-existing**: Tests were already broken on `main`. Fix it directly or file a bead.

The Refinery never merges code that fails validation.

### Processing Multiple MRs

The Refinery processes one MR at a time, strictly serialized. After each successful merge, it loops back to check for more:

```
Process MR 1 → merge → Process MR 2 → merge → ... → Queue empty → Wait for next patrol
```

This serialization ensures each merge sees the latest state of `main`, preventing cascading conflicts.

### Conflict Resolution via Fresh Polecat

When rebase conflicts occur, the Refinery does not attempt manual resolution. Instead, it:

1. Aborts the rebase
2. Creates a conflict-resolution task
3. Spawns a fresh polecat with conflict context
4. The polecat re-implements the changes against current `main`
5. The polecat submits a new MR

This keeps the Refinery focused on queue management, not creative problem-solving.

## Troubleshooting

### Merge Queue Is Backed Up

```bash
gt mq list                   # See all queued MRs
gt mq status                 # Queue metrics
gt rig status <rig>          # Is the Refinery session alive?
```

Common causes:
- Refinery session is dead or stuck
- Tests are failing on every branch
- Repeated rebase conflicts

### Tests Are Failing on Merge

The Refinery distinguishes between branch-caused and pre-existing failures. If all branches are failing:

```bash
# Check if main itself has broken tests
git checkout main && make test
```

If `main` is broken, the Refinery should file a bead or fix it directly.

### Branches Are Not Being Merged

Verify the Refinery is processing its queue:

```bash
gt mq list                   # Are MRs queued?
gt rig status <rig>          # Is the Refinery alive?
```

If MRs are queued but not processing, the Refinery may need a nudge:

```bash
gt nudge <rig>/refinery "process queue"
```

### Polecat Worktrees Accumulating

If polecat directories are not being cleaned up after merge, the `MERGED` mail signal may be failing. Check:

1. Is the Refinery sending `MERGED` mail after each merge?
2. Is the Witness receiving and processing the mail?
3. Check `gt polecat list` for polecats stuck in `done` state.

## See Also

- **[Polecats](polecats.md)** -- Submit MRs that the Refinery processes
- **[Witness](witness.md)** -- Monitors the Refinery and receives `MERGED` signals
- **[Molecules](../concepts/molecules.md)** -- The `mol-refinery-patrol` formula defines the merge workflow
- **[Beads](../concepts/beads.md)** -- MR beads track each merge request through the queue
- **[Merge Queue CLI](../cli-reference/merge-queue.md)** -- CLI commands for interacting with the merge queue
- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- Periodic processing cadence the Refinery follows
- **[Refinery Commands](../cli-reference/refinery-commands.md)** -- CLI commands for Refinery management and queue operations


========================================================================
# Polecats -- Ephemeral Workers
# URL: /docs/agents/polecats
========================================================================

# Polecats -- Ephemeral Workers

> Polecats are the hands of Gas Town. They spawn, do their job, submit their work, and disappear. No idle time. No wasted resources.

---

## Overview

Polecats are ephemeral worker agents -- the primary units that write code, fix bugs, and implement features. Each polecat is spawned for a single task, works until completion, then self-destructs. They follow the "spawn, work, done, nuke" lifecycle with zero idle time. A polecat is always in one of three states: working, stalled, or zombie. There is no "idle" state.

The name comes from the character Slit's car in Mad Max: Fury Road -- fast, aggressive, single-purpose machines.

## Key Characteristics

| Property | Value |
|----------|-------|
| **Scope** | Per-rig |
| **Lifecycle** | Ephemeral (single-task) |
| **Instance count** | Many per rig |
| **Session type** | Short-lived Claude Code session |
| **Patrol cycle** | None (monitored by Witness) |
| **Location** | `~/gt/<rig>/polecats/<name>/` |
| **Git identity** | Yes (unique per polecat) |
| **Mailbox** | Yes (while alive) |

## Lifecycle

```mermaid
stateDiagram-v2
    [*] --> Spawn: gt sling / Mayor assigns
    Spawn --> Working: Hook loaded, session starts
    Working --> Done: gt done
    Done --> Nuke: Witness cleanup
    Nuke --> [*]

    Working --> Stalled: No progress
    Stalled --> Working: Nudge successful
    Stalled --> Zombie: Unrecoverable

    Working --> Zombie: Session crashes
    Zombie --> Nuke: Witness cleanup
```

### Spawn

A polecat is spawned when work is slung to a rig:

```bash
gt sling gt-a1b2c myproject    # Spawns a polecat in myproject
```

The system:

1. Allocates a name from the name pool (or generates an anonymous name)
2. Creates a git worktree in `~/gt/<rig>/polecats/<name>/`
3. Sets up the polecat's CLAUDE.md context
4. Attaches the bead to the polecat's hook
5. Starts a Claude Code session

### Work

The polecat reads its hook, finds the assigned bead, and begins work. It has full access to the codebase within its worktree and can:

- Read and write files
- Run tests
- Create git commits
- Ask for clarification via escalation

### Done

When work is complete, the polecat runs `gt done`:

```bash
gt done
```

This command:

1. Pushes the feature branch to the remote
2. Submits a merge request to the Refinery
3. Updates the bead status
4. Exits the session cleanly

### Nuke

After the polecat exits, the Witness cleans up:

1. Removes the git worktree
2. Deletes the polecat directory
3. Reclaims the name for the pool

## Polecat States

A polecat is never idle. It exists in exactly one of three states:

| State | Description | Detected By | Action |
|-------|-------------|-------------|--------|
| **Working** | Actively making progress | Activity in session | None -- healthy |
| **Stalled** | No progress for threshold period | Witness patrol | Nudge, then escalate |
| **Zombie** | Session crashed or exited abnormally | Witness patrol | Recover work, nuke sandbox |

## Session vs Sandbox

It is important to distinguish between a polecat's **session** and its **sandbox**:

| Concept | Description |
|---------|-------------|
| **Session** | The Claude Code process running the polecat's AI agent |
| **Sandbox** | The git worktree directory containing the polecat's code |

A session can die while the sandbox persists (zombie state). The sandbox contains all the polecat's uncommitted work. The Witness checks for recoverable work before nuking a zombie's sandbox.

## Naming

Polecats are drawn from a name pool with memorable, distinct names:

- **Named pool**: Toast, Alpha, Bravo, Charlie, Delta, Echo, Foxtrot, etc.
- **Anonymous**: Auto-generated names when the pool is exhausted

Each name is unique within a rig at any given time. Names are recycled after a polecat is nuked.

### Git Identity

Every polecat gets its own git identity:

```
Author: Toast <toast@myproject.gt>
Author: Alpha <alpha@myproject.gt>
```

This makes it easy to trace which polecat made which commits in the git log.

### Beads Actor

Each polecat is registered as a beads actor, allowing it to update issue status, add comments, and log activity against its assigned bead.

## Self-Cleaning Behavior

The `gt done` workflow ensures polecats clean up after themselves:

```mermaid
flowchart TD
    Complete["Work Complete"]
    Commit["Final git commit"]
    Push["Push feature branch"]
    Submit["Submit MR to Refinery"]
    Update["Update bead status"]
    Exit["Exit session"]
    Witness["Witness nukes sandbox"]

    Complete --> Commit
    Commit --> Push
    Push --> Submit
    Submit --> Update
    Update --> Exit
    Exit --> Witness
```

If a polecat crashes before running `gt done`, the work persists in the sandbox. The Witness detects the zombie, recovers any unsaved work by pushing the branch, and then nukes the sandbox.

## Exit States

When a polecat finishes, it exits in one of four states:

| Exit State | Meaning | What Happens Next |
|------------|---------|-------------------|
| `COMPLETED` | Work done, MR submitted | Refinery processes the merge |
| `ESCALATED` | Hit a blocker, needs help | Escalation routes to Mayor/Overseer |
| `DEFERRED` | Paused, work still open | Another agent can pick it up later |
| `PHASE_COMPLETE` | Phase done, waiting on gate | Gate opens, next phase begins |

## Commands

| Command | Description |
|---------|-------------|
| `gt polecat list` | List all polecats in the current rig |
| `gt polecat status <name>` | Check a specific polecat's status |
| `gt polecat nuke <name>` | Manually destroy a polecat sandbox |
| `gt polecat gc` | Garbage collect completed polecat directories |
| `gt polecat stale` | List polecats that appear stalled |

## Configuration

Polecat behavior is configured per-rig:

| Setting | Default | Description |
|---------|---------|-------------|
| Max polecats | 10 | Maximum concurrent polecats per rig |
| Name pool | NATO phonetic | Pool of names to assign |
| Stall threshold | 15 min | Idle time before considered stalled |
| Auto-push on crash | `true` | Push branch before zombie cleanup |

## Directory Structure

```
~/gt/<rig>/polecats/
├── toast/                # Polecat sandbox (git worktree)
│   ├── .git              # Worktree link
│   ├── CLAUDE.md         # Polecat agent context
│   └── <project files>   # Full working copy
├── alpha/
│   ├── .git
│   ├── CLAUDE.md
│   └── <project files>
└── ...
```

## Tips and Best Practices

:::tip[Let Polecats Self-Clean]

Trust the `gt done` workflow. Polecats are designed to be disposable -- do not try to keep them alive or reuse them for multiple tasks.

:::

:::tip[Monitor with gt polecat list]

Use `gt polecat list` to see the current state of all workers. This shows you what the Witness sees, including any stalled or zombie polecats.

:::

:::tip[Name Pool Matters]

Named polecats are easier to track in logs and git history than anonymous ones. If you are running many concurrent workers, consider expanding the name pool.

:::

:::warning[Do Not Edit Polecat Sandboxes Directly]

Polecat worktrees are managed by the system. Editing files directly in a polecat's sandbox while it is running will cause conflicts and confusion.

:::

:::info[Polecats vs Dogs]

Polecats build features within a single rig and are ephemeral. Dogs handle infrastructure tasks across rigs and are reusable. If you need cross-rig work, use a Dog, not a Polecat.

:::

## The Polecat Work Formula

Every polecat follows the `mol-polecat-work` formula, which defines these steps:

| Step | Purpose |
|------|---------|
| **load-context** | Prime environment, check hook, understand assignment |
| **branch-setup** | Create clean feature branch, sync with main |
| **preflight-tests** | Verify tests pass on main before starting (Scotty Principle) |
| **implement** | Do the actual work, make atomic commits |
| **self-review** | Review changes for bugs, security, completeness |
| **run-tests** | Run full test suite, verify coverage |
| **cleanup-workspace** | Ensure workspace is pristine |
| **prepare-for-review** | Update issue notes, sync beads |
| **submit-and-exit** | Run `gt done` to submit to merge queue and self-destruct |

Use `bd ready` to see your current step and `bd close <step-id>` to advance.

## Common Patterns

### The GUPP Startup

When a polecat is spawned with work on its hook, it executes immediately per the [GUPP](../concepts/gupp.md) principle:

```bash
gt hook                      # Check assigned work
bd ready                     # See workflow steps
bd show <issue-id>           # Understand the assignment
bd update <id> --status=in_progress  # Claim the work
# Begin implementation immediately -- no confirmation needed
```

### The Handoff Protocol

When a polecat's context fills up mid-task, it hands off to a fresh session:

```bash
gt handoff -s "Implementing feature X" -m "Issue: gt-abc
Progress: Tests passing, 2 of 5 endpoints done
Next: Continue with endpoints 3-5
Branch: feature/add-endpoints"
```

The next session picks up context from the handoff mail.

### Branch Naming

Polecat branches follow the convention:

```
polecat/<name>/<issue-id>@<molecule-id>
```

For example: `polecat/quartz/ga-h6g@ml4pgdku`

### The Pre-Submission Checklist

Before running `gt done`, every polecat verifies:

```bash
git status                   # Must be clean
git log --oneline -3         # Verify commits are present
# Then:
gt done                      # Submit and exit
```

## Troubleshooting

### Polecat Cannot Find Its Hook

If `gt hook` shows empty but you know work was assigned:

```bash
bd ready                     # Check beads for assigned work
bd list --status=open        # See all open issues
gt mail inbox                # Check for hooked mail
```

If the hook is truly empty and you are a polecat, escalate to the Witness -- polecats should always have work.

### gt done Fails

Common `gt done` failure causes:

| Cause | Fix |
|-------|-----|
| Uncommitted changes | `git add <files> && git commit -m "..."` |
| Unpushed commits | `git push` |
| Beads not synced | `bd sync --flush-only` |
| Branch behind main | `git pull --rebase origin main` |

### Polecat Is Stuck on a Step

If you cannot complete a molecule step:

```bash
# Check what step you are on
bd ready

# If blocked by external dependency
gt escalate "Blocked on <description>" -s HIGH -m "Details..."

# If truly stuck, exit cleanly
gt done --status=ESCALATED
```

### Session Context Is Filling Up

Long implementations can exhaust session context. Use `gt handoff` to cycle to a fresh session with preserved context:

```bash
gt handoff -s "Context full, continuing work" -m "Issue: <id>
Branch: <branch>
Progress: <what is done>
Next: <what remains>"
```

## See Also

- **[GUPP](../concepts/gupp.md)** -- The propulsion principle that drives polecat execution
- **[Hooks](../concepts/hooks.md)** -- How work is attached to polecats
- **[Molecules](../concepts/molecules.md)** -- The `mol-polecat-work` formula defines polecat workflow steps
- **[Witness](witness.md)** -- Monitors polecats for health
- **[Refinery](refinery.md)** -- Processes MRs submitted by polecats via `gt done`
- **[Beads](../concepts/beads.md)** -- Issue tracking system polecats use for task management
- **[Session Cycling](../concepts/session-cycling.md)** -- How polecats hand off context when sessions fill up
- **[Handoff Ceremony](../workflows/handoff-ceremony.md)** -- Protocol for transferring work between sessions
- **[Polecat Commands](../cli-reference/polecat-commands.md)** -- CLI commands for polecat lifecycle and management


========================================================================
# Dogs -- Infrastructure Workers
# URL: /docs/agents/dogs
========================================================================

# Dogs -- Infrastructure Workers

> Dogs do the dirty work. They clean up messes, sync systems, and handle cross-rig infrastructure tasks that no single polecat or witness can manage alone.

---

## Overview

Dogs are reusable worker agents managed by the Deacon for infrastructure and maintenance tasks that span multiple rigs. While polecats are ephemeral single-rig feature builders, Dogs are multi-task cross-rig utility workers. They handle rebuilding, syncing, migrations, orphan cleanup, and any other infrastructure work that falls outside the scope of feature development.

## Key Characteristics

| Property | Value |
|----------|-------|
| **Scope** | Cross-rig (town-level) |
| **Lifecycle** | Reusable (multiple tasks) |
| **Instance count** | As needed |
| **Session type** | Task-driven Claude Code session |
| **Patrol cycle** | None (managed by Deacon) |
| **Location** | `~/gt/deacon/dogs/` |
| **Git identity** | No |
| **Mailbox** | No |

## Dogs vs Polecats

This is the most common point of confusion. Here is the distinction:

| Property | Polecats | Dogs |
|----------|----------|------|
| **Purpose** | Build features | Clean messes |
| **Scope** | One rig | Cross-rig |
| **Lifecycle** | Ephemeral (one task, then nuke) | Reusable (multiple tasks) |
| **Managed by** | Witness | Deacon |
| **Works on** | Feature code | Infrastructure |
| **Git identity** | Yes (unique per cat) | No |
| **Submits MRs** | Yes (via gt done) | Rarely |
| **Location** | `~/gt/<rig>/polecats/<name>/` | `~/gt/deacon/dogs/` |

```mermaid
graph LR
    subgraph "Polecats (Feature Work)"
        P1["Toast<br/>Rig: myproject<br/>Task: Fix auth bug"]
        P2["Alpha<br/>Rig: myproject<br/>Task: Add validation"]
    end

    subgraph "Dogs (Infrastructure)"
        D1["Dog 1<br/>Task: Sync configs<br/>across all rigs"]
        D2["Dog 2<br/>Task: Clean orphaned<br/>worktrees"]
    end
```

**Rule of thumb**: If the work produces a feature branch and an MR, it is a Polecat. If the work maintains infrastructure, it is a Dog.

## Responsibilities

### 1. Cross-Rig Infrastructure

Dogs handle tasks that touch multiple rigs:

- Syncing configuration files across rigs
- Updating shared dependencies
- Migrating data structures
- Cross-rig consistency checks

### 2. Cleanup Operations

Dogs perform bulk cleanup that is beyond a single Witness's scope:

- Orphaned worktree removal across all rigs
- Stale branch cleanup
- Disk space reclamation
- Temp file purging

### 3. System Rebuilding

When infrastructure needs rebuilding:

- Recreating broken worktrees
- Re-initializing corrupted beads databases
- Rebuilding agent context files
- Restoring from backup state

### 4. Migrations

When Gas Town itself needs updating:

- Schema migrations for beads
- Configuration format updates
- Plugin upgrades across rigs

## Work Types

| Work Type | Description | Frequency |
|-----------|-------------|-----------|
| `rebuild` | Recreate broken infrastructure | As needed |
| `sync` | Synchronize state across rigs | Periodic |
| `migrate` | Update schemas or formats | On upgrade |
| `cleanup` | Remove orphans and stale data | Regular |
| `audit` | Verify system integrity | On demand |

## Managed by the Deacon

Dogs do not operate independently. The Deacon manages their complete lifecycle:

```mermaid
sequenceDiagram
    participant Dc as Deacon
    participant Dog as Dog
    participant R1 as Rig 1
    participant R2 as Rig 2

    Dc->>Dog: Spawn with task
    Dog->>R1: Perform cleanup
    Dog->>R2: Perform cleanup
    Dog->>Dc: Report results
    Dc->>Dog: Assign next task (or release)
```

The Deacon:

1. **Spawns** Dogs when infrastructure work is needed
2. **Assigns** tasks with clear scope and completion criteria
3. **Monitors** Dog progress
4. **Reuses** Dogs for additional tasks if available
5. **Releases** Dogs when no more work is queued

## Location

All Dogs operate from the Deacon's dog directory:

```
~/gt/deacon/dogs/
├── boot/              # Boot dog (special - see Boot docs)
├── dog-001/           # General purpose dog
├── dog-002/           # Another dog
└── ...
```

Each Dog gets its own subdirectory for task context and working files.

## Commands

| Command | Description |
|---------|-------------|
| `gt dog list` | List all active dogs |
| `gt dog add <name>` | Create a new dog in the kennel |
| `gt dog status <id>` | Check a specific dog's status |

## Tips and Best Practices

:::tip[Let the Deacon Manage Dogs]

Dogs are designed to be managed by the Deacon automatically. Manual dog management (`gt dog add`) should be rare and reserved for specific infrastructure emergencies.

:::

:::tip[Check Dog Activity After Issues]

If you notice infrastructure problems (orphaned files, stale worktrees, inconsistent state), check `gt dog list` to see if a Dog is already handling it. The Deacon is likely already aware.

:::

:::info[Dogs Are Not Feature Workers]

If you need a feature implemented, use a Polecat (via `gt sling`). Dogs are strictly for infrastructure. Trying to use a Dog for feature work will bypass the merge queue and code review process.

:::

:::warning[Boot Is a Special Dog]

The [Boot](boot.md) agent is a specialized Dog with its own lifecycle (runs fresh on each daemon tick). See the Boot documentation for details.

:::

## Common Patterns

### Auto-Dispatch via gt sling

Dogs are typically dispatched by the Deacon, but the `gt sling` command can also dispatch work to idle dogs:

```bash
gt sling <infrastructure-task> deacon/dogs  # Auto-dispatches to an idle dog
```

### Dog Pool Maintenance

The Deacon maintains a minimum pool of idle dogs so infrastructure tasks can be dispatched immediately. During the patrol cycle, the Deacon checks:

- Are there enough idle dogs in the pool?
- Are any dogs stuck past their timeout?
- Should any stuck dogs receive a death warrant?

### Cross-Rig Cleanup Example

A typical dog cleanup task might span all rigs:

```
Dog receives: "Clean orphaned worktrees across all rigs"
  → Scan rig 1: Remove 3 orphaned directories
  → Scan rig 2: Remove 1 orphaned directory
  → Report results to Deacon
  → Accept next task or return to idle pool
```

### Session Garbage Collection

The Deacon dispatches dogs for session cleanup when it detects stale tmux sessions or orphaned processes. The dog handles the actual file and process cleanup.

## Troubleshooting

### Dog Is Stuck on a Task

Dogs that exceed their task timeout receive a "death warrant" from the Deacon during the health check:

```bash
gt dog list                  # Check dog states
gt dog status <id>           # See specific dog activity
```

If a dog is genuinely stuck, the Deacon will file a warrant and Boot will handle process cleanup.

### No Idle Dogs Available

If infrastructure work is queued but no dogs are available:

```bash
gt dog list                  # See active dogs
gt dog add <name>            # Manually add a dog to the pool
```

The Deacon should auto-spawn dogs, but manual intervention may be needed if the Deacon is down.

### Dog Work Is Not Being Tracked

Dogs do not submit MRs through the Refinery. Their work is tracked through the Deacon's mail system. If dog completions are not being registered:

```bash
gt mail inbox                # Check Deacon's mailbox for DOG_DONE messages
gt deacon status             # Verify Deacon is processing mail
```

## See Also

- **[Deacon](deacon.md)** -- Manages the dog pool and dispatches tasks
- **[Boot](boot.md)** -- A special-purpose dog for system triage
- **[Polecats](polecats.md)** -- Feature workers (contrast with infrastructure dogs)
- **[Rigs](../concepts/rigs.md)** -- Dogs operate across rigs, unlike polecats


========================================================================
# Crew -- Human Workspaces
# URL: /docs/agents/crew
========================================================================

# Crew -- Human Workspaces

> Crew workspaces give human developers a first-class seat at the Gas Town table -- persistent, fully integrated, and always ready for hands-on work.

---

## Overview

Crew members are persistent workspaces for human developers within a Gas Town rig. Unlike polecats (which are ephemeral AI workers), crew workspaces are full git clones that persist indefinitely and are managed by the human developer. Each crew member has a named directory, a complete copy of the repository, and full integration with Gas Town's communication and tracking systems.

Crew workspaces let humans work alongside AI agents in the same project, using the same tools and tracking systems, without interference.

## Key Characteristics

| Property | Value |
|----------|-------|
| **Scope** | Per-rig |
| **Lifecycle** | Persistent (user-managed) |
| **Instance count** | Named members per rig |
| **Session type** | User-controlled (manual or tmux) |
| **Patrol cycle** | None |
| **Location** | `~/gt/<rig>/crew/<name>/` |
| **Git identity** | Yes (developer's own) |
| **Mailbox** | No |

## Full Git Clones

Unlike polecats, which use git worktrees (lightweight branches off the canonical clone), crew workspaces are **full git clones**. This means:

| Property | Crew (Full Clone) | Polecat (Worktree) |
|----------|-------------------|---------------------|
| `.git` directory | Full repository | Link to parent |
| Independent history | Yes | Shared with canonical |
| Survives canonical changes | Yes | May need refresh |
| Disk usage | Higher | Lower |
| Branch flexibility | Full | Limited |

Full clones give human developers complete independence -- they can rebase, force-push, switch branches, and perform any git operation without affecting other agents.

## Named Members

Crew members are named after the humans who use them:

```
~/gt/myproject/crew/
├── dave/          # Dave's workspace
├── emma/          # Emma's workspace
└── fred/          # Fred's workspace
```

Each member's workspace is a complete, self-contained development environment.

## Gas Town Integration

Crew workspaces are fully integrated with Gas Town's systems:

### Beads (Issue Tracking)

Crew members can use the `bd` CLI to create, update, and close issues:

```bash
cd ~/gt/myproject/crew/dave
bd list                        # View rig issues
bd create --title "New feature"  # Create an issue
bd close gt-abc12              # Close an issue
```

### Hooks

Crew members can attach work to their hook, just like any agent:

```bash
gt hook                        # Check current hook
gt hook gt-abc12               # Attach work
gt done                        # Submit MR and complete
```

### Mail and Communication

Crew members can send messages to agents:

```bash
gt mail send mayor "Ready for review on feature-x"
gt mail inbox                  # Check messages
```

### Optional Tmux Integration

Crew workspaces can be managed with tmux for multi-pane workflows:

```bash
gt crew start myproject dave   # Start dave's workspace in tmux
```

This opens a tmux session with the crew workspace ready.

## Commands

| Command | Description |
|---------|-------------|
| `gt crew add <rig> <name>` | Create a new crew workspace |
| `gt crew remove <rig> <name>` | Remove a crew workspace |
| `gt crew list` | List all crew members across rigs |
| `gt crew start <rig> <name>` | Start a crew workspace (tmux) |
| `gt crew stop <rig> <name>` | Stop a crew workspace session |
| `gt crew at <rig> <name>` | Attach to a running crew session |
| `gt crew refresh <rig> <name>` | Pull latest changes into workspace |
| `gt crew restart <rig> <name>` | Stop and restart a crew session |

## Lifecycle

Crew workspaces follow a user-managed lifecycle:

```mermaid
stateDiagram-v2
    [*] --> Created: gt crew add
    Created --> Active: gt crew start
    Active --> Stopped: gt crew stop
    Stopped --> Active: gt crew start
    Active --> Active: gt crew refresh
    Active --> Active: gt crew restart
    Created --> [*]: gt crew remove
    Stopped --> [*]: gt crew remove
```

Unlike polecats, crew workspaces are never automatically nuked. They persist until the human explicitly removes them.

## Directory Structure

```
~/gt/<rig>/crew/<name>/
├── .git/              # Full git repository
├── CLAUDE.md          # Gas Town context (if using Claude Code)
├── .beads/            # Link to rig-level beads
└── <project files>    # Complete working copy
```

## Configuration

Crew workspaces inherit rig-level configuration with optional per-member overrides:

| Setting | Default | Description |
|---------|---------|-------------|
| Auto-refresh | `false` | Automatically pull latest on session start |
| Default branch | `main` | Branch to track |
| Tmux layout | `single-pane` | Tmux window layout preference |
| Agent runtime | `claude` | AI agent runtime for the workspace |

## Interaction Diagram

```mermaid
graph TD
    Human["Human Developer"]
    Crew["Crew: dave"]
    Beads["Beads DB"]
    Refinery["Refinery"]
    Mayor["Mayor"]

    Human -->|works in| Crew
    Crew -->|"bd create/update"| Beads
    Crew -->|"gt done (MR)"| Refinery
    Crew -->|"gt mail send"| Mayor
    Mayor -->|"gt mail send"| Crew
```

## Tips and Best Practices

:::tip[Use gt crew refresh Regularly]

Keep your crew workspace up to date with the latest `main` by running `gt crew refresh` frequently. This minimizes merge conflicts when you submit your work.

:::

:::tip[Coordinate with the Mayor]

Before starting work, check with the Mayor via `gt mail send mayor "Starting work on X"` to avoid duplicating effort with active polecats.

:::

:::tip[Use gt done for Clean Merges]

Even though you can push directly, using `gt done` from a crew workspace routes your changes through the Refinery merge queue. This ensures proper validation and serialized merges.

:::

:::info[Crew vs Polecats]

Crew workspaces are for humans who want persistent, long-lived development environments. Polecats are for AI agents executing single tasks. Do not use crew workspaces for automated work -- use polecats instead.

:::

:::warning[Crew Workspaces Are Not Backed Up]

Crew workspaces are full clones on the local filesystem. Uncommitted work exists only in your workspace. Commit and push regularly.

:::

## Common Patterns

### Setting Up a New Crew Workspace

```bash
gt crew add myproject dave           # Create workspace
gt crew start myproject dave         # Start session (tmux)

# Inside the workspace:
cd ~/gt/myproject/crew/dave
bd list                              # See available work
bd create --title "My feature" --type feature
# Work on the feature...
gt done                              # Submit through Refinery
```

### Working Alongside Polecats

Human crew members and AI polecats work in the same rig simultaneously. To avoid conflicts:

1. Check active polecats before starting: `gt polecat list`
2. Claim your issue to signal you are working on it: `bd update <id> --status=in_progress --assignee=dave`
3. Communicate via mail if needed: `gt mail send mayor "Starting work on X"`

### Submitting Work Through the Merge Queue

Crew members can use `gt done` just like polecats. This routes changes through the Refinery for proper validation:

```bash
git add <files>
git commit -m "Add feature X"
gt done                              # Submit MR to Refinery
```

This is preferred over pushing directly to `main`, even for crew members.

## Troubleshooting

### Crew Workspace Is Out of Date

```bash
gt crew refresh myproject dave       # Pull latest main
```

If there are conflicts during refresh, resolve them manually in the workspace.

### Cannot Create Crew Workspace

Verify the rig exists and has a valid git URL:

```bash
gt rig list                          # Confirm rig is registered
gt rig status myproject              # Check rig health
```

### Session Disconnected

If a tmux session disconnects:

```bash
gt crew at myproject dave            # Re-attach to existing session
# Or restart:
gt crew restart myproject dave
```

## See Also

- **[Polecats](polecats.md)** -- AI workers that crew members work alongside
- **[Refinery](refinery.md)** -- Merge queue that crew work flows through
- **[Beads](../concepts/beads.md)** -- Issue tracking available to crew members
- **[Rigs](../concepts/rigs.md)** -- Project containers where crew workspaces live
- **[Hooks](../concepts/hooks.md)** -- Mechanism for attaching work to crew members
- **[Handoff Ceremony](../workflows/handoff-ceremony.md)** -- Protocol for transferring work between sessions
- **[Session Cycling](../concepts/session-cycling.md)** -- How context limits are managed across sessions
- **[Crew Collaboration](../workflows/crew-collaboration.md)** -- Workflows for human-AI collaboration in Gas Town


========================================================================
# Boot -- Daemon Watchdog Dog
# URL: /docs/agents/boot
========================================================================

# Boot -- Daemon Watchdog Dog

> Boot is the first responder. It runs fresh on every daemon tick, takes a snapshot of the world, and decides whether the Deacon needs waking or nudging.

---

## Overview

Boot is a special-purpose Dog that runs fresh on each daemon tick. Unlike persistent agents that maintain long-running sessions, Boot starts clean every time, observes the current system state, decides what actions are needed, and either wakes the Deacon or nudges it with findings. Boot is the bridge between the "dumb" Daemon (a Go process that just sends heartbeats) and the "smart" Deacon (a Claude Code agent that makes health decisions).

Boot ensures the Deacon is awake and informed, even after crashes, restarts, or extended downtime.

## Key Characteristics

| Property | Value |
|----------|-------|
| **Scope** | Town-level |
| **Lifecycle** | Per-tick (fresh each run) |
| **Instance count** | 1 |
| **Session type** | Short-lived, clean start |
| **Patrol cycle** | Each daemon tick (~3 min) |
| **Location** | `~/gt/deacon/dogs/boot/` |
| **Git identity** | No |
| **Mailbox** | No |

## Why Boot Exists

The Daemon is intentionally simple -- a Go process that sends heartbeats and processes lifecycle requests. It does not understand agent health, work state, or system topology. But it needs to know whether the Deacon (which does understand these things) is alive and responsive.

Boot fills this gap:

```mermaid
graph LR
    Daemon["Daemon<br/>(Go process)<br/>Dumb scheduler"]
    Boot["Boot<br/>(Claude Code)<br/>Smart observer"]
    Deacon["Deacon<br/>(Claude Code)<br/>Health manager"]

    Daemon -->|"spawns on tick"| Boot
    Boot -->|"observes state"| Boot
    Boot -->|"wake/nudge"| Deacon
```

Without Boot, the Daemon would have to contain logic for understanding agent state -- violating the "dumb scheduler, smart agents" design principle.

## The Triage Cycle

Boot follows a strict three-phase triage cycle on every run:

```mermaid
flowchart LR
    Observe["1. Observe<br/>System State"]
    Decide["2. Decide<br/>Actions Needed"]
    Act["3. Act<br/>Execute"]

    Observe --> Decide --> Act
```

### Phase 1: Observe

Boot surveys the current system state:

- **Deacon status**: Is the Deacon session alive? When did it last respond?
- **Witness inventory**: How many Witnesses should exist? How many are running?
- **Polecat census**: Are there active polecats? Any obvious zombies?
- **Queue state**: Is the merge queue backed up?
- **Disk health**: Is disk space critically low?
- **Stale hooks**: Are there hooks with no active agent?

### Phase 2: Decide

Based on observations, Boot decides the appropriate action:

| Observation | Decision |
|-------------|----------|
| Deacon is dead | Wake (start new session) |
| Deacon is alive but stale | Nudge with findings |
| Deacon is healthy and recent | No action needed |
| Critical issues found | Nudge with priority findings |
| Everything healthy | No action needed |

### Phase 3: Act

Boot executes its decision:

- **Wake**: Start a new Deacon session via the Daemon
- **Nudge**: Send findings to the Deacon's session
- **No action**: Exit cleanly

## Boot vs Deacon

| Property | Boot | Deacon |
|----------|------|--------|
| **Runs** | Fresh each tick | Persistent session |
| **Observes** | System snapshot | Ongoing monitoring |
| **Decides** | Wake/nudge Deacon | Health actions for all agents |
| **Acts on** | Deacon only | Witnesses, Dogs, escalations |
| **Has state** | None (fresh each run) | Full patrol state |
| **Intelligence** | Triage only | Full health management |

Think of Boot as the alarm clock and the Deacon as the person who gets up and handles the day.

## Fresh Start Advantage

Because Boot starts clean every time, it has unique advantages:

1. **No stale state** -- Boot never carries over corrupted state from a previous run
2. **Crash-proof** -- If Boot crashes, the next tick spawns a fresh instance
3. **Unbiased observation** -- Every run sees the system as it actually is, not as it was
4. **Low resource cost** -- Short-lived sessions use minimal resources

## Location

Boot lives within the Deacon's dog directory:

```
~/gt/deacon/dogs/boot/
├── CLAUDE.md          # Boot agent context and instructions
├── last-run.json      # Timestamp and results of last run
└── triage-log.jsonl   # History of triage decisions
```

## Timing

Boot runs on the daemon's tick cycle:

```
Daemon tick (every ~3 min)
  → Spawn Boot
  → Boot observes, decides, acts
  → Boot exits
  → (wait for next tick)
  → Spawn Boot again (fresh)
```

The Boot run typically completes in seconds -- it is designed for rapid triage, not deep analysis.

## Interaction Diagram

```mermaid
sequenceDiagram
    participant Dm as Daemon
    participant B as Boot
    participant Dc as Deacon

    loop Every daemon tick (~3 min)
        Dm->>B: Spawn fresh instance
        B->>B: Observe system state
        B->>B: Decide action

        alt Deacon is dead
            B->>Dm: Request Deacon wake
            Dm->>Dc: Start new session
        else Deacon is stale
            B->>Dc: Nudge with findings
        else Everything healthy
            B->>B: Exit (no action)
        end
    end
```

## Tips and Best Practices

:::tip[Check Boot Logs for Triage History]

The triage log at `~/gt/deacon/dogs/boot/triage-log.jsonl` provides a timeline of Boot's observations and decisions. This is valuable for diagnosing intermittent issues.

:::

:::tip[Boot Runs Are Fast]

If Boot is taking a long time, something may be wrong with the system state it is trying to observe. Check disk space and agent session states manually.

:::

:::info[Boot Cannot Fix Problems]

Boot only triages and communicates. It wakes the Deacon or nudges it with findings. The Deacon is the agent that takes corrective action. Boot never directly starts Witnesses, kills polecats, or modifies state.

:::

:::info[Boot Is Automatic]

You do not start or stop Boot manually. The Daemon spawns it automatically on every tick. If the Daemon is running, Boot is running.

:::

## The Decision Matrix

Boot uses a precise decision matrix during the Decide phase:

| Deacon State | Activity Level | Action |
|--------------|----------------|--------|
| Dead session | N/A | **START** new Deacon session |
| Active output | N/A | **NOTHING** (healthy) |
| Idle < 5 min | N/A | **NOTHING** (recently active) |
| Idle 5-15 min | Has mail waiting | **NUDGE** with findings |
| Idle > 15 min | N/A | **WAKE** (restart session) |
| Errors/stuck | N/A | **INTERRUPT** (force intervention) |

## Common Patterns

### Normal Healthy Tick

Most Boot runs result in no action:

```
Spawn → Observe: Deacon active, recent output → Decide: NOTHING → Exit
```

This is the expected state. Boot confirms health and exits in seconds.

### Deacon Recovery After Crash

When the Deacon session crashes:

```
Spawn → Observe: Deacon session dead → Decide: START → Act: Request Deacon wake → Exit
Next tick: Spawn → Observe: Deacon alive but fresh → Decide: NUDGE (stale hooks, pending work) → Exit
```

Boot may need 2-3 ticks to fully recover the Deacon: one to start it, subsequent ticks to nudge it with accumulated findings.

### Stale Handoff Cleanup

Boot cleans up stale handoff messages older than 1 hour. This prevents old handoff context from confusing newly spawned sessions.

## Troubleshooting

### Boot Is Not Running

Boot is spawned automatically by the Daemon. If Boot is not running:

```bash
gt daemon status             # Is the Daemon process alive?
gt daemon start              # Restart Daemon if needed
```

Boot has no independent lifecycle -- it exists only when the Daemon spawns it.

### Deacon Is Not Being Woken

If the Deacon is dead and Boot is not starting it:

1. Verify the Daemon is running: `gt daemon status`
2. Check Boot's triage log for recent decisions:
   `~/gt/deacon/dogs/boot/triage-log.jsonl`
3. If Boot is running but deciding NOTHING, it may be misreading the Deacon state

### Boot Runs Are Taking Too Long

Boot is designed for rapid triage (seconds, not minutes). If Boot is slow:

- Disk I/O may be saturated (check disk usage)
- Agent state files may be corrupted or very large
- The system may have many rigs/agents to inventory

## See Also

- **[Deacon](deacon.md)** -- The agent that Boot monitors and wakes
- **[Dogs](dogs.md)** -- Boot is a specialized dog managed by the Deacon
- **[Molecules](../concepts/molecules.md)** -- The `mol-boot-triage` formula defines Boot's triage cycle
- **[Witness](witness.md)** -- Per-rig monitors that Boot inventories during triage
- **[Lifecycle](../operations/lifecycle.md)** -- Agent lifecycle management that Boot helps initiate


========================================================================
# Core Concepts
# URL: /docs/concepts
========================================================================

# Core Concepts

Gas Town is built on a small set of powerful primitives. Understanding these concepts is the key to effective use of the system, whether you are a human operator or an AI agent.

---

## Overview

| Concept | Purpose | Primary CLI |
|---------|---------|-------------|
| [Beads](beads.md) | AI-native, git-backed issue tracking | `bd` |
| [Hooks](hooks.md) | Persistent work state that survives crashes | `gt hook`, `gt sling` |
| [Convoys](convoys.md) | Batch tracking of related work items | `gt convoy` |
| [Molecules & Formulas](molecules.md) | Multi-step workflow orchestration | `gt mol`, `gt formula` |
| [Gates](gates.md) | Async coordination and synchronization | `bd gate` |
| [Rigs](rigs.md) | Project containers wrapping git repositories | `gt rig` |
| [The MEOW Stack](meow-stack.md) | Layered abstraction model for work organization | -- |
| [GUPP & NDI](gupp.md) | Core design principles for reliability at scale | -- |
| [Wisps](wisps.md) | Ephemeral sub-beads tracking molecule steps | `gt mol status` |
| [Session Cycling](session-cycling.md) | Context refresh without losing work | `gt handoff` |
| [Patrol Cycles](patrol-cycles.md) | Periodic health monitoring and self-healing | `gt patrol` |

## How They Fit Together

```mermaid
graph LR
    subgraph Tracking
        B[Beads<br/>Issues]
        C[Convoys<br/>Batches]
    end

    subgraph Execution
        M[Molecules<br/>Workflows]
        F[Formulas<br/>Templates]
        G[Gates<br/>Async Waits]
    end

    subgraph Infrastructure
        H[Hooks<br/>Persistence]
        R[Rigs<br/>Projects]
    end

    F -->|poured into| M
    M -->|blocked by| G
    B -->|bundled into| C
    B -->|attached via| H
    H -->|lives in| R
    M -->|tracks steps as| B
```

## The Lifecycle at a Glance

A typical unit of work moves through these primitives in order:

1. **Bead** is created to track an issue or task
2. Beads are bundled into a **Convoy** for batch tracking
3. A bead is **slung** to an agent, attaching it to the agent's **Hook**
4. The agent executes a **Molecule** (instantiated from a **Formula**) that defines the work steps
5. If a step requires an async wait, a **Gate** pauses execution until a condition is met
6. All of this runs inside a **Rig** -- the project container that hosts the git repository and agent infrastructure

:::tip[Mental Model]

Think of Gas Town as a shipping operation. **Beads** are packages, **Convoys** are trucks, **Hooks** are loading docks, **Molecules** are delivery instructions, **Gates** are traffic lights, and **Rigs** are warehouses.

:::

## Design Principles

These concepts share several common design principles, rooted in [GUPP & NDI](gupp.md):

- **Git as ground truth** -- All persistent state lives in git or git-adjacent storage (SQLite, worktrees, JSONL)
- **CLI-first** -- Every operation is available through the command line, making it natural for both humans and AI agents
- **Crash-safe** -- State is designed to survive crashes, restarts, compaction, and handoffs between sessions (see [Hooks](hooks.md) and [GUPP](gupp.md))
- **Forward-only progress** -- Operations move the system forward or leave it unchanged, never backward ([GUPP](gupp.md))
- **Discovery over tracking** -- Agents observe reality each patrol cycle rather than maintaining fragile in-memory state
- **Composable** -- Primitives combine to form complex workflows without a monolithic orchestration layer


========================================================================
# Beads (Issue Tracking)
# URL: /docs/concepts/beads
========================================================================

# Beads (Issue Tracking)

Beads is Gas Town's **AI-native, git-backed issue tracking system**. Instead of a web-based project board, issues live directly in your repository as structured data, managed entirely through the `bd` CLI. This design makes Beads seamlessly usable by AI coding agents that work through the terminal.

---

## Why Beads?

Traditional issue trackers are designed for humans clicking through web UIs. Beads is designed for AI agents executing terminal commands:

| Traditional Trackers | Beads |
|---------------------|-------|
| Web UI required | CLI-first (`bd` command) |
| External service dependency | Lives in your git repo |
| Context switching to browser | Stays in the terminal |
| Manual sync with code | Automatic git sync |
| Human-oriented workflows | AI-agent-native workflows |

:::info[Repository]

Beads is an open-source project. Learn more at [github.com/steveyegge/beads](https://github.com/steveyegge/beads).

:::

## Architecture

### Storage Backend

Beads stores issues in a **SQLite database** located in the `.beads/` directory at the root of each repository (or town):

```
.beads/
├── beads.db           # SQLite database (primary store)
├── formulas/          # TOML workflow templates
├── README.md          # Onboarding documentation
└── daemon.log         # Daemon activity log
```

The SQLite backend enables fast queries, filtering, and complex joins while remaining portable and easy to back up through git.

### Git Integration

Beads synchronizes with git automatically:

- **`bd sync`** pushes and pulls bead state to/from the remote
- Bead operations are local-first -- they work offline and sync when connected
- The `.beads/` directory is committed alongside your code
- Merge conflicts in bead data are resolved intelligently

## Bead Types

Every bead has a **type** that determines its semantics:

| Type | Purpose | Example |
|------|---------|---------|
| `task` | General work item | "Refactor auth module" |
| `bug` | Defect report | "Login fails with special characters" |
| `feature` | New functionality | "Add email notifications" |
| `message` | Communication record | Internal agent message |
| `escalation` | Priority alert | "CI broken for 2 hours" |
| `merge-request` | Merge queue entry | Polecat branch ready for merge |
| `agent` | Agent state bead | Polecat runtime status |
| `convoy` | Batch tracking | Group of related issues (see [Convoys](convoys.md)) |
| `wisp` | Ephemeral tracking | Temporary [molecule](molecules.md) step |

## Bead Status

Beads progress through a defined lifecycle:

```mermaid
stateDiagram-v2
    [*] --> pending: bd create
    pending --> open: Agent claims
    open --> in_progress: Work begins
    in_progress --> hooked: gt sling
    hooked --> in_progress: Agent picks up
    in_progress --> done: bd close
    done --> [*]
```

| Status | Meaning | Typical Transition |
|--------|---------|-------------------|
| `pending` | Created, not yet assigned | Initial state after `bd create` |
| `open` | Acknowledged, ready for work | Agent or human claims it |
| `in_progress` | Actively being worked on | Agent starts implementation |
| `hooked` | Attached to an agent's [hook](hooks.md) | After `gt sling` assigns it |
| `done` | Completed and closed | After `bd close` or merge |

## Labels, Priorities, and Dependencies

### Labels

Labels are free-form tags that categorize beads:

```bash
bd create --title "Fix auth bug" --labels "auth,security,p1"
bd list --labels "security"
```

### Priorities

Priority levels control escalation routing and work ordering:

| Priority | Code | Escalation Route |
|----------|------|-----------------|
| Critical | P0 | Bead, Mail:Mayor, Email:Human, SMS:Human |
| High | P1 | Bead, Mail:Mayor, Email:Human |
| Medium | P2 | Bead, Mail:Mayor |
| Low | P3 | Bead only |

```bash
bd create --title "Security vulnerability" --priority 0
bd create --title "Minor UI glitch" --priority 3
```

### Dependencies

Beads can declare dependencies on other beads, enabling automatic unblocking when prerequisites complete:

```bash
# Create a dependent bead
bd create --title "Deploy to prod" --depends-on gt-a1b2c

# Check blocked issues
bd blocked
```

## Cross-Project Tracking

Beads supports **cross-prefix tracking**, allowing issues in different rigs to reference each other. Each rig has its own bead prefix (configured in `config.json`):

```
Town (.beads/)  prefix: hq-
Rig A (.beads/) prefix: gt-
Rig B (.beads/) prefix: bd-
```

A [convoy](convoys.md) with ID `hq-cv-001` can track issues `gt-a1b2c` and `bd-d3e4f` across both [rigs](rigs.md). Dependencies also work cross-prefix.

## Essential Commands

### Creating Beads

```bash
# Simple creation
bd create "Add user authentication"

# Full creation with metadata
bd create --title "Fix login bug" \
  --type bug \
  --priority 1 \
  --labels "auth,critical" \
  --description "Login fails when password contains special characters"
```

### Listing and Filtering

```bash
# List all open beads
bd list

# Filter by status
bd list --status in_progress

# Filter by type and labels
bd list --type bug --labels "auth"

# JSON output for programmatic use
bd list --json

# Find ready work
bd ready
```

### Viewing Bead Details

```bash
# Show full bead details
bd show gt-a1b2c

# Show bead as JSON
bd show gt-a1b2c --json
```

### Updating Beads

```bash
# Update status
bd update gt-a1b2c --status in_progress

# Add notes
bd update gt-a1b2c --notes "Fixed the parser, testing now"

# Add labels
bd update gt-a1b2c --labels "reviewed"
```

### Closing Beads

```bash
# Close a completed bead
bd close gt-a1b2c

# Close with a reason
bd close gt-a1b2c --reason "Merged to main at abc1234"
```

### Syncing with Git

```bash
# Sync bead state with remote
bd sync

# Onboard to a repo (first time setup)
bd onboard
```

## Command Reference

| Command | Description |
|---------|-------------|
| `bd create` | Create a new bead |
| `bd list` | List beads with optional filters |
| `bd show <id>` | Show full details of a bead |
| `bd update <id>` | Update bead metadata |
| `bd close <id>` | Close a completed bead |
| `bd sync` | Sync bead state with git remote |
| `bd ready` | Find available work (pending/open beads) |
| `bd onboard` | First-time setup for a repository |
| `bd prime` | Load beads context into agent session |
| `bd blocked` | Show blocked beads waiting on dependencies |
| `bd quickstart` | Interactive getting-started guide |

## For AI Agents

Beads is specifically designed for AI agent workflows:

:::note[Agent Quick Start]

```bash
bd ready              # Find available work
bd show <id>          # Read the full issue
bd update <id> --status in_progress  # Claim it
# ... do the work ...
bd close <id>         # Mark complete
bd sync               # Push state to remote
```

:::

Agents use `bd ready` at the start of each session to find their next task. Combined with [Hooks](hooks.md), this creates a self-propelling work loop where agents always know what to do.

:::warning[Landing Protocol]

Work is **not complete** until `git push` succeeds. Agents must always push their changes and sync beads before ending a session. See the [AGENTS.md](https://github.com/steveyegge/gastown) landing protocol for the full checklist.

:::

## See Also

- **[Hooks](hooks.md)** -- Hooks attach beads to agents, creating the `hooked` status and enabling crash-safe work assignment
- **[Molecules & Formulas](molecules.md)** -- Molecules track multi-step workflows as a sequence of wisp beads (ephemeral sub-beads)
- **[Convoys](convoys.md)** -- Convoys bundle multiple beads into batches for coordinated tracking
- **[Rigs](rigs.md)** -- Each rig has its own `.beads/` directory with a unique prefix for cross-project identification
- **[Gates](gates.md)** -- Gates can block bead progress, pausing workflow until an external condition is met
- **[GUPP & NDI](gupp.md)** -- Bead statuses follow GUPP's forward-only principle: they progress from `open` to `done` and never go backward


========================================================================
# Hooks (Persistence)
# URL: /docs/concepts/hooks
========================================================================

# Hooks (Persistence)

Hooks are Gas Town's **durability primitive**. A hook is a persistent attachment point where work state is stored in a way that survives crashes, restarts, handoffs, context compaction, and even machine failures. Hooks are what make Gas Town agents self-propelling -- an agent always knows what to do by checking its hook.

---

## The Problem Hooks Solve

AI coding agents are inherently ephemeral. Sessions can end for many reasons:

- Context window fills up
- Session crashes or times out
- Operator requests a handoff
- Machine restarts

Without hooks, all in-flight work state would be lost on every session boundary. The agent would restart with no memory of what it was doing.

:::danger[Without Hooks]

Agent starts -> Does half the work -> Context fills up -> Session ends -> **All progress context lost** -> New session has no idea what to do

:::

:::success[With Hooks]

Agent starts -> Does half the work -> Context fills up -> Session ends -> New session checks hook -> **Finds work + molecule progress** -> Resumes seamlessly

:::

## How Hooks Work

Hooks are implemented as **git worktrees** with attached metadata. When work is "hooked" to an agent, the bead ID and molecule state are recorded in a persistent location tied to that agent's working directory.

```mermaid
graph TD
    A[Agent Session 1] -->|crashes| X[Session Lost]
    H[Hook<br/>git worktree] -->|persists| H
    A -->|writes progress to| H
    B[Agent Session 2] -->|reads from| H
    B -->|resumes work| W[Continues Work]
```

The hook stores:

- **Hook bead** -- The bead ID of the assigned work
- **Molecule state** -- Which step of the workflow the agent was on
- **Branch state** -- The git branch and any uncommitted progress

Because this is all stored in the git worktree (filesystem), it survives any session boundary.

## The Propulsion Principle

> **"If it's on your hook, YOU RUN IT."**

This is Gas Town's core scheduling rule. It replaces centralized job schedulers with a simple, crash-safe protocol:

1. Agent starts a new session
2. Agent runs `gt prime` to load context
3. Agent checks `gt hook` for attached work
4. **If work found** -- Execute it immediately
5. **If no work** -- Check inbox, then wait for instructions

This creates **automatic momentum**. Agents are self-propelled by their hooks. No coordinator needs to tell them what to do -- they discover it themselves every time they start.

```mermaid
flowchart TD
    Start[Session Start] --> Prime[gt prime]
    Prime --> Hook{gt hook}
    Hook -->|Work found| Execute[Execute molecule]
    Hook -->|No work| Inbox[Check gt mail inbox]
    Inbox -->|Mail found| Process[Process messages]
    Inbox -->|Empty| Wait[Wait for instructions]
    Execute -->|Done| Submit[gt done]
    Execute -->|Context full| Handoff[gt handoff]
    Handoff --> Start
```

## Commands

### Checking Your Hook

```bash
# Show what is currently on your hook
gt hook
```

Output shows the hooked bead ID and any attached molecule:

```
Hook: gt-a1b2c  "Fix login bug"
  Molecule: mol-polecat-work (step: implement)
  Branch: polecat/toast
  Status: in_progress
```

### Manually Hooking Work

```bash
# Attach a bead to your hook
gt hook gt-a1b2c
```

This is rarely done manually. Most hooking happens through `gt sling`.

### Slinging Work

The `gt sling` command is the primary way to assign work to agents. It hooks a bead to the target and spawns a worker:

```bash
# Assign to a rig (auto-spawns a polecat)
gt sling gt-a1b2c myproject

# Assign to a specific agent
gt sling gt-a1b2c myproject --agent cursor

# Assign multiple items
gt sling gt-a1b2c gt-d3e4f myproject
```

What `gt sling` does internally:

1. Changes bead status to `hooked`
2. Attaches work to the target agent's hook
3. Spawns a polecat (ephemeral worker) in the rig
4. The polecat's startup sequence finds the hook
5. The polecat begins executing the assigned molecule

### Removing Work from a Hook

```bash
# Remove a bead from the hook without completing it
gt unsling gt-a1b2c
```

This releases the work back to the available pool without marking it done. Another agent can pick it up later.

## Hook Persistence Guarantees

Hooks are the core mechanism behind [GUPP](gupp.md) (the Gas Town Universal Propulsion Principle). They ensure that no session boundary can lose work state. Hooks survive every type of disruption:

| Disruption | Hook Status |
|-----------|-------------|
| Session restart | Preserved -- new session reads hook on startup |
| Context compaction | Preserved -- hook is in filesystem, not context |
| Agent crash | Preserved -- git worktree is durable |
| Handoff (`gt handoff`) | Preserved -- successor session inherits hook |
| Machine reboot | Preserved -- git worktree is on disk |
| Manual session kill | Preserved -- hook outlives the process |

## How Hooks Drive Agent Behavior

Different agent roles respond to hooks differently:

### Polecats (Ephemeral Workers)

When a polecat spawns:

1. `gt prime` runs automatically (SessionStart hook)
2. Prime reads the hook and injects the assigned bead
3. Polecat executes the `mol-polecat-work` molecule
4. On completion, `gt done` submits work and nukes the sandbox
5. **Done means gone** -- the polecat ceases to exist

### Persistent Agents (Witness, Refinery, Deacon)

Persistent agents use hooks to track their patrol molecules:

1. On startup, check hook for active patrol molecule
2. If found, resume the patrol from the last completed step
3. If not found, create a new patrol molecule and hook it
4. Run patrol cycles until context fills up
5. Handoff to fresh session, which picks up from the hook

### The Mayor

The Mayor's hook typically holds a coordination molecule or convoy management task. The Mayor checks its hook on each session start to resume strategic planning.

## Hook and Molecule Integration

Hooks and [Molecules](molecules.md) work together to provide crash-safe workflows:

```
Hook
├── hook_bead: gt-a1b2c        # The assigned issue
└── molecule: mol-polecat-work  # The workflow template
    ├── step: load-context      [done]
    ├── step: branch-setup      [done]
    ├── step: implement         [in_progress]  <-- resume here
    ├── step: self-review       [pending]
    └── step: submit-and-exit   [pending]
```

When a session restarts, the agent:

1. Reads the hook to find `gt-a1b2c`
2. Reads the molecule to find it is on the `implement` step
3. Resumes implementation without repeating earlier steps

This is why Gas Town agents can work on complex tasks across many sessions without losing progress.

:::tip[Best Practice]

Always check `gt hook` at the start of a session before doing anything else. If work is on your hook, that is your top priority. The Propulsion Principle ensures agents stay focused and productive.

:::

## See Also

- **[Beads](beads.md)** -- The hook stores the bead ID of the assigned work; the bead's status transitions to `hooked` when slung
- **[Molecules & Formulas](molecules.md)** -- The molecule attached to a hook tracks step-level progress, enabling crash-safe resume
- **[GUPP & NDI](gupp.md)** -- Hooks are the primary mechanism that makes GUPP possible: work state persists across every kind of disruption
- **[Rigs](rigs.md)** -- Hooks are implemented as git worktrees within a rig's directory structure
- **[Gates](gates.md)** -- When a molecule step is gated, the hook preserves the parked state until the gate closes
- **[Session Cycling](session-cycling.md)** -- Hooks persist across session boundaries, enabling context refresh without losing work


========================================================================
# Convoys (Batch Tracking)
# URL: /docs/concepts/convoys
========================================================================

# Convoys (Batch Tracking)

A **Convoy** is Gas Town's unit for tracking batches of related work. Convoys bundle multiple beads together, provide a single point of progress monitoring, auto-close when all tracked items complete, and notify subscribers on landing. They are the primary coordination mechanism for multi-issue projects.

---

## Why Convoys?

When the Mayor receives a complex request -- "Fix the auth module, add email validation, and update the docs" -- that becomes three separate [beads](beads.md). Without convoys, tracking which of those three are done, which are in progress, and when the entire batch has landed would require manual checking.

Convoys solve this:

```mermaid
graph TD
    CV["Convoy: hq-cv-001<br/>'Auth System Fixes'"]
    B1["gt-a1b2c<br/>Fix login bug<br/>Status: done"]
    B2["gt-d3e4f<br/>Add email validation<br/>Status: in_progress"]
    B3["gt-g5h6i<br/>Update README<br/>Status: pending"]

    CV -->|tracks| B1
    CV -->|tracks| B2
    CV -->|tracks| B3

    style B1 fill:#2d5a2d
    style B2 fill:#5a5a2d
    style B3 fill:#5a2d2d
```

## Convoy Lifecycle

```mermaid
stateDiagram-v2
    [*] --> Open: gt convoy create
    Open --> Open: Issues completing...
    Open --> Completed: All tracked issues done
    Completed --> [*]: Auto-close + notify

    Open --> Stalled: Issues stuck
    Stalled --> Open: Unstuck / reassigned
```

| State | Meaning |
|-------|---------|
| **Open** | At least one tracked issue is not yet done |
| **Completed** | All tracked issues have closed |
| **Stalled** | Issues exist but none are making progress |

## Convoy IDs

Convoys use the `hq-cv-` prefix by default (since they are town-level coordination beads):

```
hq-cv-001   # First convoy
hq-cv-abc   # Auto-generated short ID
```

The `hq-` prefix indicates the bead belongs to the town-level `.beads/` database, not a rig-level one.

## Auto-Close Behavior

This is the key feature of convoys. The system automatically checks whether all tracked issues have completed:

1. During each Deacon patrol cycle, the `check-convoy-completion` step runs
2. For each open convoy, the Deacon queries all tracked issue statuses
3. If every tracked issue is in a terminal state (`done`, `wontfix`, etc.), the convoy auto-closes
4. On close, subscribers (typically the Mayor and Overseer) receive a notification

```bash
# The Deacon runs this check automatically:
bd list --type=convoy --status=open
# For each: check tracked issues, close if all done
```

:::info[No Manual Closing Needed]

You almost never need to close a convoy manually. The auto-close mechanism handles it. If you need to force-close a convoy (for example, abandoning remaining work), use `gt convoy close`.

:::

## Cross-Project Tracking

Convoys can track issues **across multiple rigs**. A single convoy can reference beads from different projects:

```bash
gt convoy create "Cross-rig refactor" gt-a1b2c bd-d3e4f myrig-x5y6z
```

This works because convoys support **cross-prefix tracking** -- the convoy bead stores full IDs including their prefix, allowing it to query status from any [rig's](rigs.md) `.beads/` database.

## Dashboard Visibility

Convoys provide a high-level dashboard view of work progress:

```bash
$ gt convoy list

ID          Title                  Status      Progress
hq-cv-001   Auth System Fixes     OPEN        2/3 done
hq-cv-002   API Redesign          OPEN        0/5 done
hq-cv-003   Bug Bash Sprint       COMPLETED   8/8 done
```

The Mayor uses this dashboard to report progress to the Overseer (human operator) and make strategic decisions about resource allocation.

## Commands

### Creating a Convoy

```bash
# Create from existing beads
gt convoy create "Auth System Fixes" gt-a1b2c gt-d3e4f gt-g5h6i
# Created: hq-cv-001

# Create with just a name (add issues later)
gt convoy create "Sprint 42"
# Created: hq-cv-002
```

### Adding Issues to a Convoy

```bash
# Add a single issue
gt convoy add hq-cv-001 gt-h7i8j

# Add multiple issues
gt convoy add hq-cv-001 gt-h7i8j gt-k9l0m
```

### Listing Convoys

```bash
# List all convoys
gt convoy list

# List only open convoys
gt convoy list --status open
```

### Checking Convoy Status

```bash
# Show convoy details with tracked issue status
gt convoy show hq-cv-001

# Get status summary
gt convoy status hq-cv-001
```

Example output:

```
Convoy: hq-cv-001 "Auth System Fixes"
Status: OPEN
Progress: 2/3 issues completed

Tracked Issues:
  [DONE]        gt-a1b2c  Fix login bug
  [IN_PROGRESS] gt-d3e4f  Add email validation
  [PENDING]     gt-g5h6i  Update README

Created: 2026-01-15T10:30:00Z
```

### Closing a Convoy

```bash
# Manual close (usually not needed -- auto-close handles this)
gt convoy close hq-cv-001

# Close with reason
gt convoy close hq-cv-001 --reason "Remaining work deprioritized"
```

### Checking Completion

```bash
# Manually trigger completion check
gt convoy check hq-cv-001
```

### Finding Stranded Convoys

A stranded convoy has ready work that is not assigned to any agent:

```bash
# Find convoys with unassigned work
gt convoy stranded
```

This is useful for the Mayor to identify work that needs to be [slung](hooks.md) to polecats.

## Command Reference

| Command | Description |
|---------|-------------|
| `gt convoy create <title> [bead-ids...]` | Create a new convoy |
| `gt convoy add <convoy-id> <bead-ids...>` | Add issues to a convoy |
| `gt convoy list` | List all convoys |
| `gt convoy status <convoy-id>` | Show convoy progress summary |
| `gt convoy show <convoy-id>` | Show full convoy details |
| `gt convoy close <convoy-id>` | Manually close a convoy |
| `gt convoy check <convoy-id>` | Check if convoy can auto-close |
| `gt convoy stranded` | Find convoys with unassigned work |

## Convoy + Mayor Workflow

In the recommended [Mayor Workflow](../workflows/mayor-workflow.md), convoys are created automatically:

1. You describe work to the Mayor
2. Mayor creates beads for each task
3. Mayor bundles beads into a convoy
4. Mayor slings work to rigs (spawning polecats)
5. Mayor monitors convoy progress
6. Convoy auto-closes when everything lands
7. Mayor notifies you of completion

You can also create convoys manually for finer control -- see the [Manual Convoy Workflow](../workflows/manual-convoy.md).

:::tip[Monitoring Progress]

Run `gt convoy list` frequently to see the big picture. For live progress, use `gt feed` to watch the activity stream as polecats complete work and convoys close.

:::

## See Also

- **[Beads](beads.md)** -- Convoys track collections of beads; each tracked item is a bead with its own status and lifecycle
- **[Hooks](hooks.md)** -- When work is slung to a polecat, the bead is hooked to that agent for execution
- **[Molecules & Formulas](molecules.md)** -- Each bead in a convoy is executed through a molecule that defines the work steps
- **[Rigs](rigs.md)** -- Convoys can span multiple rigs for cross-project coordination
- **[The MEOW Stack](meow-stack.md)** -- Convoys represent Layer 2 (Epics) in the MEOW abstraction model
- **[Gates](gates.md)** -- Individual beads within a convoy may be gated on external conditions


========================================================================
# Molecules & Formulas
# URL: /docs/concepts/molecules
========================================================================

# Molecules & Formulas

**Molecules** are Gas Town's multi-step workflow execution units. A molecule guides an agent through a sequence of steps, tracking progress so that work can resume after crashes, handoffs, or context compaction. **Formulas** are the TOML-defined templates from which molecules are created.

---

## The Relationship

Think of it as a class-instance relationship:

| Concept | Analogy | Storage |
|---------|---------|---------|
| **Formula** | Blueprint / Template | `.beads/formulas/*.formula.toml` |
| **Molecule** | Running instance | Bead in `.beads/beads.db` |

A formula is **poured** into a molecule -- this creates a live workflow instance with step beads that track progress. Multiple molecules can be poured from the same formula simultaneously.

```mermaid
graph LR
    F["Formula<br/>(TOML template)"] -->|pour| M1["Molecule Instance 1"]
    F -->|pour| M2["Molecule Instance 2"]
    F -->|pour| M3["Molecule Instance 3"]

    M1 --> S1a["Step: load-context ✓"]
    M1 --> S1b["Step: implement ●"]
    M1 --> S1c["Step: test ○"]
```

## Molecules

### What a Molecule Tracks

A molecule is a bead that contains:

- **Steps** -- Ordered list of work items, each tracked as a sub-bead (wisp)
- **Dependencies** -- Which steps must complete before others can start
- **Progress** -- Which steps are done, in progress, or pending
- **Variables** -- Runtime values injected from the formula (e.g., issue ID)
- **[Gates](gates.md)** -- Async wait conditions that pause execution

### Step States

Each step in a molecule has a status:

| Symbol | State | Meaning |
|--------|-------|---------|
| `○` | pending | Not yet started, waiting for dependencies |
| `●` | in_progress | Currently being worked on |
| `✓` | done | Completed successfully |
| `✗` | failed | Failed (may need retry) |
| `⏸` | gated | Waiting on an async [gate](gates.md) |

### Step Dependencies

Steps declare dependencies using the `needs` field. A step cannot start until all its dependencies are complete:

```mermaid
graph TD
    A["load-context"] --> B["branch-setup"]
    B --> C["preflight-tests"]
    C --> D["implement"]
    D --> E["self-review"]
    E --> F["run-tests"]
    F --> G["cleanup-workspace"]
    G --> H["prepare-for-review"]
    H --> I["submit-and-exit"]
```

### Molecule Types

Gas Town uses molecules for two distinct purposes:

**Work molecules** -- Guide a polecat through a single task:

- `mol-polecat-work` -- Full polecat work lifecycle
- `mol-polecat-code-review` -- Code review assignment
- `mol-polecat-conflict-resolve` -- Merge conflict resolution

**Patrol molecules** -- Guide persistent agents through monitoring cycles:

- `mol-witness-patrol` -- Witness health monitoring loop
- `mol-refinery-patrol` -- Merge queue processing loop
- `mol-deacon-patrol` -- Deacon background coordination loop

### Molecule Commands

```bash
# Show current molecule status
gt mol status

# Show the molecule attached to current session
gt mol current

# Show progress of a specific molecule
gt mol progress <mol-id>

# Mark the current step as done
gt mol step done

# Attach to an existing molecule
gt mol attach <mol-id>

# Detach from current molecule
gt mol detach

# Burn a completed molecule (archive)
gt mol burn <mol-id>

# Squash a molecule into a digest bead
gt mol squash <mol-id> --summary "Patrol cycle: 3 merges, 0 failures"

# Show the dependency graph
gt mol dag <mol-id>
```

### The Squash Pattern

Patrol molecules use a **squash-and-respawn** pattern to avoid accumulating stale step beads:

1. Complete a patrol cycle
2. Squash the molecule into a single digest bead: `gt mol squash`
3. Create a fresh patrol molecule for the next cycle
4. The digest preserves audit trail without per-step pollution

This keeps the beads database clean while maintaining full observability.

## Formulas

### Formula Structure

Formulas are TOML files stored in `.beads/formulas/`. Here is the anatomy of a formula:

```toml
# Top-level metadata
description = "What this formula does"
formula = "formula-name"
version = 1

# Steps define the workflow
[[steps]]
id = "step-one"
title = "First Step"
description = "Detailed instructions for this step"

[[steps]]
id = "step-two"
title = "Second Step"
needs = ["step-one"]   # Depends on step-one completing first
description = "Instructions for step two"

# Variables injected at pour time
[vars]
[vars.feature]
description = "The feature being implemented"
required = true

[vars.assignee]
description = "Who is assigned"
```

### Formula Types

Formulas come in two flavors:

**Workflow formulas** (`type = "workflow"`) -- Linear or DAG-shaped step sequences executed by a single agent:

```toml
formula = "shiny"
type = "workflow"

[[steps]]
id = "design"
title = "Design {{feature}}"

[[steps]]
id = "implement"
needs = ["design"]
title = "Implement {{feature}}"
```

**Convoy formulas** (`type = "convoy"`) -- Parallel execution with multiple agents (legs), followed by a synthesis step:

```toml
formula = "code-review"
type = "convoy"

[[legs]]
id = "correctness"
title = "Correctness Review"

[[legs]]
id = "security"
title = "Security Review"

[synthesis]
title = "Review Synthesis"
depends_on = ["correctness", "security"]
```

### Built-in Formulas

Gas Town ships with 30+ built-in formulas:

#### Work Formulas

| Formula | Type | Description |
|---------|------|-------------|
| `mol-polecat-work` | workflow | Full polecat work lifecycle (9 steps) |
| `mol-polecat-code-review` | workflow | Polecat code review assignment |
| `mol-polecat-conflict-resolve` | workflow | Merge conflict resolution |
| `mol-polecat-review-pr` | workflow | Pull request review |
| `mol-polecat-lease` | workflow | Polecat lease management |
| `shiny` | workflow | Design-implement-review-test-submit |
| `shiny-secure` | workflow | Shiny with security audit step |
| `shiny-enterprise` | workflow | Shiny with full enterprise gates |

#### Patrol Formulas

| Formula | Type | Description |
|---------|------|-------------|
| `mol-witness-patrol` | workflow | Witness per-rig health monitoring |
| `mol-refinery-patrol` | workflow | Refinery merge queue processing |
| `mol-deacon-patrol` | workflow | Deacon background coordination |
| `mol-boot-triage` | workflow | Boot dog triage assessment |

#### Convoy Formulas

| Formula | Type | Description |
|---------|------|-------------|
| `code-review` | convoy | Parallel multi-dimension code review |
| `design` | convoy | Parallel design exploration |
| `security-audit` | convoy | Security-focused analysis |
| `rule-of-five` | convoy | Five-perspective analysis |

#### Operational Formulas

| Formula | Type | Description |
|---------|------|-------------|
| `mol-convoy-cleanup` | workflow | Clean up completed convoys |
| `mol-convoy-feed` | workflow | Generate convoy activity feed |
| `mol-dep-propagate` | workflow | Propagate dependency resolution |
| `mol-digest-generate` | workflow | Generate patrol digest |
| `mol-gastown-boot` | workflow | Town cold-start bootstrap |
| `mol-orphan-scan` | workflow | Scan for orphaned work |
| `mol-session-gc` | workflow | Session garbage collection |
| `mol-shutdown-dance` | workflow | Graceful rig shutdown |
| `mol-sync-workspace` | workflow | Workspace synchronization |
| `mol-town-shutdown` | workflow | Town-wide shutdown |
| `gastown-release` | workflow | Gas Town release process |
| `beads-release` | workflow | Beads release process |

### Formula Commands

```bash
# List all available formulas
gt formula list

# Show formula details
gt formula show mol-polecat-work

# Run a formula (pour into a molecule)
gt formula run shiny --var feature="Add notifications"

# Run a convoy formula
gt formula run code-review --pr=123

# Create a custom formula
gt formula create my-workflow
```

### Creating Custom Formulas

You can create your own formulas by adding TOML files to `.beads/formulas/`:

```toml
# .beads/formulas/my-deploy.formula.toml
description = "Production deployment workflow"
formula = "my-deploy"
type = "workflow"
version = 1

[[steps]]
id = "pre-checks"
title = "Run pre-deployment checks"
description = "Verify build passes, tests green, no blockers"

[[steps]]
id = "deploy-staging"
title = "Deploy to staging"
needs = ["pre-checks"]
description = "Push to staging environment and verify"

[[steps]]
id = "smoke-test"
title = "Run smoke tests"
needs = ["deploy-staging"]
description = "Execute smoke test suite against staging"

[[steps]]
id = "deploy-prod"
title = "Deploy to production"
needs = ["smoke-test"]
description = "Push to production with canary rollout"

[vars]
[vars.version]
description = "Version being deployed"
required = true
```

Then run it:

```bash
gt formula run my-deploy --var version="2.3.1"
```

## Molecule + Hook Integration

Molecules are stored on an agent's [Hook](hooks.md), creating the crash-safe execution model:

```
Hook
├── hook_bead: gt-a1b2c            # The assigned issue
└── molecule: mol-polecat-work     # Active workflow
    ├── load-context      [done]
    ├── branch-setup      [done]
    ├── implement         [in_progress]  <-- resume here
    ├── self-review       [pending]
    └── submit-and-exit   [pending]
```

On session restart, the agent reads the hook, finds the molecule, and resumes from the last completed step. No work is repeated.

:::tip[When to Use Formulas]

Use the built-in `mol-polecat-work` for standard feature work. Use `shiny` for design-first workflows. Use `code-review` for thorough parallel code reviews. Create custom formulas when you have a repeatable multi-step process that agents should follow consistently.

:::

## See Also

- **[Hooks](hooks.md)** -- Molecules are stored on an agent's hook, enabling crash-safe workflow resume across sessions
- **[Beads](beads.md)** -- Each molecule step is tracked as a wisp bead; the molecule itself is also a bead
- **[Wisps](wisps.md)** -- Ephemeral sub-beads that track individual molecule steps
- **[Gates](gates.md)** -- Steps can be gated on external conditions (CI, human approval, timers), pausing the molecule until the gate closes
- **[Convoys](convoys.md)** -- Convoy formulas orchestrate multiple molecules in parallel with a synthesis step
- **[Rigs](rigs.md)** -- Formulas are stored in the rig's `.beads/formulas/` directory; molecules execute within a rig's context
- **[GUPP & NDI](gupp.md)** -- Molecules implement GUPP through step-level checkpointing: completed steps are never re-executed, and NDI allows different agents to produce different but equivalent implementations
- **[The MEOW Stack](meow-stack.md)** -- Molecules are Layer 3, Formulas are Layer 5 in the MEOW abstraction model


========================================================================
# Gates (Async Coordination)
# URL: /docs/concepts/gates
========================================================================

# Gates (Async Coordination)

**Gates** are synchronization points in Gas Town workflows. When a molecule step needs to wait for an external condition -- a CI pipeline finishing, a human approving a change, or a timer elapsing -- it parks on a gate. The gate holds the workflow until the condition is met, then releases it to continue.

---

## Why Gates?

AI agents work fast. But some things cannot be sped up:

- CI pipelines take minutes to run
- Humans need time to review and approve
- External services have their own timelines
- Timers enforce cooldown periods

Without gates, an agent would need to busy-wait (burning context and cost) or exit and lose its place. Gates solve this by **parking** the workflow and **resuming** it when the condition clears. This aligns with [GUPP](gupp.md) -- the workflow's progress is preserved, and it resumes forward when the gate opens.

```mermaid
sequenceDiagram
    participant Agent
    participant Gate
    participant External

    Agent->>Gate: Create gate (await CI)
    Agent->>Agent: Park workflow, exit
    Note over Agent: Session ends cleanly

    External->>Gate: CI completes
    Gate->>Gate: Condition met, close

    Note over Agent: Next patrol cycle
    Agent->>Gate: Check gate status
    Gate-->>Agent: CLOSED (ready)
    Agent->>Agent: Resume workflow
```

## Gate Types

Gas Town supports several gate types, each waiting on a different kind of external condition:

| Type | Await Key | Condition | Who Closes It |
|------|-----------|-----------|--------------|
| **Timer** | `timer` | Elapsed time since creation exceeds timeout | Deacon patrol (automatic) |
| **GitHub Actions** | `gh:run` | GitHub Actions workflow run completes | Deacon patrol (polls GitHub) |
| **GitHub PR** | `gh:pr` | Pull request reaches target state | Deacon patrol (polls GitHub) |
| **Human Approval** | `human` | A human explicitly approves | Human via `bd gate approve` |
| **Mail** | `mail` | A specific mail message arrives | Mail system (automatic) |

### Timer Gates

Timer gates are the simplest type. They close automatically after a specified duration:

```bash
# Create a gate that opens after 30 minutes
bd gate create --type timer --timeout 30m --title "Cooldown before retry"
```

The Deacon's patrol cycle checks all timer gates and closes any that have elapsed:

```bash
# Deacon runs this automatically:
bd gate check --type=timer --escalate
```

:::note[Escalation on Expiry]

Timer gates do not just silently close. When they expire, they escalate to the overseer for awareness. This ensures human oversight of timeout conditions.

:::

### GitHub Actions Gates

These gates wait for a GitHub Actions workflow run to complete:

```bash
# Wait for CI to pass on a specific commit
bd gate create --type gh:run --run-id 12345 --title "Wait for CI"
```

The Deacon polls GitHub during patrol cycles to check run status.

### Human Approval Gates

Human gates require explicit human action to proceed. They are used for critical decisions that should not be automated:

```bash
# Create a gate requiring human approval
bd gate create --type human --title "Approve production deploy"
```

The gate stays open until a human explicitly approves it:

```bash
# Human approves the gate
bd gate approve gt-gate-123
```

### Mail Gates

Mail gates close when a specific message arrives in an agent's mailbox:

```bash
# Wait for a MERGED notification
bd gate create --type mail --subject "MERGED polecat/toast" --title "Wait for merge"
```

## Gate Lifecycle

```mermaid
stateDiagram-v2
    [*] --> Open: bd gate create
    Open --> Open: Condition not yet met
    Open --> Closed: Condition met / approved
    Closed --> [*]: Workflow resumes

    Open --> Expired: Timer exceeds limit
    Expired --> Escalated: Auto-escalate
    Escalated --> Closed: Human resolves
```

| State | Meaning |
|-------|---------|
| **Open** | Waiting for condition to be met |
| **Closed** | Condition met, workflow can resume |
| **Expired** | Timer elapsed, escalated for attention |

## Gates in Molecules

Gates are typically embedded in [molecule](molecules.md) steps. When a step encounters a gate, the workflow pauses at that step until the gate closes.

### Example: CI Gate in a Deploy Workflow

```toml
[[steps]]
id = "run-ci"
title = "Trigger CI and wait for results"
description = """
Trigger the CI pipeline and create a gate to wait for completion.

```bash
# Trigger CI
gh workflow run ci.yml --ref $(git branch --show-current)

# Create gate to wait for CI
bd gate create --type gh:run --run-id <run-id> --title "Wait for CI"

# Park on the gate
gt mol step park --gate <gate-id>
```

The Deacon will close this gate when CI completes. Your next patrol
cycle (or a fresh session) will pick up from here.
"""
```

### Example: Human Gate Before Production Deploy

```toml
[[steps]]
id = "human-approval"
title = "Get human approval for production deploy"
description = """
Create a human gate and wait for approval.

```bash
bd gate create --type human --title "Approve deploy v2.3.1 to production"

# Notify the overseer
gt mail send mayor/ -s "APPROVAL NEEDED: Deploy v2.3.1" \
  -m "Please review and approve: bd gate approve <gate-id>"
```

This step cannot proceed until a human runs `bd gate approve`.
"""
```

## Gate Evaluation by the Deacon

The Deacon is responsible for evaluating gates during its patrol cycle. The `gate-evaluation` step in the `mol-deacon-patrol` formula handles this:

1. **List all open gates**: `bd gate list --json`
2. **For each timer gate**: Check if `created_at + timeout < now`
3. **For each GitHub gate**: Poll the GitHub API for run/PR status
4. **Close ready gates**: `bd gate close <id> --reason "Condition met"`
5. **Notify waiters**: Send mail to agents blocked on the gate

After gate evaluation, the `dispatch-gated-molecules` step finds molecules that were blocked on now-closed gates and dispatches them:

```bash
# Find molecules ready to resume
bd mol ready --gated --json

# Dispatch each to the appropriate rig
gt sling <mol-id> <rig>/polecats
```

## Commands

### Creating Gates

```bash
# Timer gate
bd gate create --type timer --timeout 30m --title "Cooldown period"

# Human approval gate
bd gate create --type human --title "Approve deploy"

# GitHub Actions gate
bd gate create --type gh:run --run-id 12345 --title "Wait for CI"

# Mail gate
bd gate create --type mail --subject "MERGED" --title "Wait for merge"
```

### Viewing Gates

```bash
# Show gate details
bd gate show <gate-id>

# List all open gates
bd gate list

# List gates as JSON
bd gate list --json
```

### Closing Gates

```bash
# Close a gate (condition met)
bd gate close <gate-id> --reason "CI passed"

# Approve a human gate
bd gate approve <gate-id>

# Wake agents waiting on a gate
gt gate wake <gate-id>
```

## Command Reference

| Command | Description |
|---------|-------------|
| `bd gate create` | Create a new gate |
| `bd gate show <id>` | Show gate details |
| `bd gate list` | List all open gates |
| `bd gate close <id>` | Close a gate (condition met) |
| `bd gate approve <id>` | Approve a human gate |
| `gt gate wake <id>` | Wake agents waiting on a gate |

## Use Cases

### Waiting for CI

```mermaid
graph LR
    A[Push code] --> B[Create gh:run gate]
    B --> C[Park workflow]
    C --> D[Deacon polls GitHub]
    D --> E{CI done?}
    E -->|No| D
    E -->|Yes| F[Close gate]
    F --> G[Resume workflow]
```

### Human Approval Workflow

```mermaid
graph LR
    A[Work complete] --> B[Create human gate]
    B --> C[Notify overseer]
    C --> D[Wait for approval]
    D --> E[Human runs bd gate approve]
    E --> F[Gate closes]
    F --> G[Deploy proceeds]
```

### Timer-Based Retry

```mermaid
graph LR
    A[Operation fails] --> B[Create timer gate: 15m]
    B --> C[Park workflow]
    C --> D[15 minutes pass]
    D --> E[Deacon closes gate]
    E --> F[Retry operation]
```

:::tip[When to Use Gates]

Use gates whenever your workflow needs to wait for something external. Prefer gates over busy-waiting (polling in a loop), which wastes context and compute. Gates let the agent exit cleanly and resume only when the condition is met.

:::

## See Also

- **[Molecules & Formulas](molecules.md)** -- Gates are embedded within molecule steps, pausing the workflow at the gated step
- **[Hooks](hooks.md)** -- When an agent parks on a gate and exits, the hook preserves the molecule state so a fresh session can check the gate and resume
- **[Beads](beads.md)** -- Gates are themselves beads with their own status lifecycle (open, closed, expired)
- **[GUPP & NDI](gupp.md)** -- Gates respect GUPP: they pause forward progress but never move the system backward; when the gate opens, the workflow resumes from where it left off
- **[Rigs](rigs.md)** -- The Deacon evaluates gates across all active rigs during its patrol cycle


========================================================================
# Rigs (Project Containers)
# URL: /docs/concepts/rigs
========================================================================

# Rigs (Project Containers)

A **Rig** is Gas Town's project container. Each rig wraps a git repository with the full agent infrastructure needed to manage, develop, and merge code. Rigs are the physical structure of a Gas Town workspace -- every project you manage with Gas Town becomes a rig.

---

## What is a Rig?

The name comes from the Mad Max universe -- an oil rig is a self-contained mobile unit. In Gas Town, a rig is a self-contained project unit with its own:

- Git repository clone(s)
- Issue tracking database
- Merge queue (Refinery)
- Health monitor (Witness)
- Worker sandboxes (Polecats)
- Human developer workspaces (Crew)
- Configuration

```mermaid
graph TD
    subgraph "Town (~/gt/)"
        Mayor
        Deacon
        R1["Rig: myproject"]
        R2["Rig: docs"]
        R3["Rig: api-server"]
    end

    Mayor -->|coordinates| R1
    Mayor -->|coordinates| R2
    Mayor -->|coordinates| R3
    Deacon -->|monitors| R1
    Deacon -->|monitors| R2
    Deacon -->|monitors| R3
```

## Rig Directory Structure

When you add a rig with `gt rig add`, the following directory structure is created:

```
~/gt/myproject/
├── .beads/              # Rig-level issue tracking (SQLite)
│   ├── beads.db         # Issue database
│   └── formulas/        # TOML workflow templates
├── config.json          # Rig configuration
├── refinery/
│   └── rig/             # Canonical main clone (merge queue)
├── mayor/
│   └── rig/             # Mayor's working copy
├── crew/                # Human developer workspaces
│   ├── dave/            # Dave's persistent clone
│   └── emma/            # Emma's persistent clone
├── witness/             # Witness agent state
├── polecats/            # Ephemeral worker directories
│   ├── toast/           # Polecat "toast" worktree
│   └── alpha/           # Polecat "alpha" worktree
└── plugins/             # Rig-level plugins
```

### Key Directories

| Directory | Purpose | Lifecycle |
|-----------|---------|-----------|
| `.beads/` | Issue tracking database and formulas | Persistent, synced via git |
| `refinery/rig/` | Canonical clone used for merge operations | Persistent, always on main |
| `mayor/rig/` | Mayor's read-only working copy for analysis | Persistent |
| `crew/` | Human developer persistent workspaces | Persistent per-developer |
| `witness/` | Witness agent monitoring state | Persistent |
| `polecats/` | Ephemeral worker sandboxes (git worktrees) | Created and destroyed per-task |
| `plugins/` | Rig-specific plugin configurations | Persistent |

## Per-Rig Agents

Each rig has two dedicated persistent agents:

### Witness

The Witness is the **pit boss** for the rig. It monitors polecat health, detects stalled or zombie workers, and handles cleanup:

- Runs the `mol-witness-patrol` molecule in a loop
- Checks polecat status via agent beads
- Nudges stuck polecats, escalates unresponsive ones
- Nukes completed polecat sandboxes
- Reports health to the Deacon via WITNESS_PING

### Refinery

The Refinery is the **merge queue processor** for the rig. It takes completed polecat branches and merges them to main:

- Runs the `mol-refinery-patrol` molecule in a loop
- Picks up MERGE_READY notifications from Witnesses
- Rebases polecat branches on latest main
- Runs tests to validate the merge
- Fast-forward merges clean branches
- Creates conflict-resolution tasks when rebasing fails

```mermaid
graph LR
    subgraph "Rig: myproject"
        W["Witness<br/>(monitors workers)"]
        R["Refinery<br/>(merges code)"]
        P1["Polecat: toast"]
        P2["Polecat: alpha"]

        W -->|monitors| P1
        W -->|monitors| P2
        P1 -->|submits MR| R
        P2 -->|submits MR| R
        R -->|merges to| Main[main branch]
    end
```

## Rig Configuration

Each rig has a `config.json` file that defines its settings:

```json
{
  "type": "rig",
  "version": 1,
  "name": "myproject",
  "git_url": "git@github.com:you/repo.git",
  "default_branch": "main",
  "created_at": "2026-01-15T10:30:00Z",
  "beads": {
    "prefix": "gt"
  }
}
```

| Field | Description |
|-------|-------------|
| `name` | Rig identifier (directory name) |
| `git_url` | Remote repository URL |
| `default_branch` | Main branch name (`main` or `master`) |
| `beads.prefix` | Prefix for beads created in this rig |

## Rig States

Rigs have operational states that control agent behavior:

| State | Meaning | Agent Behavior |
|-------|---------|----------------|
| **Active** | Normal operation | All agents running |
| **Parked** | Temporarily paused | Agents idle, no new work |
| **Docked** | Fully shut down | No agents, no monitoring |

```mermaid
stateDiagram-v2
    [*] --> Active: gt rig start
    Active --> Parked: gt rig park
    Parked --> Active: gt rig unpark
    Active --> Docked: gt rig dock
    Docked --> Active: gt rig undock
    Parked --> Docked: gt rig dock
```

:::warning[Docked Rigs]

When a rig is docked, the Deacon skips all health checks for it. No agents are monitored, no polecats are spawned, and no merges are processed. Use docking for rigs that are temporarily inactive (e.g., a project that is on hold).

:::

## Commands

### Adding a Rig

```bash
# Add from a remote repository
gt rig add myproject https://github.com/you/repo.git

# Add with SSH URL
gt rig add myproject git@github.com:you/repo.git
```

### Listing Rigs

```bash
# List all rigs with status
gt rig list
```

Example output:

```
Name          Status    Branch    Polecats    MQ
myproject     Active    main      2/3         1 pending
docs          Active    master    0/0         0 pending
api-server    Docked    main      -           -
```

### Starting and Stopping

```bash
# Start a rig (launch Witness + Refinery)
gt rig start myproject

# Stop a rig (graceful shutdown of agents)
gt rig stop myproject

# Shutdown (immediate stop)
gt rig shutdown myproject
```

### Checking Status

```bash
# Detailed rig status
gt rig status myproject
```

### Lifecycle Management

```bash
# Reboot a rig (stop + start)
gt rig reboot myproject

# Boot a rig (cold start)
gt rig boot myproject

# Reset a rig (clear state and restart)
gt rig reset myproject
```

### Parking and Docking

```bash
# Park a rig (pause without shutting down)
gt rig park myproject

# Unpark (resume)
gt rig unpark myproject

# Dock a rig (fully shut down)
gt rig dock myproject

# Undock (bring back online)
gt rig undock myproject
```

## Command Reference

| Command | Description |
|---------|-------------|
| `gt rig add <name> <url>` | Add a new rig from a git repository |
| `gt rig list` | List all rigs with status |
| `gt rig start <name>` | Start rig agents (Witness + Refinery) |
| `gt rig stop <name>` | Gracefully stop rig agents |
| `gt rig shutdown <name>` | Immediately stop rig agents |
| `gt rig status <name>` | Show detailed rig status |
| `gt rig reset <name>` | Clear state and restart |
| `gt rig boot <name>` | Cold-start a rig |
| `gt rig reboot <name>` | Stop and restart a rig |
| `gt rig park <name>` | Pause rig (keep state, stop new work) |
| `gt rig unpark <name>` | Resume a parked rig |
| `gt rig dock <name>` | Fully shut down a rig |
| `gt rig undock <name>` | Bring a docked rig back online |

## Rig in the Bigger Picture

Rigs are the physical foundation on which all other concepts operate:

- **[Beads](beads.md)** live in each rig's `.beads/` directory
- **[Hooks](hooks.md)** are implemented as git worktrees within the rig
- **Polecats** are spawned as worktrees under `polecats/`
- **The Refinery** processes merges from the rig's `refinery/rig/` clone
- **[Convoys](convoys.md)** can span multiple rigs for cross-project coordination
- **[Molecules](molecules.md)** execute within a rig's context
- **[Gates](gates.md)** are evaluated across all active rigs by the Deacon
- All rig operations follow [GUPP](gupp.md): state always moves forward, crashes are recoverable

:::tip[Adding Your First Rig]

See the [Quick Start](../getting-started/quickstart.md) guide for a step-by-step walkthrough of adding your first project as a rig and starting work.

:::

## See Also

- **[Beads](beads.md)** -- Each rig has its own `.beads/` database with a unique prefix for issue identification
- **[Hooks](hooks.md)** -- Agent hooks are implemented as git worktrees within the rig's directory
- **[Convoys](convoys.md)** -- Convoys can track beads across multiple rigs using cross-prefix tracking
- **[Molecules & Formulas](molecules.md)** -- Formulas are stored in `.beads/formulas/`; molecules execute within the rig's context
- **[Gates](gates.md)** -- The Deacon evaluates gates across all active rigs during patrol cycles
- **[GUPP & NDI](gupp.md)** -- Rig state (Active, Parked, Docked) follows forward-only transitions
- **[The MEOW Stack](meow-stack.md)** -- Rigs are the physical infrastructure in which the entire MEOW stack operates


========================================================================
# The MEOW Stack
# URL: /docs/concepts/meow-stack
========================================================================

# The MEOW Stack

The **MEOW Stack** (Molecules, Epics, Orchestration, Workflows) is Gas Town's layered abstraction model for organizing and executing work. Each layer builds on the one below it, creating a composable system that scales from a single task to an entire project buildout.

---

## The Layers

```mermaid
graph TD
    F["Formulas<br/>(Reusable templates)"] --> P["Protomolecules<br/>(Convoy-level orchestration)"]
    P --> M["Molecules<br/>(Multi-step workflows)"]
    M --> E["Epics / Convoys<br/>(Batched work items)"]
    E --> B["Beads<br/>(Atomic work units)"]
```

### Layer 1: Beads (Atomic Work Units)

**Beads** are the foundation — individual, trackable units of work. Each bead represents a single issue, task, bug fix, or feature request. They are stored in git (via the `bd` CLI) and persist across crashes, restarts, and agent handoffs.

```bash
bd create --title "Add input validation to /api/users" --type task
```

Beads are the atoms of Gas Town. Everything else is built from them.

| Property | Description |
|----------|-------------|
| **ID** | Unique identifier (e.g., `gt-a1b2c`) |
| **Status** | `open`, `in_progress`, `done`, `deferred` |
| **Hook** | Which agent is currently working on it |
| **Convoy** | Which batch it belongs to |

See [Beads](beads.md) for the full reference.

### Layer 2: Epics / Convoys (Batched Work)

**Convoys** group related beads into batches that travel together. When you tell the Mayor "build the auth system," it creates a convoy containing all the individual beads needed:

```
Convoy: auth-system-v2
├── gt-a1b2c  Add login endpoint
├── gt-d3e4f  Add JWT middleware
├── gt-g5h6i  Add password reset flow
├── gt-j7k8l  Write auth integration tests
└── gt-m9n0o  Update API documentation
```

Convoys provide batch-level tracking: how many beads are done, how many are in progress, whether the overall effort is on track.

See [Convoys](convoys.md) for details.

### Layer 3: Molecules (Multi-Step Workflows)

**Molecules** are execution plans for individual beads. When a polecat picks up a bead, it follows a molecule — a sequence of ordered steps with dependencies, gates, and checkpoints.

```
Molecule: mol-polecat-work
├── load-context      [done]
├── branch-setup      [done]
├── preflight-tests   [done]
├── implement         [in_progress]  ← agent is here
├── self-review       [pending]
├── run-tests         [pending]
└── submit-and-exit   [pending]
```

The molecule tracks exactly where an agent is in its workflow. If the agent crashes, a fresh agent reads the molecule and resumes from the last completed step.

See [Molecules & Formulas](molecules.md) for the full reference.

### Layer 4: Protomolecules (Convoy-Level Orchestration)

**Protomolecules** are higher-order orchestration patterns that coordinate multiple molecules working in parallel. They represent convoy-level workflows where multiple agents work simultaneously on related tasks with coordination points.

Examples:
- **Parallel code review**: Multiple agents review different dimensions (correctness, security, performance), then a synthesis step combines findings
- **Multi-rig deployment**: Changes are pushed to staging across multiple rigs, then promoted to production
- **Design exploration**: Multiple agents explore different design approaches, then the Mayor evaluates results

```mermaid
graph TD
    PM["Protomolecule: Code Review"] --> M1["Mol: Correctness Review"]
    PM --> M2["Mol: Security Review"]
    PM --> M3["Mol: Performance Review"]
    M1 --> S["Synthesis: Combined Review"]
    M2 --> S
    M3 --> S
```

### Layer 5: Formulas (Reusable Templates)

**Formulas** are the TOML-defined templates from which molecules and protomolecules are created. They are the blueprints — reusable, parameterized, and version-controlled.

```toml
formula = "shiny"
type = "workflow"
version = 1

[[steps]]
id = "design"
title = "Design {{feature}}"

[[steps]]
id = "implement"
needs = ["design"]
title = "Implement {{feature}}"
```

A formula is **poured** into a molecule — creating a live instance with real bead IDs and runtime state. You can pour the same formula many times, creating independent workflow instances.

Gas Town ships with 30+ built-in formulas. See [Molecules & Formulas](molecules.md) for the full catalog.

---

## Why "MEOW"?

The name is a backronym: **M**olecules, **E**pics, **O**rchestration, **W**orkflows. But more importantly, it reflects Gas Town's philosophy that work organization should be:

- **Composable**: Each layer builds naturally on the one below
- **Observable**: You can inspect state at any layer
- **Recoverable**: Crashes at any layer are handled gracefully (see [GUPP & NDI](gupp.md))
- **Scalable**: Works for 1 agent or 30

---

## MEOW in Practice

Here's how a typical Gas Town work session flows through the stack:

1. **Human** tells Mayor: "Build user notifications"
2. **Mayor** creates a **Convoy** (Layer 2) with 5 **Beads** (Layer 1)
3. Mayor **slings** each bead to a polecat
4. Each polecat **pours** the `mol-polecat-work` **Formula** (Layer 5) into a **Molecule** (Layer 3)
5. Polecats execute their molecules in parallel, coordinated by the **Protomolecule** (Layer 4) convoy pattern
6. The **Refinery** merges completed work to main
7. The **Convoy** tracks overall progress until all beads are done

```bash
# See the full stack in action
gt convoy list          # Layer 2: batch tracking
gt mol status           # Layer 3: workflow progress
gt formula list         # Layer 5: available templates
bd list --convoy cv-01  # Layer 1: individual beads
```

## Cross-Cutting Concerns

Two concepts cut across all MEOW layers rather than living in a single layer:

- **[Hooks](hooks.md)** -- Hooks persist work state at every layer. A hooked molecule preserves convoy membership, step progress, and bead assignments across crashes.
- **[Gates](gates.md)** -- Gates can pause execution at any layer. A molecule step can be gated, a convoy can be blocked on gated beads, and formulas can define gate conditions in their step definitions.
- **[GUPP & NDI](gupp.md)** -- The forward-only progress guarantee applies at every layer: beads move forward, molecules checkpoint steps, convoys auto-close when complete, and formulas produce idempotent molecules.

## See Also

- **[Beads](beads.md)** -- Layer 1: the atomic work units that form the foundation
- **[Convoys](convoys.md)** -- Layer 2: batch tracking of related beads
- **[Molecules & Formulas](molecules.md)** -- Layers 3 and 5: workflow execution and reusable templates
- **[Hooks](hooks.md)** -- Cross-cutting: persistence mechanism for all layers
- **[Gates](gates.md)** -- Cross-cutting: async coordination within molecules
- **[GUPP & NDI](gupp.md)** -- Cross-cutting: design principles ensuring recoverability at every layer
- **[Rigs](rigs.md)** -- The physical infrastructure in which the entire MEOW stack operates


========================================================================
# GUPP & NDI
# URL: /docs/concepts/gupp
========================================================================

# GUPP & NDI

Two foundational principles govern how Gas Town handles the inherent chaos of multi-agent AI coordination: the **Gas Town Universal Propulsion Principle (GUPP)** and **Nondeterministic Idempotence (NDI)**.

---

## The Gas Town Universal Propulsion Principle (GUPP)

> **Every operation in Gas Town must move the system forward or leave it unchanged. No operation should move the system backward.**

GUPP is the single most important design principle in Gas Town. It means:

- **Crashes cannot lose work.** If an agent crashes mid-task, all completed steps are persisted in the [molecule](molecules.md). A fresh agent resumes from the last checkpoint via the [hook](hooks.md).
- **Retries are safe.** Running an operation twice produces the same result as running it once (or advances further).
- **Partial completion is valid.** A half-finished [convoy](convoys.md) is better than no progress at all.
- **State always moves forward.** [Bead](beads.md) statuses go `open → in_progress → done`. They never go backward.

### GUPP in Practice

```
Before crash:     load-context [done] → branch-setup [done] → implement [in_progress]
After restart:    load-context [done] → branch-setup [done] → implement [in_progress]
                                                                ↑ resumes here
```

The [molecule](molecules.md) tracks step completion in the [beads](beads.md) database. When a fresh polecat picks up the work via its [hook](hooks.md), it reads the molecule state and continues from exactly where the crashed agent left off.

### Why GUPP Matters

Without GUPP, multi-agent systems are fragile. A crash at the wrong moment could:
- Leave the git worktree in an inconsistent state
- Lose track of what was already done
- Cause the next agent to redo expensive work
- Create merge conflicts from duplicate effort

GUPP eliminates these failure modes by design.

---

## Nondeterministic Idempotence (NDI)

> **Operations may produce different outputs each time, but the system state after execution is equivalent.**

NDI is the practical companion to GUPP. AI agents are inherently nondeterministic — ask Claude to implement the same feature twice and you will get different code. NDI acknowledges this reality and works with it rather than against it.

### What NDI Means

- **Same intent, different implementation.** Two agents implementing "add input validation" will write different code. Both are valid if they pass tests.
- **Retry safety with variation.** If a test step fails and the agent retries, it may fix the problem differently. The molecule only cares that the step reaches `done`.
- **State equivalence, not bit equality.** After running `implement`, the worktree contains a valid implementation. It does not matter that a retry produced different variable names.

### NDI vs. Traditional Idempotence

| Traditional Idempotence | Nondeterministic Idempotence |
|------------------------|------------------------------|
| `f(x) = f(f(x))` always | `f(x)` and `g(x)` both reach valid end state |
| Same output every time | Different output, equivalent result |
| Deterministic operations | Acknowledges AI nondeterminism |
| Exact replay | Forward-only progress |

### NDI in Practice

Consider a polecat working on bead `gt-a1b2c` (add input validation):

```
Run 1:  Implements validation with Joi library
        Tests pass → step marked done

Run 2:  (after crash and restart)
        Step already marked done → skipped
        Agent continues to next step
```

If the agent had crashed *during* implementation (before marking done):

```
Run 1:  Starts implementing with Joi, crashes at 60%
        Step still marked in_progress

Run 2:  Fresh agent reads step, sees in_progress
        Implements with Zod instead (different but valid)
        Tests pass → step marked done
```

The second run produced different code, but the system state (valid implementation, passing tests) is equivalent. This is NDI.

---

## The Nudge Workaround

Sometimes an agent gets stuck — looping on a failed test, unable to resolve a merge conflict, or confused by ambiguous requirements. Gas Town uses **nudges** as the escape hatch.

### What is a Nudge?

A nudge is a synchronous message from a human or higher-level agent that interrupts the current agent's execution with new context or instructions.

```bash
# Human nudges a stuck polecat
gt nudge polecat-3 "The test failure is a known flaky test. Skip it with --skip-flaky flag."

# Mayor nudges a witness that's reporting false positives
gt nudge witness-myapp "The API is intentionally returning 503 during migration. Ignore health check failures for 30 minutes."
```

### When to Use Nudges

Nudges are appropriate when:

- An agent is stuck in a retry loop
- New information changes the requirements
- A human wants to steer an agent's approach
- An upstream dependency was resolved and the agent should retry

### Nudges and GUPP

Nudges respect GUPP — they provide new information that enables forward progress, but never instruct an agent to undo completed work. A nudge says "try this approach instead," not "go back and redo what you did."

### Nudges and NDI

Nudges are a natural fit for NDI. After a nudge, the agent may take a completely different approach to the current step. The output will differ from what it would have produced without the nudge, but the end state (step completed, tests passing) is equivalent.

---

## GUPP + NDI Together

The two principles work in concert:

| Scenario | GUPP | NDI |
|----------|------|-----|
| Agent crashes mid-step | Completed steps preserved, resume from checkpoint | Fresh agent may implement differently |
| Test failure | Step stays `in_progress`, agent retries | Retry may use different fix strategy |
| Merge conflict | Conflict resolution step activates | Agent resolves differently than predecessor would have |
| Human nudge | System moves forward with new context | New approach produces different but equivalent output |
| Full restart | All molecule state preserved in beads | New session continues with its own implementation style |

Together, GUPP and NDI make Gas Town **resilient to the fundamental unpredictability of AI agents** while ensuring that work always progresses toward completion.

## How GUPP & NDI Are Implemented

These principles are not just abstract ideals -- they are enforced by specific Gas Town primitives:

| Primitive | GUPP Role | NDI Role |
|-----------|-----------|----------|
| [Hooks](hooks.md) | Persist work assignment across crashes | Fresh agent picks up where predecessor left off |
| [Molecules](molecules.md) | Track step completion as permanent state | Steps marked `done` are never re-executed, regardless of implementation differences |
| [Beads](beads.md) | Forward-only status progression | Different agents may close the same bead via different approaches |
| [Gates](gates.md) | Pause without losing progress | Gate resolution may trigger different downstream execution |
| [Rigs](rigs.md) | Git worktrees provide durable filesystem state | Each polecat worktree is independent |

## See Also

- **[Hooks](hooks.md)** -- The primary mechanism that makes GUPP possible: hooks persist across all session boundaries
- **[Molecules & Formulas](molecules.md)** -- Molecules implement GUPP through step-level checkpointing in the beads database
- **[Beads](beads.md)** -- Bead status progression is a direct expression of GUPP's forward-only rule
- **[Gates](gates.md)** -- Gates enable GUPP-compliant async waits: the system pauses without losing state
- **[The MEOW Stack](meow-stack.md)** -- GUPP and NDI apply at every layer of the MEOW abstraction model


========================================================================
# Wisps (Ephemeral Sub-Beads)
# URL: /docs/concepts/wisps
========================================================================

# Wisps (Ephemeral Sub-Beads)

**Wisps** are lightweight tracking units that represent individual steps within a [molecule](molecules.md). Each step in a molecule's workflow is tracked as a wisp -- an ephemeral sub-bead that records progress, state transitions, and completion without persisting to the main issue database.

---

## Wisps vs Beads

| Property | Bead | Wisp |
|----------|------|------|
| **Persistence** | Permanent (exported to JSONL) | Ephemeral by default |
| **Scope** | Standalone issue or tracking unit | Always belongs to a molecule |
| **Creation** | Manual (`bd create`) or automated | Automatic (when molecule is poured) |
| **Lifecycle** | Survives indefinitely | Cleaned up with parent molecule |
| **Type field** | `task`, `bug`, `feature`, etc. | `wisp` |

Wisps exist because molecule steps need tracking granularity without the overhead of full beads. A molecule with 9 steps creates 9 wisps -- one per step. These wisps track whether each step is pending, in progress, done, or failed.

---

## How Wisps Are Created

Wisps are created automatically when a formula is **poured** into a molecule. Each step defined in the formula's TOML becomes a wisp:

```toml
# Example formula steps (each becomes a wisp)
[[steps]]
name = "load-context"
description = "Load bead details and repository context"

[[steps]]
name = "implement"
description = "Write the code changes"
needs = ["load-context"]

[[steps]]
name = "run-tests"
description = "Execute test suite"
needs = ["implement"]
```

When this formula is poured, three wisps are created -- one for `load-context`, one for `implement`, and one for `run-tests`.

---

## Wisp States

Wisps follow the same state model as molecule steps:

| Symbol | State | Meaning |
|--------|-------|---------|
| `○` | pending | Not yet started, waiting for dependencies |
| `●` | in_progress | Currently being worked on |
| `✓` | done | Completed successfully |
| `✗` | failed | Failed (may need retry) |
| `⏸` | gated | Waiting on an async [gate](gates.md) |

---

## Wisp Lifecycle

```mermaid
graph LR
    P["Pour formula"] --> W1["Wisp: pending ○"]
    W1 --> W2["Wisp: in_progress ●"]
    W2 --> W3["Wisp: done ✓"]
    W2 --> W4["Wisp: failed ✗"]
    W4 --> W2
    W3 --> C["Molecule completes"]
    C --> D["Wisps cleaned up"]
```

1. **Creation**: Formula is poured, wisps are scaffolded for each step
2. **Execution**: Agent works through steps, advancing wisp states
3. **Completion**: When all wisps reach `done`, the molecule completes
4. **Cleanup**: Wisps are removed with their parent molecule

---

## Cleanup Wisps

A special use of wisps occurs in the [Witness](../agents/witness.md) workflow. When a polecat exits with a dirty state (uncommitted changes, unpushed commits), the Witness creates a **cleanup wisp** to track recovery:

- **Clean exit** (branch pushed, MR submitted, git clean): No wisp needed -- auto-nuke immediately
- **Dirty exit** (uncommitted changes, unpushed commits): Cleanup wisp created to track recovery, then nuke

This means the common case (clean `gt done`) is fast and automatic. Cleanup wisps are the exception, not the rule.

---

## Viewing Wisps

```bash
# See molecule status including all wisps
gt mol status

# Show detailed step/wisp states
gt mol current
```

---

## Ephemeral by Design

Wisps are deliberately ephemeral. They are **not exported** to JSONL during `bd sync` and do not appear in `bd list` output. This keeps the beads database focused on meaningful work items while allowing molecules to track arbitrarily fine-grained progress internally.

If you need persistent tracking of sub-tasks, use regular beads with dependency links instead of wisps.

---

## See Also

- **[Molecules & Formulas](molecules.md)** -- Wisps are the per-step tracking units within molecules
- **[Beads](beads.md)** -- Wisps are a specialized bead type (`type: wisp`) with ephemeral lifecycle
- **[Gates](gates.md)** -- A wisp can be gated, pausing its molecule step until an external condition resolves
- **[Hooks](hooks.md)** -- The parent molecule (containing wisps) is attached to an agent's hook


========================================================================
# Session Cycling
# URL: /docs/concepts/session-cycling
========================================================================

# Session Cycling

**Session cycling** is Gas Town's mechanism for refreshing an agent's context window without losing work. When a session fills its context or finishes a logical chunk, it creates a handoff and a fresh session picks up where it left off.

---

## Why Sessions Need Cycling

AI coding agents have finite context windows. As an agent works -- reading files, writing code, communicating -- its context fills up. At capacity, the agent becomes sluggish, repetitive, or loses track of its goals.

Gas Town solves this with session cycling:

| Problem | Solution |
|---------|----------|
| Context window fills up | Hand off to a fresh session |
| Agent loses track of work | Hook persists across sessions |
| Notes and progress lost | Handoff mail carries context |
| Restart means starting over | `gt prime` reloads full role context |

---

## The Two Persistence Mechanisms

Session cycling relies on two independent mechanisms:

### 1. The Hook (What You're Working On)

The [hook](hooks.md) persists the agent's current work assignment. Whether it's a [molecule](molecules.md), a bead, or hooked mail, the hook survives session restarts. When the new session starts and runs `gt prime`, it finds the work still on its hook and continues.

### 2. Handoff Mail (Context Notes)

The handoff mail is optional context that captures nuances the hook doesn't:

```bash
gt handoff -s "Working on auth bug" -m "
Found the issue is in token refresh logic.
Check line 145 in auth.go first.
The failing test is TestRefreshExpired.
"
```

This creates a mail bead addressed to yourself. Your next session reads it for orientation.

---

## How It Works

```mermaid
sequenceDiagram
    participant S1 as Session 1
    participant Hook as Hook (persistent)
    participant Mail as Handoff Mail
    participant S2 as Session 2

    S1->>S1: Context filling up
    S1->>Mail: gt handoff -m "progress notes"
    S1->>S1: Session exits

    Note over Hook: Work assignment persists

    S2->>S2: New session starts
    S2->>S2: gt prime (load role context)
    S2->>Hook: gt hook (find assigned work)
    S2->>Mail: gt mail inbox (read handoff notes)
    S2->>S2: Resume work immediately (GUPP)
```

---

## When to Cycle

### Polecats

Polecats cycle when context pressure builds. The [Witness](../agents/witness.md) monitors for signs of context exhaustion (sluggishness, repetition) and may nudge a polecat to cycle.

```bash
# Polecat self-cycles
gt handoff -s "Context filling" -m "Implemented X, Y remains. Next: finish Y and run tests."
```

### Persistent Agents (Mayor, Witness, Deacon)

Persistent agents experience automatic compaction but may lose nuance. If an agent seems confused after compaction, `gt prime` reloads the full role context.

```bash
# From outside: nudge agent to recover context
gt nudge witness "Run gt prime to reset context"

# Or restart with fresh context
gt witness restart --fresh
```

### Crew Workers

Crew cycling is relaxed. You cycle when it feels right:
- Context getting full
- Finished a logical chunk of work
- Need a fresh perspective
- Human asks you to

```bash
# Crew handoff with context notes
gt handoff -s "Finished sidebar fixes" -m "
Closed beads ga-abc, ga-def.
Remaining work: ga-ghi needs review.
Build is clean, all pushed to main.
"
```

---

## Context Recovery

When a new session starts (or after compaction), run:

```bash
gt prime
```

This reloads:
- Full role context from CLAUDE.md
- Hook state (current work assignment)
- Beads database access
- Agent identity and permissions

Then check for handoff notes:

```bash
gt mail inbox
```

---

## Handoff vs Park

| Action | `gt handoff` | `gt park` |
|--------|-------------|-----------|
| **Intent** | Continue immediately in new session | Pause, resume later |
| **Successor** | Expected immediately | No immediate successor |
| **Use case** | Context full, keep working | End of day, break |
| **State** | Hook + mail persist | Hook persists |

---

## Best Practices

1. **Always include handoff notes.** The next session (even if it's "you") has no memory of the previous one. Good notes save significant ramp-up time.

2. **Commit and push before cycling.** Unpushed work in a multi-agent environment is work at risk. Always land your changes before handing off.

3. **Keep notes actionable.** Focus on what to do next, not a history of what was tried:
   - Good: "Next: fix the auth test in line 145, then run `go test ./auth/...`"
   - Bad: "I spent a while looking at various things and found some issues"

4. **Cycle proactively.** Don't wait until the context window is completely full. If you notice yourself re-reading the same files or losing track, it's time.

---

## See Also

- **[GUPP](gupp.md)** -- The propulsion principle ensures the new session immediately executes hooked work
- **[Hooks](hooks.md)** -- The persistence mechanism that survives session cycling
- **[Molecules](molecules.md)** -- Multi-step workflows track progress across session boundaries
- **[Patrol Cycles](patrol-cycles.md)** -- Persistent agents use patrol molecules that squash-and-respawn across session boundaries
- **[Handoff Ceremony](../workflows/handoff-ceremony.md)** -- Step-by-step guide for clean session transitions
- **[Session Commands](../cli-reference/sessions.md)** -- CLI commands for session management and handoff


========================================================================
# Patrol Cycles
# URL: /docs/concepts/patrol-cycles
========================================================================

# Patrol Cycles

**Patrol cycles** are the heartbeat of Gas Town's monitoring infrastructure. Every 5 minutes, persistent agents — the [Deacon](../agents/deacon.md), [Witness](../agents/witness.md), and [Refinery](../agents/refinery.md) — wake up, observe the current state of the system, and take corrective action.

---

## Why Patrol Cycles Exist

AI agent systems are inherently chaotic. Sessions crash, context windows fill up, processes become zombies, and work gets stuck. Gas Town addresses this with a simple pattern borrowed from operations engineering: **periodic inspection and remediation**.

Rather than relying on event-driven notifications (which can be lost, duplicated, or arrive out of order), patrol cycles ensure that the system self-heals on a fixed cadence. If something goes wrong between cycles, it will be caught at the next tick.

| Problem | Patrol Response |
|---------|----------------|
| Polecat stalled for 15 min | Witness nudges it |
| Polecat still stalled after nudge | Witness escalates to Deacon |
| Witness itself is unresponsive | Deacon restarts Witness |
| Merge request pending in queue | Refinery processes it |
| Orphaned worktree consuming disk | Deacon cleans it up |
| Zombie process lingering | Deacon terminates it |

---

## The Three Patrol Agents

Each patrol agent has a distinct focus area:

### Witness (Per-Rig)

Each [rig](rigs.md) has one Witness that supervises all polecats in that rig.

**Patrol focus:** Polecat health, stall detection, zombie cleanup, Refinery health.

```mermaid
flowchart TD
    Start["Patrol Tick"]
    List["List All Polecats"]
    Each{"For Each Polecat"}
    Active["Check Activity"]
    Stale{"Stale?"}
    Nudge["Nudge Session"]
    Responded{"Responded?"}
    Escalate["Escalate to Deacon"]
    Zombie{"Zombie?"}
    Clean["Clean Up"]
    Next["Next Polecat"]
    Done["Patrol Complete"]

    Start --> List --> Each --> Active --> Stale
    Stale -->|Yes| Nudge --> Responded
    Responded -->|No| Escalate --> Next
    Responded -->|Yes| Next
    Stale -->|No| Zombie
    Zombie -->|Yes| Clean --> Next
    Zombie -->|No| Next
    Next --> Each
    Each -->|All checked| Done
```

### Deacon (Town-Wide)

The [Deacon](../agents/deacon.md) is the town-level health coordinator.

**Patrol focus:** Witness health, lifecycle requests, stale hooks, orphaned resources, zombie processes.

```mermaid
flowchart TD
    Start["Patrol Tick"]
    Check["Check All Witnesses"]
    Stale{"Any Stale?"}
    Nudge["Nudge Stale Witness"]
    Response{"Responded?"}
    Restart["Restart Witness"]
    Hooks["Check Stale Hooks"]
    Orphans["Cleanup Orphans"]
    Zombies["Scan for Zombies"]
    Done["Patrol Complete"]

    Start --> Check --> Stale
    Stale -->|Yes| Nudge --> Response
    Response -->|No| Restart --> Hooks
    Response -->|Yes| Hooks
    Stale -->|No| Hooks
    Hooks --> Orphans --> Zombies --> Done
```

### Refinery (Per-Rig)

The [Refinery](../agents/refinery.md) processes the merge queue for its rig.

**Patrol focus:** Merge requests, rebase validation, merge to main.

```
Patrol Tick → Check Queue → Process Next MR → Rebase → Run Tests → Merge → Repeat
```

---

## Timing and Intervals

| Agent | Interval | Trigger |
|-------|----------|---------|
| **Deacon** | 5 min | Daemon heartbeat (every 3 min) |
| **Witness** | 5 min | Patrol molecule step |
| **Refinery** | 5 min | Patrol molecule step |
| **Daemon** | 3 min | Go process timer |

The Daemon (a Go process) sends heartbeats to the Deacon on a 3-minute interval. These heartbeats trigger the Deacon's patrol cycle and carry system state information such as rig list and session inventory.

:::tip[Not Perfectly Synchronized]

The 5-minute intervals are approximate. Each agent runs its own patrol molecule independently. There is no global clock — this is by design. Agents that depend on synchronized timing are fragile; agents that discover state on their own schedule are resilient.

:::

---

## Discovery Over Tracking

Patrol cycles embody Gas Town's core design principle: **discovery over tracking**.

Traditional systems maintain state tables — "polecat-3 started at 14:02, should finish by 14:30." When something goes wrong (process crash, state corruption, race condition), the tracking data becomes stale and misleading.

Gas Town's patrol agents take the opposite approach:

> **Observe reality each cycle. Don't trust yesterday's notes.**

Each patrol cycle, agents:
1. **Scan** the actual system state (processes, sessions, worktrees, hooks)
2. **Compare** against expected health criteria
3. **Act** on any discrepancies found

This means:
- **State corruption is self-healing.** Even if tracking data is lost, the next patrol cycle rediscovers the truth.
- **No single point of failure.** Patrol doesn't depend on event delivery or message queues.
- **Late detection is bounded.** The worst case is one interval (5 minutes) of latency before a problem is noticed.

### Example: Discovery vs Tracking

**Tracking approach (fragile):**
```
Polecat spawned → Tracker records "polecat-3: active"
Polecat crashes → Crash handler fails to fire
Tracker still shows "polecat-3: active"
Work sits stuck indefinitely
```

**Discovery approach (resilient):**
```
Patrol tick → Witness lists all polecats
Witness checks polecat-3's session → not responding
Witness nudges polecat-3 → no response
Witness escalates to Deacon → Deacon cleans up
Work recovers within one patrol cycle
```

---

## Patrol Molecules

Patrol cycles are implemented as [molecules](molecules.md) — the same workflow mechanism used for regular work. The key difference is the **squash-and-respawn** pattern:

1. **Execute** a patrol cycle (check health, take actions)
2. **Squash** the completed molecule into a single digest bead: `gt mol squash`
3. **Respawn** a fresh patrol molecule for the next cycle

This prevents step [wisps](wisps.md) from accumulating over time. Without squashing, a Witness running 288 patrol cycles per day would generate thousands of ephemeral beads.

```bash
# Squash after a patrol cycle
gt mol squash <mol-id> --summary "Patrol cycle: 3 polecats healthy, 1 nudged, 0 escalations"

# Aggregate daily patrol digests
gt patrol digest
gt patrol digest --yesterday
```

### Standard Patrol Formulas

| Formula | Agent | Purpose |
|---------|-------|---------|
| `mol-witness-patrol` | Witness | Health monitoring loop |
| `mol-refinery-patrol` | Refinery | Merge queue processing loop |
| `mol-deacon-patrol` | Deacon | Background coordination loop |

---

## Observing Patrol Activity

### Patrol Digests

Use `gt patrol digest` to see aggregated patrol findings:

```bash
gt patrol digest                          # Today's digest
gt patrol digest --agent witness --rig myapp  # Specific agent/rig
gt patrol digest --yesterday              # Yesterday's digest
```

### Logs

Use `gt log` to see real-time patrol activity:

```bash
gt log --agent witness    # Watch witness patrol in real time
gt log --agent deacon     # Watch deacon patrol
```

### Status

Use `gt status` for a snapshot of current patrol state:

```bash
gt status           # Overview of all agents and rigs
gt witness status   # Witness health and last patrol time
gt deacon status    # Deacon health and last patrol time
```

---

## Agents Without Patrol Cycles

Not all agents run patrol cycles:

| Agent | Patrol? | Why |
|-------|:-------:|-----|
| **Mayor** | On-demand | Responds to requests, doesn't monitor |
| **Polecats** | No | Single-task workers, monitored by Witness |
| **Crew** | No | Human-managed, no automated monitoring |
| **Dogs** | No | Task workers managed by Deacon |
| **Boot** | No | Short-lived triage, spawned by Deacon |

---

## Tuning Patrol Behavior

Patrol intervals and thresholds can be adjusted for cost management:

| Setting | Default | Low-Cost Mode |
|---------|---------|---------------|
| Patrol interval | 5 min | 15 min |
| Stall threshold | 15 min | 30 min |
| Zombie scan | Every cycle | Every 3rd cycle |

See [Cost Management](../guides/cost-management.md) for details on reducing patrol overhead.

:::warning

Longer patrol intervals mean slower detection of problems. A 15-minute interval means a crashed polecat could sit idle for up to 15 minutes before being noticed. Balance cost savings against acceptable detection latency.

:::

---

## See Also

- **[GUPP & NDI](gupp.md)** -- Patrol cycles embody GUPP: each cycle moves the system forward or leaves it unchanged
- **[Molecules & Formulas](molecules.md)** -- Patrol cycles are implemented as patrol molecules with squash-and-respawn
- **[Wisps](wisps.md)** -- Patrol steps create wisps that are squashed after each cycle
- **[Session Cycling](session-cycling.md)** -- Persistent agents cycle their own sessions when context fills
- **[Hooks](hooks.md)** -- Patrol agents discover stale hooks during their cycles
- **[Design Principles](../architecture/design-principles.md)** -- "Discovery over Tracking" is the principle behind patrol cycles


========================================================================
# Workflows
# URL: /docs/workflows
========================================================================

# Workflows

Gas Town supports several workflow patterns for getting work done. Choose the one that best matches your needs and infrastructure.

---

## Choosing a Workflow

| Workflow | Best For | Requires Tmux | Automation Level |
|----------|----------|:-------------:|:----------------:|
| [Mayor Workflow (MEOW)](mayor-workflow.md) | Complex multi-issue work | Yes | Full |
| [Minimal Mode](minimal-mode.md) | Getting started, limited resources | No | Low |
| [Manual Convoy](manual-convoy.md) | Fine-grained control | Optional | Medium |
| [Formula Workflow](formula-workflow.md) | Repeatable processes | Optional | Medium |
| [Code Review](code-review.md) | Thorough parallel reviews | Yes | Full |
| [Crew Collaboration](crew-collaboration.md) | Humans working alongside polecats | Optional | Medium |
| [Handoff Ceremony](handoff-ceremony.md) | Clean session transitions | No | Low |

## Decision Tree

```mermaid
flowchart TD
    Start{What do you need?} -->|Complex project work| A{Have Tmux?}
    A -->|Yes| MEOW["Mayor Workflow (MEOW)"]
    A -->|No| Minimal[Minimal Mode]

    Start -->|Fine control over assignments| Manual[Manual Convoy]
    Start -->|Repeatable process| Formula[Formula Workflow]
    Start -->|Code review| Review[Code Review]
    Start -->|Work alongside polecats| Crew[Crew Collaboration]

    MEOW --> Done[Work gets done]
    Minimal --> Done
    Manual --> Done
    Formula --> Done
    Review --> Done
    Crew --> Done
```

## Workflow Comparison

### Automation Spectrum

```
  Manual                                              Fully Automated
    |                                                        |
    |   Minimal    Manual Convoy    Formula    MEOW          |
    |------●-----------●--------------●---------●-----------|
```

- **Minimal Mode**: You create beads, sling work, and monitor manually. Gas Town provides persistence and tracking.
- **Manual Convoy**: You create convoys and assign issues yourself. Gas Town handles execution and merge.
- **Formula Workflow**: You select a predefined formula. Gas Town orchestrates the steps.
- **Mayor Workflow**: You describe work in natural language. The Mayor handles everything.

### What Each Workflow Automates

| Task | Minimal | Manual Convoy | Formula | MEOW |
|------|:-------:|:------------:|:-------:|:----:|
| Create beads | Manual | Manual | Auto | Auto |
| Create convoy | Manual | Manual | Manual | Auto |
| Assign to rigs | Manual | Manual | Auto | Auto |
| Spawn polecats | Manual | Auto | Auto | Auto |
| Monitor progress | Manual | Manual | Auto | Auto |
| Handle failures | Manual | Manual | Partial | Auto |
| Merge code | Manual | Auto | Auto | Auto |

## Common Patterns Across Workflows

Regardless of which workflow you choose, several patterns are universal:

### The Hook-Propulsion Loop

Every agent follows the same startup pattern:

```bash
gt prime           # Load context
gt hook            # Check for attached work
gt mail inbox      # Check for messages
```

### The Polecat Lifecycle

Workers always follow the self-cleaning model:

```
Spawn -> Work -> Done -> Nuke
```

### The Merge Pipeline

Completed work always flows through the Refinery:

```
Polecat pushes branch -> Witness sends MERGE_READY ->
Refinery rebases + tests -> Refinery merges to main
```

### Convoy Tracking

All workflows use convoys (explicitly or implicitly) to track batch progress and auto-close when complete.

## Getting Started

If you are new to Gas Town, start with the [Mayor Workflow](mayor-workflow.md) -- it is the recommended approach for most users. If you need to work without Tmux or want more hands-on control, try [Minimal Mode](minimal-mode.md) first.

:::tip[Mix and Match]

These workflows are not mutually exclusive. You can use the Mayor for most work, drop to Manual Convoy for specific tasks, and run Formulas for specialized processes -- all in the same town.


:::


========================================================================
# Mayor Workflow (MEOW)
# URL: /docs/workflows/mayor-workflow
========================================================================

# Mayor Workflow (MEOW)

The **Mayor Workflow** -- also known as **MEOW** (Mayor-Executed Orchestrated Work) -- is the recommended workflow for complex multi-issue coordination. You describe what you want in natural language, and the Mayor handles bead creation, convoy management, agent assignment, progress monitoring, and completion notification.

---

## When to Use This Workflow

- You have multiple related tasks that need coordination
- You want maximum automation with minimum manual intervention
- You have Tmux installed for multi-agent session management
- You are comfortable letting the Mayor make assignment decisions

:::info[Prerequisites]

- Gas Town installed with at least one rig configured
- Tmux 3.0+ installed
- Core agents started (`gt start --all`)

:::

## Overview

```mermaid
sequenceDiagram
    participant Human
    participant Mayor
    participant Deacon
    participant Witness
    participant Polecats
    participant Refinery

    Human->>Mayor: Describe work
    Mayor->>Mayor: Create beads
    Mayor->>Mayor: Create convoy
    Mayor->>Witness: Sling work to rig
    Witness->>Polecats: Spawn workers
    Polecats->>Polecats: Implement features
    Polecats->>Refinery: Submit MRs
    Refinery->>Refinery: Rebase + test + merge
    Refinery->>Witness: MERGED notification
    Witness->>Witness: Nuke polecat sandboxes
    Deacon->>Mayor: Convoy complete
    Mayor->>Human: Work landed
```

## Step-by-Step

### Step 1: Attach to the Mayor

```bash
gt mayor attach
```

This opens an interactive session with the Mayor agent. The Mayor already has context about your town, rigs, and current state.

### Step 2: Describe Your Work

Tell the Mayor what you want in natural language. Be specific about outcomes, not implementation details:

> "Fix the 5 failing tests in the auth module, add input validation to the user registration endpoint, and update the README with the new auth flow."

Or for a simpler request:

> "There's a bug where login fails when the password has special characters. Fix it and add a regression test."

### Step 3: Mayor Creates Beads and Convoy

The Mayor automatically:

1. **Analyzes your request** and breaks it into discrete tasks
2. **Creates beads** for each task with appropriate type, priority, and labels
3. **Bundles beads into a convoy** for batch tracking
4. **Reports the plan** back to you for confirmation

Example Mayor response:

```
I've created the following plan:

Convoy: hq-cv-012 "Auth Module Improvements"

Issues:
  gt-a1b2c  [bug]     Fix failing auth tests (P1)
  gt-d3e4f  [feature] Add registration input validation (P2)
  gt-g5h6i  [task]    Update README with auth flow (P3)

Shall I proceed with assigning these to polecats?
```

### Step 4: Mayor Assigns Work

Once confirmed, the Mayor slings each bead to the appropriate rig:

```bash
# Mayor runs internally:
gt sling gt-a1b2c myproject
gt sling gt-d3e4f myproject
gt sling gt-g5h6i myproject
```

Each `gt sling` spawns a polecat worker that immediately begins work.

### Step 5: Monitor Progress

You can check progress at any time:

```bash
# Check convoy status
gt convoy list
gt convoy show hq-cv-012

# Watch the live activity feed
gt feed

# Check individual polecat status
gt polecat list
```

Or simply ask the Mayor:

> "How's the auth work going?"

### Step 6: Work Completes

As each polecat finishes:

1. Polecat runs `gt done` (submits MR, nukes sandbox)
2. Witness detects completion, sends MERGE_READY to Refinery
3. Refinery rebases, tests, and merges to main
4. Refinery sends MERGED notification to Witness
5. Witness completes cleanup

When all convoy items are done, the convoy auto-closes and the Mayor notifies you.

## Example Session

Here is a complete example session from start to finish:

```
$ gt mayor attach

Mayor> Welcome! I see 2 active rigs: myproject, docs. How can I help?

You> The login endpoint returns 500 when the password contains a
     single quote. Fix the bug and add a test. Also update the
     API docs for the login endpoint.

Mayor> I'll break this into 3 tasks across 2 rigs:

  Convoy: hq-cv-015 "Login Quote Bug Fix"

  myproject:
    gt-x1y2z  [bug]  Fix SQL injection in login password handling (P1)
    gt-a3b4c  [task] Add regression test for special char passwords (P1)

  docs:
    gt-d5e6f  [task] Update login endpoint API documentation (P3)

  Ready to assign. Proceed?

You> Yes

Mayor> Assigning...
  ✓ gt-x1y2z slung to myproject (polecat: toast)
  ✓ gt-a3b4c slung to myproject (polecat: alpha)
  ✓ gt-d5e6f slung to docs (polecat: bravo)

  3 polecats working. I'll monitor progress.

  [... time passes ...]

Mayor> Progress update on hq-cv-015:
  ✓ gt-x1y2z  Fixed SQL injection - merged to main
  ✓ gt-a3b4c  Regression test added - merged to main
  ● gt-d5e6f  Docs update in progress

  [... more time ...]

Mayor> Convoy hq-cv-015 "Login Quote Bug Fix" is complete!
  All 3 issues merged to main. Duration: 12 minutes.
```

## Advanced Usage

### Providing Constraints

You can give the Mayor additional constraints:

> "Fix the auth tests, but only use the auth-service rig, not the monolith."

> "Add the feature, but make sure to create a design doc first."

### Requesting Specific Formulas

You can ask the Mayor to use a particular formula:

> "Run a code review on the last 3 PRs using the full preset."

> "Do a design exploration for adding notification levels."

### Cross-Rig Work

The Mayor handles cross-rig coordination automatically:

> "The API changes need to land in api-server first, then update the client in frontend-app to use the new endpoints."

The Mayor will create the beads with dependencies and ensure the work happens in the correct order.

### Intervening Mid-Flight

You can always interrupt:

```bash
# Stop a specific polecat
gt polecat stop myproject/toast

# Release an issue back to the pool
gt release gt-a1b2c

# Add more work to an existing convoy
gt convoy add hq-cv-015 gt-newbead
```

## Troubleshooting

| Problem | Solution |
|---------|----------|
| Mayor not responding | Check `gt mayor status`. Restart with `gt mayor restart` |
| Polecat stuck | Witness will detect it. Or manually: `gt nudge myproject/polecats/toast "status?"` |
| Merge conflict | Refinery spawns a conflict-resolution polecat automatically |
| Work not starting | Check `gt convoy stranded` for unassigned work |
| Convoy not closing | Run `gt convoy check hq-cv-015` to diagnose |

:::tip[The Mayor Remembers]

The Mayor maintains context across the session. You can refer back to earlier work, ask for status updates, or modify plans. If the Mayor's session restarts (context compaction), it reads its hook to resume.

:::

## See Also

- **[Mayor](../agents/mayor.md)** -- The Mayor agent that drives this workflow
- **[MEOW Stack](../concepts/meow-stack.md)** -- The layered abstraction model
- **[Convoys](../concepts/convoys.md)** -- Batch tracking used by the Mayor


========================================================================
# Minimal Mode (No Tmux)
# URL: /docs/workflows/minimal-mode
========================================================================

# Minimal Mode (No Tmux)

**Minimal Mode** lets you use Gas Town's persistence and tracking features without requiring Tmux or the full agent hierarchy. You manually spawn agents and manage work, while Gas Town provides hooks, beads, and convoy tracking. This is ideal for getting started, running on limited resources, or when you prefer hands-on control.

---

## When to Use This Workflow

- You do not have Tmux installed
- You are running on a machine with limited resources
- You want to learn Gas Town concepts incrementally
- You prefer direct control over each agent

:::info[Prerequisites]

- Gas Town installed (`gt` CLI available)
- Beads installed (`bd` CLI available)
- At least one rig configured
- Claude Code (or compatible runtime) available

:::

## Overview

In Minimal Mode, you replace the automated agent hierarchy with manual commands. The key difference from the full Mayor Workflow is that **you** are the coordinator:

```mermaid
graph TD
    You["You (Human)"] -->|create issues| BD[Beads]
    You -->|create convoy| CV[Convoy]
    You -->|sling work| Hook[Hook]
    You -->|resume agent| Agent[Claude Code]
    Agent -->|reads hook| Hook
    Agent -->|works on| Code[Code Changes]
    Agent -->|completes| Done[gt done]
```

## Step-by-Step

### Step 1: Create Issues

Use the `bd` CLI to create beads for your work:

```bash
bd create --title "Fix login bug" --type bug --priority 1
# Created: gt-a1b2c

bd create --title "Add email validation" --type feature
# Created: gt-d3e4f
```

### Step 2: Create a Convoy

Bundle the issues for tracking:

```bash
gt convoy create "Auth Fixes" gt-a1b2c gt-d3e4f
# Created: hq-cv-001
```

### Step 3: Sling Work to a Hook

Attach the first issue to an agent's hook:

```bash
gt sling gt-a1b2c myproject
```

This does two things:

1. Updates the bead status to `hooked`
2. Attaches the bead ID to the target location's hook

:::note[Without Tmux]

In Minimal Mode, `gt sling` may not auto-spawn a polecat if Tmux is unavailable. You will need to start the agent session manually in the next step.

:::

### Step 4: Start the Agent Manually

Open a terminal in the rig's working directory and start Claude Code:

```bash
cd ~/gt/myproject/polecats/toast  # Or crew/yourname for persistent workspace
claude
```

When Claude starts, it should run `gt prime` (via the SessionStart hook) and discover the work on its hook. If it does not auto-prime, tell it:

```
Run gt prime and check your hook for assigned work.
```

The agent will:

1. Run `gt prime` to load context
2. Run `gt hook` to find the assigned bead
3. Read the bead with `bd show gt-a1b2c`
4. Begin working on the task

### Step 5: Monitor Progress

In a separate terminal:

```bash
# Check convoy progress
gt convoy show hq-cv-001

# Check bead status
bd show gt-a1b2c

# Check what's in the merge queue
gt mq list
```

### Step 6: Handle Completion

When the agent finishes, it runs `gt done` which:

1. Pushes the branch
2. Submits to the merge queue
3. Cleans up the sandbox

If you are not running the Refinery, you may need to merge manually:

```bash
cd ~/gt/myproject/refinery/rig
git fetch origin
git merge --ff-only origin/polecat/toast
git push origin main
```

### Step 7: Move to Next Issue

Repeat steps 3-6 for the next issue:

```bash
gt sling gt-d3e4f myproject
# Start a new agent session manually
```

### Step 8: Check Convoy Completion

```bash
gt convoy show hq-cv-001
# If all issues done, convoy shows COMPLETED
```

## Example Session

```bash
# 1. Create work items
$ bd create --title "Fix auth timeout" --type bug --priority 1
Created: gt-x1y2z

$ bd create --title "Add rate limiting" --type feature
Created: gt-a3b4c

# 2. Bundle into convoy
$ gt convoy create "Security Sprint" gt-x1y2z gt-a3b4c
Created: hq-cv-005

# 3. Sling first item
$ gt sling gt-x1y2z myproject

# 4. Start agent in another terminal
$ cd ~/gt/myproject/crew/dave
$ claude
# Agent reads hook, works on gt-x1y2z, runs gt done

# 5. Check progress
$ gt convoy show hq-cv-005
# gt-x1y2z: done
# gt-a3b4c: pending

# 6. Sling next item
$ gt sling gt-a3b4c myproject

# 7. Start agent again
$ cd ~/gt/myproject/crew/dave
$ claude
# Agent reads hook, works on gt-a3b4c, runs gt done

# 8. Convoy completes
$ gt convoy show hq-cv-005
# COMPLETED: 2/2 done
```

## Scaling Up from Minimal Mode

Once you are comfortable with Minimal Mode, you can incrementally add automation:

### Add Tmux

Install Tmux and let Gas Town manage sessions:

```bash
# Now gt sling will auto-spawn polecats
gt sling gt-a1b2c myproject
# Polecat spawns in a tmux pane automatically
```

### Start the Witness

Let the Witness monitor your polecats:

```bash
gt witness start myproject
```

### Start the Refinery

Let the Refinery handle merges:

```bash
gt refinery start myproject
```

### Start the Full Stack

When ready, start everything:

```bash
gt start --all
gt mayor attach
```

You are now running the full [Mayor Workflow](mayor-workflow.md).

## Tips for Minimal Mode

:::tip[Use Crew Workspaces]

For persistent work, use crew workspaces (`gt crew add myproject yourname`) instead of polecat directories. Crew workspaces persist between sessions while polecat directories are ephemeral.

:::

:::tip[Always Push]

Since you do not have the automated Refinery, always push your changes and sync beads before ending a session:
```bash
git push origin HEAD
bd sync
```

:::

:::warning[No Automatic Recovery]

Without Witness and Deacon monitoring, stuck or crashed sessions will not be automatically detected. Check on your agents periodically.

:::

## Comparison with Full Workflow

| Feature | Minimal Mode | Full Workflow |
|---------|:------------:|:------------:|
| Bead tracking | Yes | Yes |
| Convoy tracking | Yes | Yes |
| Hook persistence | Yes | Yes |
| Auto-spawn polecats | No | Yes |
| Auto-merge (Refinery) | No | Yes |
| Health monitoring (Witness) | No | Yes |
| Coordination (Mayor) | No | Yes |
| Requires Tmux | No | Yes |
| Resource usage | Low | Higher |

## See Also

- **[Beads](../concepts/beads.md)** -- Manual issue tracking in minimal mode
- **[Hooks](../concepts/hooks.md)** -- Work assignment mechanism
- **[Getting Started](../getting-started/quickstart.md)** -- Quick setup for minimal mode


========================================================================
# Manual Convoy Workflow
# URL: /docs/workflows/manual-convoy
========================================================================

# Manual Convoy Workflow

The **Manual Convoy Workflow** gives you direct control over work distribution while still using Gas Town's convoy tracking, merge queue, and agent infrastructure. Unlike the Mayor Workflow (where the Mayor decides assignments), you create the convoy, add issues, and sling them individually to specific rigs or agents.

---

## When to Use This Workflow

- You want precise control over which agent handles which task
- You need to route specific issues to specific rigs
- You want convoy tracking without full Mayor coordination
- You are debugging an issue that requires hands-on assignment

:::info[Prerequisites]

- Gas Town installed with at least one rig
- Tmux recommended (for auto-spawning polecats)
- Witness and Refinery running on target rigs

:::

## Overview

```mermaid
graph TD
    You["You (Human)"] -->|1. Create beads| BD[bd create]
    BD --> B1["gt-a1b2c"]
    BD --> B2["gt-d3e4f"]
    BD --> B3["gt-g5h6i"]

    You -->|2. Create convoy| CV["gt convoy create"]
    CV --> Convoy["hq-cv-001"]

    B1 --> Convoy
    B2 --> Convoy
    B3 --> Convoy

    You -->|3. Sling individually| S1["gt sling gt-a1b2c myproject"]
    You -->|3. Sling individually| S2["gt sling gt-d3e4f myproject"]
    You -->|3. Sling individually| S3["gt sling gt-g5h6i docs"]

    S1 --> P1["Polecat: toast"]
    S2 --> P2["Polecat: alpha"]
    S3 --> P3["Polecat: bravo"]

    You -->|4. Monitor| Monitor["gt convoy show"]
```

## Step-by-Step

### Step 1: Create Issues

Create beads with clear titles and metadata:

```bash
# Bug fix with high priority
bd create --title "Fix SQL injection in login" \
  --type bug \
  --priority 1 \
  --labels "security,auth" \
  --description "Login endpoint is vulnerable to SQL injection via the password field. Use parameterized queries."
# Created: gt-a1b2c

# Feature with medium priority
bd create --title "Add rate limiting to API" \
  --type feature \
  --priority 2 \
  --labels "api,security"
# Created: gt-d3e4f

# Documentation task
bd create --title "Update API security docs" \
  --type task \
  --priority 3 \
  --labels "docs"
# Created: gt-g5h6i
```

:::tip[Detailed Descriptions]

The more detail you provide in the bead description, the better the polecat will understand the task. Include file paths, expected behavior, and acceptance criteria when possible.

:::

### Step 2: Create a Convoy

Bundle the beads into a convoy for batch tracking:

```bash
gt convoy create "API Security Hardening" gt-a1b2c gt-d3e4f gt-g5h6i
# Created: hq-cv-007
```

### Step 3: Sling Work to Rigs

Assign each issue individually to the appropriate rig:

```bash
# Send security fix to the backend rig
gt sling gt-a1b2c backend

# Send rate limiting to the same rig
gt sling gt-d3e4f backend

# Send docs to the docs rig
gt sling gt-g5h6i docs
```

Each `gt sling` command:

1. Updates the bead status to `hooked`
2. Attaches the bead to the target rig
3. Spawns a polecat worker (if Tmux available)
4. The polecat finds its hook and begins working

### Step 4: Monitor Progress

Track progress through multiple views:

```bash
# Convoy-level view
gt convoy show hq-cv-007

# Activity feed (live updates)
gt feed

# Rig-level polecat status
gt polecat list

# Merge queue status
gt mq list
```

Example convoy output:

```
Convoy: hq-cv-007 "API Security Hardening"
Status: OPEN
Progress: 1/3 issues completed

Tracked Issues:
  [DONE]        gt-a1b2c  Fix SQL injection in login
  [IN_PROGRESS] gt-d3e4f  Add rate limiting to API
  [HOOKED]      gt-g5h6i  Update API security docs

Polecats:
  backend/toast  -> gt-a1b2c (completed, awaiting merge)
  backend/alpha  -> gt-d3e4f (implementing)
  docs/bravo     -> gt-g5h6i (loading context)
```

### Step 5: Handle Issues

If a polecat gets stuck or fails:

```bash
# Release the bead back to the pool
gt release gt-d3e4f

# Re-sling to a different rig or agent
gt sling gt-d3e4f backend --agent gemini

# Or manually nudge the polecat
gt nudge backend/polecats/alpha "Status update? Need help?"
```

### Step 6: Add More Work

You can add issues to the convoy at any time:

```bash
# Create a new bead
bd create --title "Add API key rotation" --type feature --priority 2
# Created: gt-n1e2w

# Add to existing convoy
gt convoy add hq-cv-007 gt-n1e2w

# Sling it
gt sling gt-n1e2w backend
```

### Step 7: Convoy Completes

When all tracked issues are done, the convoy auto-closes. You can verify:

```bash
gt convoy list
# hq-cv-007  API Security Hardening  [COMPLETED]  4/4 done
```

## Handling Dependencies

For work that must happen in sequence, use bead dependencies:

```bash
# Create the dependent bead
bd create --title "Update client for new API" \
  --type task \
  --depends-on gt-d3e4f \
  --description "After rate limiting lands, update the client SDK"
# Created: gt-c1l2i

# Add to convoy
gt convoy add hq-cv-007 gt-c1l2i

# Don't sling it yet -- it's blocked on gt-d3e4f
bd show gt-c1l2i
# Status: pending (blocked by gt-d3e4f)

# When gt-d3e4f completes, the dependency resolves automatically
# Now you can sling it:
gt sling gt-c1l2i frontend
```

## Cross-Rig Coordination

Manual convoys are particularly useful for cross-rig work:

```bash
# Backend changes
gt sling gt-a1b2c backend

# Frontend changes that depend on backend
gt sling gt-d3e4f frontend

# Shared library changes
gt sling gt-g5h6i shared-lib

# All tracked in one convoy
gt convoy show hq-cv-007
```

## Comparison with Other Workflows

| Aspect | Manual Convoy | Mayor Workflow | Minimal Mode |
|--------|:------------:|:--------------:|:------------:|
| Who creates beads | You | Mayor | You |
| Who creates convoy | You | Mayor | You |
| Who slings work | You | Mayor | You |
| Auto-spawn polecats | Yes (with Tmux) | Yes | No |
| Auto-merge (Refinery) | Yes | Yes | No |
| Progress monitoring | Manual | Mayor reports | Manual |
| Assignment flexibility | Full | Mayor decides | Full |

:::tip[Stranded Convoy Check]

If you forget to sling some issues, use `gt convoy stranded` to find convoys with unassigned work:
```bash
gt convoy stranded
# hq-cv-007 has 1 unassigned issue: gt-n1e2w
```

:::

## See Also

- **[Convoys](../concepts/convoys.md)** -- The convoy concept
- **[Sling CLI](../cli-reference/sling.md)** -- Dispatching work to rigs
- **[Convoy CLI](../cli-reference/convoys.md)** -- Convoy management commands


========================================================================
# Formula Workflow
# URL: /docs/workflows/formula-workflow
========================================================================

# Formula Workflow

The **Formula Workflow** uses predefined TOML templates to orchestrate structured, repeatable processes. Instead of describing work in free-form text, you select a formula that defines the exact steps, run it with variables, and let Gas Town execute the workflow. This is ideal for standardized processes like releases, design explorations, and audits.

---

## When to Use This Workflow

- You have a repeatable process with defined steps
- You want consistency across executions (every release follows the same steps)
- You want to leverage built-in formulas for code review, design, or audits
- You are creating custom workflows for your team

:::info[Prerequisites]

- Gas Town installed with at least one rig
- Formulas available in `.beads/formulas/`
- For convoy formulas: Tmux and multiple polecats

:::

## Overview

```mermaid
graph LR
    F["Formula<br/>(TOML template)"] -->|gt formula run| M["Molecule<br/>(live instance)"]
    M --> S1["Step 1: Design ✓"]
    M --> S2["Step 2: Implement ●"]
    M --> S3["Step 3: Review ○"]
    M --> S4["Step 4: Test ○"]
    M --> S5["Step 5: Submit ○"]
```

## Step-by-Step

### Step 1: Browse Available Formulas

List all available formulas in your workspace:

```bash
gt formula list
```

Example output:

```
Name                        Type      Steps  Description
shiny                       workflow  5      Design-implement-review-test-submit
shiny-secure                workflow  6      Shiny with security audit
code-review                 convoy    10     Parallel multi-dimension code review
design                      convoy    6      Parallel design exploration
security-audit              convoy    4      Security-focused analysis
mol-polecat-work            workflow  9      Full polecat work lifecycle
rule-of-five                convoy    5      Five-perspective analysis
...
```

### Step 2: Examine a Formula

View the details of a formula before running it:

```bash
gt formula show shiny
```

This shows:

- Description and purpose
- Step sequence with dependencies
- Required and optional variables
- For convoy formulas: parallel legs and synthesis step

### Step 3: Run the Formula

Run a formula with the required variables:

```bash
# Run a simple workflow formula
gt formula run shiny --var feature="Add notification system"

# Run with multiple variables
gt formula run shiny --var feature="Add notifications" --var assignee="polecat/toast"
```

For convoy formulas with parallel execution:

```bash
# Run a code review
gt formula run code-review --pr=42

# Run a design exploration
gt formula run design --problem="Redesign the merge queue"

# Run with a specific preset
gt formula run code-review --pr=42 --preset=gate
```

### Step 4: Track Progress

Monitor the molecule's progress through its steps:

```bash
# Show molecule status
gt mol status

# Show detailed progress
gt mol progress <mol-id>

# Show the dependency graph
gt mol dag <mol-id>
```

Example progress output:

```
Molecule: mol-shiny-x1y2z "Add notification system"
Formula: shiny v1
Progress: 2/5 steps complete

Steps:
  [✓] design          Design Add notification system
  [✓] implement       Implement Add notification system
  [●] review          Review implementation
  [ ] test            Test Add notification system
  [ ] submit          Submit for merge
```

### Step 5: Advance Steps

For workflow formulas, the assigned agent advances through steps. You can also manually mark steps:

```bash
# Mark current step as done
gt mol step done

# The next step becomes active automatically
gt mol status
```

### Step 6: Completion

When all steps are complete, the molecule finishes. For convoy formulas, the synthesis step runs after all parallel legs complete.

## Formula Types in Detail

### Workflow Formulas

Workflow formulas define a linear (or DAG) sequence of steps executed by a single agent:

```mermaid
graph LR
    A[design] --> B[implement]
    B --> C[review]
    C --> D[test]
    D --> E[submit]
```

**Built-in workflow formulas:**

| Formula | Steps | Use Case |
|---------|-------|----------|
| `shiny` | 5 | Standard: design, implement, review, test, submit |
| `shiny-secure` | 6 | Adds security audit after review |
| `shiny-enterprise` | 7+ | Full enterprise gates (human approvals) |
| `mol-polecat-work` | 9 | Complete polecat lifecycle with preflight |

**Example: The `shiny` formula**

```toml
[[steps]]
id = "design"
title = "Design {{feature}}"
description = "Think carefully about architecture..."

[[steps]]
id = "implement"
needs = ["design"]
title = "Implement {{feature}}"
description = "Write the code..."

[[steps]]
id = "review"
needs = ["implement"]
title = "Review implementation"
description = "Self-review the changes..."

[[steps]]
id = "test"
needs = ["review"]
title = "Test {{feature}}"
description = "Write and run tests..."

[[steps]]
id = "submit"
needs = ["test"]
title = "Submit for merge"
description = "Final check and submit..."
```

### Convoy Formulas

Convoy formulas spawn multiple parallel agents (legs), each working on a different dimension of a problem. A synthesis step combines the results:

```mermaid
graph TD
    Start[Formula Run] --> L1[Leg: correctness]
    Start --> L2[Leg: security]
    Start --> L3[Leg: performance]
    Start --> L4[Leg: elegance]

    L1 --> Synth[Synthesis]
    L2 --> Synth
    L3 --> Synth
    L4 --> Synth

    Synth --> Result[Unified Output]
```

**Built-in convoy formulas:**

| Formula | Legs | Use Case |
|---------|------|----------|
| `code-review` | 10 | Comprehensive code review from multiple perspectives |
| `design` | 6 | Design exploration across API, data, UX, scale, security, integration |
| `security-audit` | 4 | Focused security analysis |
| `rule-of-five` | 5 | Five-perspective general analysis |

**Code review presets:**

| Preset | Legs | Purpose |
|--------|------|---------|
| `gate` | 4 | Light review: wiring, security, smells, test-quality |
| `full` | 10 | All legs for thorough review |
| `security-focused` | 4 | Security-heavy: security, resilience, correctness, wiring |
| `refactor` | 4 | Quality focus: elegance, smells, style, commit-discipline |

```bash
# Run with a preset
gt formula run code-review --pr=42 --preset=gate

# Run specific legs only
gt formula run code-review --pr=42 --legs=security,correctness,wiring
```

## Creating Custom Formulas

### Basic Workflow Formula

Create a new TOML file in `.beads/formulas/`:

```toml
# .beads/formulas/my-release.formula.toml
description = "Release workflow for production deployment"
formula = "my-release"
type = "workflow"
version = 1

[[steps]]
id = "version-bump"
title = "Bump version to {{version}}"
description = "Update version numbers in all relevant files"

[[steps]]
id = "changelog"
title = "Update changelog"
needs = ["version-bump"]
description = "Generate and review changelog entries"

[[steps]]
id = "build"
title = "Build release artifacts"
needs = ["changelog"]
description = "Run the build pipeline and verify artifacts"

[[steps]]
id = "smoke-test"
title = "Run smoke tests"
needs = ["build"]
description = "Execute smoke test suite against built artifacts"

[[steps]]
id = "tag-release"
title = "Tag and publish v{{version}}"
needs = ["smoke-test"]
description = "Create git tag and publish release"

[vars]
[vars.version]
description = "Release version (e.g., 2.3.1)"
required = true
```

Run it:

```bash
gt formula run my-release --var version="2.3.1"
```

### Custom Convoy Formula

```toml
# .beads/formulas/incident-review.formula.toml
description = "Post-incident review from multiple perspectives"
formula = "incident-review"
type = "convoy"
version = 1

[inputs]
[inputs.incident]
description = "Incident ID or description"
type = "string"
required = true

[[legs]]
id = "timeline"
title = "Timeline Reconstruction"
focus = "What happened and when"
description = "Reconstruct the incident timeline..."

[[legs]]
id = "root-cause"
title = "Root Cause Analysis"
focus = "Why it happened"
description = "Identify the root cause..."

[[legs]]
id = "impact"
title = "Impact Assessment"
focus = "What was affected"
description = "Assess the blast radius..."

[[legs]]
id = "prevention"
title = "Prevention Plan"
focus = "How to prevent recurrence"
description = "Propose preventive measures..."

[synthesis]
title = "Incident Report"
description = "Synthesize all analyses into a unified incident report..."
depends_on = ["timeline", "root-cause", "impact", "prevention"]
```

### Formula CLI Reference

```bash
# Create a formula interactively
gt formula create my-workflow

# List all formulas
gt formula list

# Show formula details
gt formula show <name>

# Run a formula
gt formula run <name> --var key=value

# Run with preset (convoy formulas)
gt formula run <name> --preset=<preset-name>
```

## Variables and Templating

Formulas use Go `text/template` syntax for variable interpolation:

```toml
[[steps]]
title = "Implement {{.feature}}"
description = "Build the {{.feature}} feature for {{.assignee}}"
```

Variables are provided at runtime via `--var`:

```bash
gt formula run shiny --var feature="notifications" --var assignee="toast"
```

### Variable Definitions

```toml
[vars]
[vars.feature]
description = "The feature being implemented"
required = true

[vars.assignee]
description = "Who is assigned"
required = false
default = "auto"
```

| Field | Purpose |
|-------|---------|
| `description` | Explains what the variable is for |
| `required` | Whether the formula fails without it |
| `default` | Default value if not provided |

## Best Practices

:::tip[Start with Built-in Formulas]

Gas Town ships with well-tested formulas for common workflows. Try `shiny` for feature work and `code-review` for reviews before creating custom formulas.

:::

:::tip[Keep Steps Atomic]

Each step should represent one logical unit of work. If a step is too large, the agent may lose context. If too small, the overhead of step tracking outweighs the benefit.

:::

:::tip[Use Gates for External Waits]

If a step needs to wait for CI, human approval, or a timer, use a [Gate](../concepts/gates.md) instead of busy-waiting. Gates let the agent park the workflow and resume when the condition is met.

:::

:::warning[Formula Versioning]

Increment the `version` field when making breaking changes to a formula. Existing molecules poured from the old version will continue using the old step definitions.

:::

## See Also

- **[Molecules & Formulas](../concepts/molecules.md)** -- The concept behind formulas and molecules
- **[Formula CLI](../cli-reference/formula.md)** -- Formula management commands
- **[MEOW Stack](../concepts/meow-stack.md)** -- Where formulas fit in the abstraction model


========================================================================
# Code Review Workflow
# URL: /docs/workflows/code-review
========================================================================

# Code Review Workflow

Gas Town provides a powerful **parallel code review** system using the `code-review` convoy formula. Instead of a single reviewer reading through a PR, Gas Town spawns multiple specialized reviewer polecats -- each examining the code from a different perspective -- then synthesizes their findings into a unified, prioritized review.

---

## When to Use This Workflow

- You want thorough, multi-perspective code review
- You have a PR or branch ready for review
- You want automated detection of issues that human reviewers commonly miss
- You need a structured review before merging to main

:::info[Prerequisites]

- Gas Town installed with at least one rig
- Tmux for parallel polecat spawning
- The rig's Witness and Refinery running

:::

## How It Works

```mermaid
graph TD
    PR["PR #42 or Branch"] -->|gt formula run code-review| Launch[Launch Review]

    Launch --> L1["Polecat 1<br/>Correctness"]
    Launch --> L2["Polecat 2<br/>Security"]
    Launch --> L3["Polecat 3<br/>Performance"]
    Launch --> L4["Polecat 4<br/>Elegance"]
    Launch --> L5["Polecat 5<br/>Resilience"]
    Launch --> L6["Polecat 6<br/>Style"]
    Launch --> L7["Polecat 7<br/>Code Smells"]
    Launch --> L8["Polecat 8<br/>Wiring"]
    Launch --> L9["Polecat 9<br/>Commit Discipline"]
    Launch --> L10["Polecat 10<br/>Test Quality"]

    L1 --> Synth["Synthesis<br/>Unified Review"]
    L2 --> Synth
    L3 --> Synth
    L4 --> Synth
    L5 --> Synth
    L6 --> Synth
    L7 --> Synth
    L8 --> Synth
    L9 --> Synth
    L10 --> Synth

    Synth --> Output["review-summary.md"]
```

Each polecat works in parallel, examining the code from its specialized perspective. When all legs complete, a synthesis step combines findings into a single prioritized document.

## Review Dimensions

The `code-review` formula examines code from 10 different perspectives:

### Analysis Legs (Read and Analyze Code)

| Leg | Focus | What It Catches |
|-----|-------|-----------------|
| **Correctness** | Logic and edge cases | Off-by-one errors, null handling, race conditions, dead code |
| **Performance** | Bottlenecks and efficiency | O(n^2) algorithms, N+1 queries, memory leaks, missing caching |
| **Security** | Vulnerabilities and attack surface | Injection, auth bypass, exposed secrets, OWASP Top 10 |
| **Elegance** | Design clarity and abstraction | Unclear naming, over-engineering, coupling, SOLID violations |
| **Resilience** | Error handling and failure modes | Swallowed errors, missing timeouts, partial failure states |
| **Style** | Convention compliance | Naming violations, formatting, import organization, doc gaps |
| **Code Smells** | Anti-patterns and tech debt | God classes, long methods, deep nesting, copy-paste code |

### Verification Legs (Check Implementation Quality)

| Leg | Focus | What It Catches |
|-----|-------|-----------------|
| **Wiring** | Installed-but-not-wired gaps | Dependencies added but never imported, old implementations not replaced |
| **Commit Discipline** | Commit quality and atomicity | Giant WIP commits, poor messages, unatomic changes |
| **Test Quality** | Test meaningfulness | Weak assertions, missing negative tests, tests that cannot fail |

## Running a Code Review

### Review a Pull Request

```bash
gt formula run code-review --pr=42
```

### Review a Branch

```bash
gt formula run code-review --branch=feature/auth-redesign
```

### Review Specific Files

```bash
gt formula run code-review --files="src/auth/*.go"
```

## Presets

Presets let you control which legs run, balancing thoroughness against speed and cost:

### Gate Preset (Light Review)

Fast, focused on blockers. Good for automatic merge-gate reviews:

```bash
gt formula run code-review --pr=42 --preset=gate
```

| Legs | Purpose |
|------|---------|
| wiring | Catch installed-but-not-used dependencies |
| security | Catch vulnerabilities |
| smells | Catch anti-patterns |
| test-quality | Verify tests are meaningful |

### Full Preset (Comprehensive)

All 10 legs. Use for major features, security-sensitive changes, or important releases:

```bash
gt formula run code-review --pr=42 --preset=full
```

### Security-Focused Preset

Heavy on security analysis:

```bash
gt formula run code-review --pr=42 --preset=security-focused
```

| Legs | Purpose |
|------|---------|
| security | Vulnerability analysis |
| resilience | Error handling and failure modes |
| correctness | Logic errors that could be exploited |
| wiring | Dependency gaps |

### Refactor Preset

Focus on code quality during refactoring:

```bash
gt formula run code-review --pr=42 --preset=refactor
```

| Legs | Purpose |
|------|---------|
| elegance | Design clarity |
| smells | Anti-patterns |
| style | Convention compliance |
| commit-discipline | Commit quality |

### Custom Leg Selection

Run specific legs only:

```bash
gt formula run code-review --pr=42 --legs=security,correctness,wiring,test-quality
```

## Review Output

### Individual Leg Findings

Each leg writes its findings to `.reviews/<review-id>/<leg-id>-findings.md`:

```
.reviews/review-abc123/
├── correctness-findings.md
├── security-findings.md
├── performance-findings.md
├── elegance-findings.md
├── resilience-findings.md
├── style-findings.md
├── smells-findings.md
├── wiring-findings.md
├── commit-discipline-findings.md
├── test-quality-findings.md
└── review-summary.md          # Synthesized review
```

### Findings Format

Each leg's findings follow a standard format:

```markdown
# Correctness Review

## Summary
Brief overview of findings from this perspective.

## Critical Issues (P0 - Must fix before merge)
- **src/auth/login.go:42** -- SQL injection via string concatenation
  - Impact: Remote code execution
  - Fix: Use parameterized queries

## Major Issues (P1 - Should fix before merge)
- **src/auth/validate.go:18** -- Missing null check on user input
  - Impact: Panic on nil pointer
  - Fix: Add nil guard before accessing field

## Minor Issues (P2 - Nice to fix)
- ...

## Observations
- ...
```

### Synthesized Review

The `review-summary.md` combines all leg findings into a single document:

```markdown
# Code Review Summary: PR #42

## Executive Summary
Overall assessment and merge recommendation.

## Critical Issues (P0)
Deduplicated critical findings from all legs.

## Major Issues (P1)
Grouped by theme.

## Minor Issues (P2)
Briefly listed.

## Wiring Gaps
Dependencies added but not wired.

## Commit Quality
Assessment from commit-discipline leg.

## Test Quality
Assessment from test-quality leg.

## Positive Observations
What is done well.

## Recommendations
Actionable next steps.
```

## Integration with Merge Queue

The code review workflow integrates naturally with the Refinery merge queue:

```mermaid
graph LR
    PR[PR Submitted] --> Review[Code Review<br/>gate preset]
    Review --> Decision{Issues Found?}
    Decision -->|P0 Critical| Block[Block Merge]
    Decision -->|P1 Major| Flag[Flag for Review]
    Decision -->|Clean| Merge[Refinery Merges]
    Block --> Fix[Author Fixes]
    Fix --> Review
```

### Automatic Gate Review

For automated pipelines, use the `gate` preset as a merge gate:

```bash
# In CI or merge queue configuration:
gt formula run code-review --pr=$PR_NUMBER --preset=gate

# Check for critical issues
# If P0 issues found, block the merge
# If clean, proceed to merge
```

### Manual Full Review

For important changes, run the full review before the author finishes:

```bash
gt formula run code-review --branch=feature/big-refactor --preset=full
```

Share the `review-summary.md` with the team for discussion.

## Example Session

```bash
# Start a code review for PR #42
$ gt formula run code-review --pr=42 --preset=full

Launching code review for PR #42...
Spawning 10 review polecats:
  ✓ correctness (polecat: rev-alpha)
  ✓ performance (polecat: rev-bravo)
  ✓ security (polecat: rev-charlie)
  ✓ elegance (polecat: rev-delta)
  ✓ resilience (polecat: rev-echo)
  ✓ style (polecat: rev-foxtrot)
  ✓ smells (polecat: rev-golf)
  ✓ wiring (polecat: rev-hotel)
  ✓ commit-discipline (polecat: rev-india)
  ✓ test-quality (polecat: rev-juliet)

Review in progress... Monitor with: gt mol status

# Check progress
$ gt mol status
Progress: 7/10 legs complete, 3 in progress

# When complete
$ cat .reviews/review-abc123/review-summary.md

# Review Summary: PR #42 "Add rate limiting"
#
# ## Executive Summary
# Overall solid implementation with 1 critical security issue
# and 2 major performance concerns. Recommend fixing P0 before merge.
# ...
```

## Tips

:::tip[Start with Gate Preset]

The `gate` preset (4 legs) runs quickly and catches the most impactful issues. Use it as your default and escalate to `full` for major changes.

:::

:::tip[Review Before Merging]

Run the code review before polecats submit to the merge queue. This catches issues early when they are cheapest to fix.

:::

:::warning[Cost Awareness]

The `full` preset spawns 10 polecats plus a synthesis step. This uses significant compute. Use presets strategically -- `gate` for routine PRs, `full` for releases and major features.

:::

:::note[Custom Review Dimensions]

You can create your own code review formula with custom legs tailored to your codebase. For example, add legs for "accessibility", "i18n", or "API compatibility" based on your project's needs.

:::

## See Also

- **[Refinery](../agents/refinery.md)** -- Merge queue agent


========================================================================
# Crew Collaboration
# URL: /docs/workflows/crew-collaboration
========================================================================

# Crew Collaboration

This workflow describes how human developers in [Crew workspaces](../agents/crew.md) work alongside AI polecats. The core pattern: humans produce **guzzoline** (specifications, designs, reviews) and polecats consume it (implementation, testing, fixes).

---

## When to Use This Workflow

Use crew collaboration when:

- You want to actively work alongside polecats in the same rig
- You need to review and guide AI output in real-time
- You're doing design work that feeds polecat implementation tasks
- You want to maintain code quality while scaling with AI workers

---

## The Guzzoline Pattern

Crew members produce specifications; polecats consume them.

```mermaid
graph LR
    H["Crew Member<br/>(Human)"] -->|creates beads| B["Issue Backlog"]
    B -->|slung to rig| P["Polecats<br/>(AI Workers)"]
    P -->|submit MRs| R["Refinery"]
    R -->|merged to main| M["main branch"]
    M -->|pull| H
    H -->|reviews & creates follow-ups| B
```

### Step 1: Create Detailed Beads

Write clear, implementable beads with enough context for a polecat to execute:

```bash
bd create --title "Add rate limiting to /api/users endpoint" \
  --type task --priority 1 \
  --description "Add rate limiting middleware to the /api/users endpoint.
Use the existing ratelimit package. Config: 100 requests per minute per IP.
Return 429 with Retry-After header when exceeded.
Tests: unit test for the middleware, integration test hitting the endpoint."
```

### Step 2: Sling to Polecats

```bash
gt sling ga-abc12 myproject
```

### Step 3: Monitor and Guide

```bash
# Watch polecat progress
gt peek polecat:toast --rig myproject

# Check convoy status
gt convoy list

# Review merge queue
gt mq list --rig myproject
```

### Step 4: Pull and Review

```bash
# After polecat work merges to main
git pull

# Review the changes
git log --oneline -5
git diff HEAD~1
```

### Step 5: Create Follow-ups

If the polecat's work needs adjustment:

```bash
bd create --title "Fix: rate limiter missing IPv6 normalization" \
  --type bug --priority 1
```

---

## Parallel Work Pattern

The most productive crew collaboration involves working in parallel with polecats:

```
You (Crew)                    Polecats
─────────────                 ─────────
Design feature A              Implement feature B (from earlier spec)
  ↓                             ↓
Create beads for A            Submit MR for B
  ↓                             ↓
Review B's MR (git pull)      Pick up feature A beads
  ↓                             ↓
File follow-ups for B         Implement feature A
  ↓                             ↓
Design feature C              ...
```

The key insight: while polecats implement your previous specs, you design the next batch. This creates a pipeline where neither you nor the polecats are ever idle.

---

## Direct Collaboration via Mail

When a polecat needs guidance or you need to redirect:

```bash
# Send instructions to a specific polecat
gt nudge polecat:toast --rig myproject "Focus on the API tests first, skip UI for now"

# Send work instructions via mail
gt mail send myproject/polecats/toast -s "Priority change" -m "
Drop the current task. Pick up ga-xyz instead - it's blocking the release."
```

---

## PR Sheriff Pattern

A crew member can take on a standing role as **PR Sheriff** -- reviewing and managing pull requests:

```bash
# On each session startup
gh pr list --state open

# Classify PRs
# - Simple fixes → approve and merge
# - Complex changes → review in detail
# - Polecat output → verify tests pass, check for quality
```

The PR Sheriff hooks this work to their session, creating a permanent review cadence.

---

## Quality Gates

Crew members are the quality backstop. While polecats run tests before submitting, crew reviews catch:

- **Architectural drift** -- Polecat implementations that don't match the system's design
- **Missing edge cases** -- Tests that pass but don't cover real-world scenarios
- **Security issues** -- Input validation, authentication gaps, injection risks
- **Code style** -- Patterns that work but don't match the project's conventions

```bash
# After pulling merged polecat work
git pull
go test ./...              # Verify tests still pass
golangci-lint run ./...    # Check style and quality
```

---

## Working Across Rigs

Crew members can work on multiple rigs using [worktrees](../concepts/rigs.md):

```bash
# Create a worktree in another rig
gt worktree beads

# Work in the other rig's codebase
cd ~/gt/beads/crew/myproject-healer/

# When done, remove the worktree
gt worktree remove beads
```

---

## Tips

1. **Write specs, not code.** Your highest-leverage activity is creating well-specified beads. A 10-minute spec saves a polecat from spending 30 minutes exploring the wrong approach.

2. **Review before designing.** Pull merged work and review it before starting new design work. Polecat output often reveals issues or patterns that inform your next spec.

3. **Stay in sync.** Run `git pull` frequently. In an active rig with multiple polecats, `main` can advance several commits per hour.

4. **Use beads for everything.** Even small follow-ups should be beads. This creates an audit trail and lets polecats pick up work autonomously.

5. **Handoff when context fills.** Crew sessions have the same context limits as any agent. Use `gt handoff` to cycle to a fresh session with notes.

---

## See Also

- **[Crew Workspaces](../agents/crew.md)** -- Full documentation of crew workspace setup and capabilities
- **[Mayor Workflow](mayor-workflow.md)** -- The fully automated alternative where the Mayor coordinates everything
- **[Manual Convoy](manual-convoy.md)** -- Fine-grained control over work assignment
- **[Session Cycling](../concepts/session-cycling.md)** -- How to handle context limits in crew sessions
- **[Crew](../agents/crew.md)** -- The Crew agent type
- **[Handoff Ceremony](handoff-ceremony.md)** -- Session transition protocol for crew
- **[Hooks](../concepts/hooks.md)** -- Work assignment for crew members


========================================================================
# Handoff Ceremony
# URL: /docs/workflows/handoff-ceremony
========================================================================

# Handoff Ceremony

The **handoff ceremony** is the protocol agents follow when transitioning from one session to the next. It ensures that work survives context boundaries — no progress is lost, no context is orphaned, and the next session can resume immediately.

---

## Why Handoffs Matter

AI agents have finite context windows. As a session progresses — reading files, writing code, communicating — the context fills up. At capacity, the agent becomes sluggish, repetitive, or loses track of its goals.

The handoff ceremony solves this by defining a clean exit protocol:

1. **Land all work** (commit and push)
2. **Record state** (update beads, close finished work)
3. **Write context notes** (handoff mail for the next session)
4. **Exit** (the next session picks up via the hook)

Without a ceremony, session transitions are chaotic — uncommitted code, lost context, duplicated effort. The ceremony makes transitions deterministic.

---

## The Two Persistence Mechanisms

Handoffs rely on two independent systems:

### 1. The Hook (What You're Working On)

The [hook](../concepts/hooks.md) persists the agent's current work assignment. Whether it is a [molecule](../concepts/molecules.md), a [bead](../concepts/beads.md), or hooked mail, the hook survives session restarts. When the new session starts and runs `gt prime`, it finds the work still on its hook and continues.

The hook is **automatic** — you do not need to do anything special to preserve it.

### 2. Handoff Mail (Context Notes)

Handoff mail is **optional but strongly recommended**. It captures nuances the hook does not — what you tried, what worked, what to do next, which files to look at first.

```bash
gt handoff -s "Working on auth bug" -m "
Found the issue is in token refresh logic.
Check line 145 in auth.go first.
The failing test is TestRefreshExpired.
"
```

The mail is addressed to yourself. Your next session reads it for orientation.

---

## Step-by-Step: Polecat Handoff

Polecats follow the strictest handoff ceremony because their work flows through the merge pipeline.

### 1. Finish Current Step

Complete the logical unit of work you are on. Do not hand off mid-implementation if you can avoid it.

### 2. Commit and Push

```bash
git add <files>
git commit -m "Implement input validation for signup form"
git push
```

:::danger[Non-Negotiable]

Unpushed code in a multi-agent environment is code at risk. Another agent may be assigned to the same rig. Always push before handing off.

:::

### 3. Update Beads

Close finished work, update status on in-progress items:

```bash
bd close ga-abc --reason "Implemented and tests passing"
bd update ga-def --status=in_progress --comment "50% done, form validation complete"
```

### 4. Submit or Handoff

If the work is **done**, submit to the merge queue:

```bash
gt done    # Submits MR, Witness nukes sandbox
```

If the work is **not done** but context is full, hand off:

```bash
gt handoff -s "Auth validation 50% done" -m "
Completed:
- Form validation (signup, login)
- Error message display

Remaining:
- Token refresh validation
- Rate limiting tests

Next step: Look at auth/refresh.go line 145.
"
```

---

## Step-by-Step: Crew Handoff

Crew handoffs are more relaxed — you are human-managed, not monitored by a Witness. But the same principles apply.

### 1. Land Your Changes

```bash
git pull --rebase           # Catch any remote changes
git add <files>
git commit -m "Add sidebar navigation fixes"
git push                    # MANDATORY
git status                  # Verify "up to date with origin/main"
```

### 2. File Follow-Up Work

Create beads for anything that needs attention later:

```bash
bd create "Follow-up: add error handling for edge case in auth" -t task
bd create "Follow-up: flaky test in TestRefreshExpired" -t bug
```

### 3. Close Finished Beads

```bash
bd close ga-abc --reason "Completed — sidebar navigation fixed"
```

### 4. Handoff

```bash
gt handoff -s "Sidebar fixes done, filed follow-ups" -m "
Completed:
- Fixed 9 misaligned sidebar_position values
- Removed orphaned logo.svg

Filed:
- ga-xyz: Follow-up for auth error handling
- ga-def: Flaky test issue

Build is clean. All pushed to main.
"
```

---

## Step-by-Step: Persistent Agent Handoff

Persistent agents (Deacon, Witness, Refinery) rarely hand off explicitly. They experience **automatic compaction** as their context window fills. However, they can be manually cycled:

```bash
# From outside: restart with fresh context
gt witness restart --fresh

# From inside the agent: self-cycle
gt handoff -s "Context full after 200 patrol cycles" -m "
All polecats healthy. No pending escalations.
Merge queue empty. Last patrol at 14:30.
"
```

After restart, the agent runs `gt prime` to reload role context and resumes its patrol molecule from the hook.

---

## What the Next Session Does

The receiving session (which may be "you" with a fresh context) follows the startup protocol:

```bash
# 1. Load full role context
gt prime

# 2. Check hook for assigned work
gt hook

# 3. Read handoff mail
gt mail inbox
gt mail read <handoff-mail-id>

# 4. Resume work immediately (GUPP)
# Do not wait for confirmation — the hook IS the assignment
```

```mermaid
sequenceDiagram
    participant S1 as Session 1
    participant Hook as Hook (persistent)
    participant Mail as Handoff Mail
    participant S2 as Session 2

    S1->>S1: Context filling up
    S1->>S1: git push (land changes)
    S1->>S1: bd close / bd update (beads)
    S1->>Mail: gt handoff -m "notes"
    S1->>S1: Session exits

    Note over Hook: Work assignment persists

    S2->>S2: New session starts
    S2->>S2: gt prime (load role context)
    S2->>Hook: gt hook (find assigned work)
    S2->>Mail: gt mail inbox (read handoff notes)
    S2->>S2: Resume work immediately (GUPP)
```

---

## Writing Good Handoff Notes

Handoff notes are for your next session — a version of you with no memory of this session.

### Do

- **Focus on what to do next**, not what you did
- **Include specific file paths and line numbers**
- **List beads you filed** so the next session can find them
- **Note any gotchas** (flaky tests, known issues, workarounds)
- **State the build status** (clean, broken, which tests fail)

### Don't

- Write a novel — keep it under 20 lines
- Repeat information that is already in beads
- Include emotional commentary ("I spent hours on this")
- Omit the build status — the next session needs to know immediately

### Example: Good Handoff

```
gt handoff -s "Token refresh bug — root cause found" -m "
Root cause: RefreshToken() in auth/refresh.go:145 doesn't check expiry.
Fix approach: Add time.Now().After(token.ExpiresAt) guard.

Remaining:
- Implement the fix (5 lines)
- Update TestRefreshExpired
- Run full test suite

Build: clean on main. Tests passing except TestRefreshExpired (the bug).
Filed: ga-xyz for related rate-limiting issue found during investigation.
"
```

### Example: Bad Handoff

```
gt handoff -m "Worked on the auth bug for a while. Made some progress but
couldn't figure it all out. The tests are kind of weird. Check the auth
folder I guess."
```

---

## Handoff vs Park

| Aspect | `gt handoff` | `gt park` |
|--------|-------------|-----------|
| **Intent** | Continue immediately in new session | Pause, resume later |
| **Successor** | Expected immediately | No immediate successor |
| **Use case** | Context full, keep working | End of day, break, deprioritized |
| **Hook state** | Persists | Persists |
| **Mail** | Handoff mail sent to self | No mail |
| **GUPP** | Next session runs immediately | No urgency |

---

## Common Mistakes

### Not Pushing Before Handoff

```bash
# WRONG: Handing off with uncommitted work
gt handoff -m "Almost done with the feature"
# The next session finds a dirty worktree and has to figure out what's staged

# RIGHT: Always push first
git add <files> && git commit -m "WIP: auth validation" && git push
gt handoff -m "Auth validation WIP pushed. Resume from auth/validate.go."
```

### Empty Handoff Notes

```bash
# WRONG: No context for the next session
gt handoff

# RIGHT: Always include notes
gt handoff -s "Feature X progress" -m "Completed A and B. Next: do C. See auth/refresh.go:145."
```

### Handing Off Mid-Thought

If you are in the middle of debugging and context is critical, capture it:

```bash
# Capture debugging state
gt handoff -s "Debugging race condition in convoy close" -m "
Hypothesis: The convoy-close check in deacon patrol races with polecat gt done.
Evidence: bd show ga-abc shows closed, but convoy ga-def still open.
Next: Add logging to convoy_close() in convoys.go:220 and reproduce.
Reproduction: Run gt convoy create with 3 beads, gt done on all 3 within 5s.
"
```

---

## See Also

- **[Session Cycling](../concepts/session-cycling.md)** -- The broader concept of context refresh that handoffs enable
- **[GUPP](../concepts/gupp.md)** -- The propulsion principle that drives immediate execution after handoff
- **[Hooks](../concepts/hooks.md)** -- The persistence mechanism that makes handoffs possible
- **[Molecules](../concepts/molecules.md)** -- Multi-step workflows track progress across handoff boundaries
- **[Crew Collaboration](crew-collaboration.md)** -- Crew-specific workflow patterns including handoff


========================================================================
# Operations
# URL: /docs/operations
========================================================================

# Operations

Running Gas Town in production means managing a fleet of AI agents across multiple projects. This section covers the day-to-day operational tasks: starting and stopping the system, monitoring health, handling escalations, troubleshooting problems, and extending functionality with plugins.

---

## Sections

| Guide | Description |
|-------|-------------|
| [Starting & Stopping](lifecycle.md) | Launch, pause, and shut down agents and rigs |
| [Monitoring & Health](monitoring.md) | Real-time feeds, dashboards, patrols, and audits |
| [Escalation System](escalations.md) | Priority-routed alerts with severity levels |
| [Troubleshooting](troubleshooting.md) | Common issues, diagnostics, and recovery procedures |
| [Plugins](plugins.md) | Extend Gas Town with custom gates and automation |

## Operational Mindset

Gas Town is a **supervisor tree** modeled on Erlang/OTP. Understanding three key operational principles will save you hours:

### 1. The Daemon is Dumb, Agents are Smart

The daemon process is a simple Go scheduler that sends heartbeats and processes lifecycle requests. All decision-making lives in the agents. If something is wrong, look at agent state (mail, hooks, beads) rather than the daemon.

### 2. Escalations Flow Upward

```
Polecat (stuck)
  --> Witness detects stall
    --> Witness nudges polecat
      --> If still stuck: Witness escalates to Deacon
        --> Deacon escalates to Mayor
          --> Mayor escalates to Human/Overseer
```

You only need to intervene when escalations reach you. The system handles lower-level recovery automatically.

### 3. Everything is in Git

All persistent state (beads, hooks, config, context) lives in git or the filesystem. This means you can always recover from failures by inspecting git history, and you can always understand what happened by reading the activity log.

## Quick Reference

```bash
# Start everything
gt start --all

# Check system health
gt doctor

# Watch real-time activity
gt feed

# View recent agent output
gt trail

# Stop everything, keep state
gt down

# Stop and clean up
gt shutdown --all
```

:::tip[Daily Operations Checklist]

1. Check `gt feed` for activity anomalies
2. Review `gt escalate list` for unacknowledged escalations
3. Run `gt doctor` to verify system health
4. Check `gt costs` to monitor token spend
5. Review `gt convoy list` for stalled or stranded convoys

:::

---

## Incident Response Playbook

When something goes wrong, follow this decision tree:

```
Is the system completely down?
├── Yes → gt daemon start && gt start --all (see Lifecycle: Emergency Recovery)
└── No
    ├── Is one rig broken?
    │   └── gt rig reboot <name> (see Lifecycle: Emergency Recovery)
    ├── Are polecats stuck?
    │   └── gt shutdown --polecats-only (see Troubleshooting: Stale Polecats)
    ├── Is the merge queue backed up?
    │   └── gt mq status (see Troubleshooting: Merge Conflicts)
    ├── Are costs spiking?
    │   └── gt costs --by-agent (see Escalations: Cost Spike scenario)
    └── Are escalations piling up?
        └── gt escalate list (see Escalations: Managing Escalations)
```

## Weekly Review Checklist

Run these checks once a week to catch slow-building problems:

```bash
# 1. Overall system health
gt doctor

# 2. Cost trends for the week
gt costs --since 7d --by-rig

# 3. Find orphaned resources consuming disk
gt orphans

# 4. Review and close stale escalations
gt escalate stale

# 5. Check for stranded convoys
gt convoy stranded

# 6. Clean up finished polecat worktrees
gt cleanup
```

## Operational Anti-Patterns

Avoid these common mistakes when running Gas Town:

| Anti-Pattern | Why It Hurts | What to Do Instead |
|-------------|-------------|-------------------|
| Manually fixing things on `main` | Bypasses the merge queue; can conflict with in-flight polecat work | Use a crew workspace or sling a fix bead |
| Ignoring P3 escalations | They accumulate and mask real problems | Review and close (or promote) P3s weekly |
| Restarting everything when one thing breaks | Disrupts working agents unnecessarily | Use surgical restarts: `gt rig reboot` or per-agent commands |
| Never running `gt cleanup` | Disk fills with orphaned worktrees | Schedule regular cleanup or add it to your weekly checklist |
| Over-slinging work to one rig | Creates merge queue bottlenecks | Distribute work across rigs when possible |


========================================================================
# Starting & Stopping
# URL: /docs/operations/lifecycle
========================================================================

# Starting & Stopping

Gas Town provides granular control over the system lifecycle -- from starting the entire fleet to managing individual agents and rigs. Understanding the distinction between **pause** (`gt down`) and **shutdown** (`gt shutdown`) is critical: pausing preserves worktrees and state for fast restart, while shutdown cleans up resources.

---

## Town-Level Lifecycle

### Starting Gas Town

#### `gt start` -- Core Agents

Starts the **Deacon** and **Mayor**, the two town-level persistent agents.

```bash
gt start
```

What happens:

1. The daemon process starts (if not already running)
2. A Deacon session launches in tmux
3. A Mayor session launches in tmux
4. Both agents run `gt prime` to load context

:::note

`gt start` does **not** start per-rig agents (Witnesses, Refineries). Use `gt start --all` to include them.

:::

#### `gt start --all` -- Full Fleet

Starts everything: Deacon, Mayor, plus all Witnesses and Refineries across all active rigs.

```bash
gt start --all
```

What happens:

1. Daemon starts
2. Deacon and Mayor sessions launch
3. For each active (non-parked) rig:
    - Witness session starts
    - Refinery session starts
4. All agents prime and begin patrol cycles

```bash
# Start with verbose output to watch the boot sequence
gt start --all --verbose
```

:::tip

After `gt start --all`, run `gt rig list` to confirm all rigs show the expected agent status.

:::

### Stopping Gas Town

Gas Town offers two stopping modes with very different behaviors:

#### `gt down` -- Pause (Preserve State)

Stops all running agent processes but **keeps worktrees intact**. Use this when you plan to resume soon.

```bash
gt down
```

| What it stops | What it keeps |
|---------------|---------------|
| Mayor session | Git worktrees |
| Deacon session | Hook attachments |
| Witness sessions | Bead state |
| Refinery sessions | Mail queues |
| Daemon process | Config and context |

Resume with:

```bash
gt start --all
```

:::info[When to use `gt down`]

Use `gt down` when taking a break, switching tasks, or performing maintenance. All work state is preserved and agents resume where they left off.

:::

#### `gt shutdown` -- Stop and Clean Up

Stops all processes **and** removes worktrees, cleaning up disk resources.

```bash
gt shutdown
```

This removes polecat worktrees and temporary state. Persistent state (beads, config, hooks) is preserved.

#### `gt shutdown --all` -- Full Cleanup

Stops everything including crew sessions and removes all ephemeral resources.

```bash
gt shutdown --all
```

| Flag | Behavior |
|------|----------|
| `gt shutdown` | Stop agents, remove polecat worktrees |
| `gt shutdown --all` | Also stop crew sessions, full cleanup |
| `gt shutdown --polecats-only` | Only stop and clean up polecats |

:::warning

`gt shutdown --all` stops crew sessions. Make sure human developers have committed and pushed their work before running this.

:::

#### `gt shutdown --polecats-only`

Targets only ephemeral workers. Useful when polecats are stuck but you want to keep persistent agents running.

```bash
gt shutdown --polecats-only
```

This is a surgical operation:

1. All running polecat sessions are terminated
2. Polecat worktrees are removed
3. Witnesses, Refineries, Mayor, and Deacon continue running
4. Hooked beads are released back to `pending` state

---

## Daemon Management

The daemon is a Go process that provides the heartbeat and lifecycle management layer underneath the agent sessions.

### `gt daemon start`

Start the background daemon process.

```bash
gt daemon start
```

The daemon:

- Sends heartbeats to the Deacon every 3 minutes
- Processes lifecycle requests (start/stop agents)
- Restarts crashed sessions when requested
- Polls external services (Discord, webhooks)

### `gt daemon stop`

Stop the daemon process.

```bash
gt daemon stop
```

:::warning

Stopping the daemon means no heartbeats reach the Deacon. The Deacon will detect the missing heartbeat and may escalate. Stop the Deacon first if doing planned maintenance.

:::

### `gt daemon status`

Check whether the daemon is running and view its state.

```bash
gt daemon status
```

Sample output:

```
Daemon: running (PID 12345)
Uptime: 4h 23m
Last heartbeat: 12s ago
Pending requests: 0
```

### `gt daemon logs`

View daemon log output.

```bash
# View recent logs
gt daemon logs

# Follow logs in real-time
gt daemon logs --follow

# Filter by level
gt daemon logs --level error
```

---

## Per-Agent Lifecycle

Individual agents can be started and stopped independently, which is useful for targeted restarts or debugging.

### Starting a Specific Agent

```bash
# Start the Mayor
gt mayor start

# Start the Deacon
gt deacon start

# Start a Witness for a specific rig
gt witness start --rig myproject

# Start a Refinery for a specific rig
gt refinery start --rig myproject
```

### Stopping a Specific Agent

```bash
# Stop the Mayor
gt mayor stop

# Stop the Deacon
gt deacon stop

# Stop a Witness
gt witness stop --rig myproject

# Stop a Refinery
gt refinery stop --rig myproject
```

### Restarting an Agent

```bash
# Restart the Mayor (stop + start)
gt mayor restart

# Restart with fresh context
gt mayor restart --fresh
```

:::note

When an agent restarts, it runs `gt prime` to reload context. All hook-attached work persists across restarts -- agents resume their work automatically.

:::

### Attaching to an Agent Session

```bash
# Attach to the Mayor session (interactive)
gt mayor attach

# Attach to a Witness
gt witness attach --rig myproject

# Attach to a specific polecat
gt polecat attach toast --rig myproject
```

---

## Death Warrants (Agent Cleanup)

When an agent becomes unresponsive, exceeds its timeout, or turns into a zombie process, Gas Town uses **death warrants** to coordinate cleanup. A death warrant is not an immediate kill -- it's a structured cleanup request that ensures work is recovered before the agent is terminated.

### How Death Warrants Work

```mermaid
sequenceDiagram
    participant D as Deacon
    participant W as Witness
    participant P as Stuck Polecat
    participant B as Boot

    D->>D: Patrol detects zombie polecat
    D->>D: Files death warrant (bead)
    D->>B: Boot picks up warrant on next tick
    B->>P: Attempts graceful shutdown
    B->>B: Recovers uncommitted work if possible
    B->>P: Terminates process
    B->>D: Reports cleanup complete
```

1. **Detection**: The [Deacon](../agents/deacon.md) detects zombies during its patrol cycle -- sessions that are alive but not responding, or processes consuming resources without producing output.
2. **Filing**: The Deacon files a death warrant as a bead, not an immediate kill. This creates an audit trail.
3. **Execution**: [Boot](../agents/boot.md) processes death warrants on daemon ticks. It attempts graceful shutdown first, recovers any uncommitted work, then terminates the process.
4. **Recovery**: Hooked work is released back to `pending` status. The [Witness](../agents/witness.md) can then re-sling the work to a fresh polecat.

### Death Warrants for Dogs

[Dogs](../agents/dogs.md) (Deacon infrastructure helpers) that exceed their task timeout also receive death warrants:

```bash
gt dog list                  # Check dog states
# Dogs past timeout show "death warrant pending"
```

### Why Not Kill Immediately?

The warrant system exists because immediate kills risk data loss:

- A polecat may have uncommitted changes worth saving
- A dog may be mid-write on a critical file
- The cleanup process itself needs coordination (remove worktree, release hooks, update beads)

The warrant gives Boot time to do this safely.

---

## Per-Rig Lifecycle

Rigs (project containers) have their own lifecycle commands that control the agents running within them.

### `gt rig start`

Start all agents for a rig (Witness + Refinery).

```bash
gt rig start myproject
```

### `gt rig stop`

Stop all agents in a rig.

```bash
gt rig stop myproject
```

### `gt rig boot`

Cold-start a rig from scratch. Creates fresh worktrees and initializes all agent state.

```bash
gt rig boot myproject
```

This is used when first adding a rig or after a `gt rig dock`.

### `gt rig reboot`

Stop all rig agents, clean up, and restart fresh.

```bash
gt rig reboot myproject
```

Equivalent to `gt rig stop` + cleanup + `gt rig boot`.

:::tip

`gt rig reboot` is the fastest way to recover from a rig that is in a bad state. It preserves beads and configuration but gives agents a clean start.

:::

### `gt rig park`

Take a rig offline without removing it. Parked rigs are ignored by `gt start --all`.

```bash
gt rig park myproject
```

Use cases:

- Temporarily disable a project while focusing on others
- Reduce resource consumption during low-activity periods
- Prevent agents from working on a rig during manual maintenance

### `gt rig unpark`

Bring a parked rig back online.

```bash
gt rig unpark myproject
```

After unparking, run `gt rig start myproject` to launch agents.

### `gt rig dock`

Archive a rig. Stops agents, removes worktrees, and marks the rig as docked.

```bash
gt rig dock myproject
```

Docked rigs retain their configuration and bead history but consume no runtime resources.

### `gt rig undock`

Restore a docked rig to active status.

```bash
gt rig undock myproject
```

After undocking, run `gt rig boot myproject` to create worktrees and start agents.

---

## Startup Order and Dependencies

When starting Gas Town, components must come up in a specific order. `gt start --all` handles this automatically, but understanding the dependency chain helps when troubleshooting partial starts or doing manual recovery.

### Required Startup Order

```mermaid
graph TD
    D[Daemon] --> DE[Deacon]
    D --> M[Mayor]
    DE --> W1[Witness: rig1]
    DE --> W2[Witness: rig2]
    W1 --> R1[Refinery: rig1]
    W2 --> R2[Refinery: rig2]
    R1 --> P1[Polecats: rig1]
    R2 --> P2[Polecats: rig2]
```

| Order | Component | Depends On | Why |
|-------|-----------|------------|-----|
| 1 | Daemon | Nothing | Provides heartbeat and lifecycle management |
| 2 | Deacon | Daemon | Monitors daemon heartbeats, manages town health |
| 3 | Mayor | Daemon | Coordinates work distribution across rigs |
| 4 | Witnesses | Deacon | Monitor per-rig health; report to Deacon |
| 5 | Refineries | Witnesses | Process merge queue; Witness monitors their health |
| 6 | Polecats | Refineries | Do the work; Refinery merges their output |

:::note

If you start a Witness before the Deacon, it will still function but the Deacon will not know about it until its next patrol cycle. Starting in the correct order ensures immediate monitoring coverage.

:::

### What Each Agent Needs at Startup

Every agent runs `gt prime` on startup to load its role context. This requires:

- Its home directory to exist (worktree for polecats, rig directory for others)
- The beads database to be accessible
- Hook state to be intact (for polecats, their assigned work)
- Mail queue to be readable (for checking incoming messages)

If any of these are missing, the agent will log errors. Use `gt doctor` to identify missing prerequisites.

---

## Emergency Recovery

When the normal lifecycle commands are not working, use these procedures to recover the system.

### Scenario: Full System Unresponsive

Nothing is responding -- `gt` commands hang or error out.

```bash
# Step 1: Check if the daemon is alive
gt daemon status

# Step 2: If daemon is dead, start it
gt daemon start

# Step 3: If gt commands still hang, check for tmux
tmux list-sessions

# Step 4: Kill all tmux sessions and restart clean
tmux kill-server
gt start --all
```

### Scenario: Single Rig in Bad State

One rig is misbehaving but others are fine.

```bash
# Surgical shutdown of the problem rig
gt rig stop myproject

# Clean up orphaned resources
gt cleanup --rig myproject

# Fresh start for the rig
gt rig boot myproject
```

### Scenario: Polecats Spawning and Immediately Dying

Polecats start but crash within seconds, repeatedly.

```bash
# Stop the spawn cycle
gt shutdown --polecats-only --rig myproject

# Check what is causing the crash
gt trail --rig myproject --last 20

# Common cause: broken main branch
# Verify tests pass on main
cd ~/gt/myproject/crew/yourname
git fetch origin && git checkout origin/main
# Run project tests

# If main is broken, fix it before resuming polecat work
```

### Scenario: Lost Work from Crashed Polecat

A polecat crashed before pushing its changes.

```bash
# Check if the worktree still exists
ls ~/gt/myproject/polecats/*/myproject/

# If it exists, recover the work
cd ~/gt/myproject/polecats/<name>/myproject
git status
git log --oneline -5
# Commit and push if there are changes worth saving

# If the worktree is gone, check for orphaned commits
gt orphans --commits --rig myproject
gt orphans --recover <commit-hash>
```

---

## Graceful Degradation

Gas Town is designed to continue operating even when some components fail. Understanding what keeps working and what stops helps you prioritize recovery.

| Failed Component | What Still Works | What Stops |
|-----------------|-----------------|------------|
| Daemon | All agents continue running | No new agents can be spawned; no heartbeats |
| Deacon | Rigs operate independently | No town-wide health monitoring; no zombie cleanup |
| Mayor | Existing work continues | No new work distribution; convoys not coordinated |
| Witness (one rig) | Other rigs unaffected | Stale polecats in that rig are not detected |
| Refinery (one rig) | Polecats can still work | Completed work accumulates but does not merge |
| Single polecat | All other polecats fine | One bead's work is delayed |

:::tip

The system degrades gracefully because each agent operates independently with its own state. A failed Witness in one rig has zero impact on polecats in another rig. Prioritize recovering components based on what is actually blocked.

:::

---

## Lifecycle State Machine

```mermaid
stateDiagram-v2
    [*] --> Added: gt rig add
    Added --> Active: gt rig boot
    Active --> Stopped: gt rig stop
    Stopped --> Active: gt rig start
    Active --> Parked: gt rig park
    Parked --> Active: gt rig unpark + start
    Active --> Docked: gt rig dock
    Docked --> Added: gt rig undock
    Active --> Rebooting: gt rig reboot
    Rebooting --> Active: auto
```

---

## Lifecycle Quick Reference

| Command | Scope | Processes | Worktrees | State |
|---------|-------|-----------|-----------|-------|
| `gt start` | Town | Start Mayor + Deacon | Unchanged | Unchanged |
| `gt start --all` | Town + Rigs | Start all agents | Unchanged | Unchanged |
| `gt down` | Town | Stop all | **Kept** | Preserved |
| `gt shutdown` | Town | Stop all | **Removed** | Preserved |
| `gt shutdown --all` | Town + Crew | Stop all + crew | **Removed** | Preserved |
| `gt shutdown --polecats-only` | Polecats | Stop polecats | Polecat trees removed | Beads released |
| `gt rig start <name>` | Single rig | Start rig agents | Unchanged | Unchanged |
| `gt rig stop <name>` | Single rig | Stop rig agents | Unchanged | Unchanged |
| `gt rig boot <name>` | Single rig | Start fresh | **Created** | Initialized |
| `gt rig reboot <name>` | Single rig | Stop + restart | **Recreated** | Reset |
| `gt rig park <name>` | Single rig | Stop | Unchanged | Parked |
| `gt rig dock <name>` | Single rig | Stop | **Removed** | Archived |

## See Also

- **[Agent Hierarchy](../architecture/agent-hierarchy.md)** -- How agents are organized for lifecycle management
- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- Periodic health monitoring that detects lifecycle issues
- **[Deacon](../agents/deacon.md)** -- The agent responsible for lifecycle coordination


========================================================================
# Monitoring & Health
# URL: /docs/operations/monitoring
========================================================================

# Monitoring & Health

Gas Town provides a suite of monitoring tools to observe real-time activity, audit historical work, diagnose system health, and track costs. Effective monitoring is how you keep a fleet of 10-30 agents running smoothly without micromanaging each one.

---

## Real-Time Activity

### `gt feed` -- Activity Dashboard

The primary real-time monitoring command. Shows a live stream of events across all agents and rigs.

```bash
gt feed
```

The feed displays:

- Agent starts and stops
- Work assignments and completions
- Merge queue activity
- Escalations raised and resolved
- Convoy progress updates
- Error events

```bash
# Filter to a specific rig
gt feed --rig myproject

# Filter by event type
gt feed --type merge
gt feed --type escalation

# Show only errors and warnings
gt feed --level warn

# JSON output for piping to other tools
gt feed --json
```

Sample output:

```
14:23:01 [myproject] polecat:toast  STARTED  gt-a1b2c "Fix login bug"
14:23:45 [docs]      polecat:bravo  DONE     gt-d3e4f "Update API docs"
14:24:02 [myproject] refinery       MERGE    gt-d3e4f merged to main
14:25:11 [myproject] witness        STALE    polecat:alpha idle 12m
14:25:12 [myproject] witness        NUDGE    polecat:alpha "check progress"
```

:::tip[Keep a Feed Running]

Open a dedicated terminal with `gt feed` running at all times. It is the fastest way to spot problems before they escalate.

:::

### `gt trail` -- Recent Agent Activity

Shows a summary of recent agent activity -- more concise than `gt feed`, focused on what happened rather than a live stream.

```bash
gt trail
```

```bash
# Show last N events
gt trail --last 50

# Filter by agent
gt trail --agent witness

# Filter by rig
gt trail --rig myproject

# Show activity in the last hour
gt trail --since 1h
```

Sample output:

```
Last 1h activity (23 events):

  myproject  4 polecats spawned, 3 completed, 1 active
  myproject  2 merges completed, 0 pending
  docs       1 polecat spawned, 1 completed

  Escalations: 0 new, 0 open
  Convoys: 2 active, 1 completed
```

### `gt peek` -- View Recent Session Output

Peek at the most recent output from an agent session without attaching to it.

```bash
# Peek at the Mayor's recent output
gt peek mayor

# Peek at a specific polecat
gt peek polecat:toast --rig myproject

# Peek at the Witness
gt peek witness --rig myproject

# Show more lines
gt peek mayor --lines 100
```

:::note

`gt peek` is read-only. To interact with an agent, use `gt mayor attach` or `gt polecat attach`.

:::

---

## Activity & Audit

### `gt activity` -- Event System

The activity system provides structured event emission and querying. Every significant action in Gas Town emits an activity event.

```bash
# View recent activity
gt activity

# Emit a custom event
gt activity emit "deployment started" --type deploy --rig myproject

# Query events with filters
gt activity --type convoy --since 24h
gt activity --actor witness --rig myproject
```

Events are stored in `.events.jsonl` at the town level and queryable with standard JSON tools.

### `gt audit` -- Work History by Actor

Query the full work history of a specific actor (agent or human).

```bash
# Audit a specific agent
gt audit witness --rig myproject

# Audit a polecat by name
gt audit polecat:toast

# Audit all work in a time range
gt audit --since 2025-01-01 --until 2025-01-31

# Audit by bead
gt audit --bead gt-a1b2c

# JSON output
gt audit --json
```

Sample output:

```
Audit: polecat:toast (myproject)

  2025-06-15 14:23  SPAWNED   hook: gt-a1b2c "Fix login bug"
  2025-06-15 14:41  COMMIT    3 files changed, +45 -12
  2025-06-15 14:52  MR_SUBMIT gt-a1b2c -> refinery
  2025-06-15 14:52  EXIT      COMPLETED (29m elapsed)

  Total: 1 task, 1 completed, 0 escalated
  Avg duration: 29m
```

### `gt log` -- Town Activity Log

View the structured town activity log, which captures higher-level operational events.

```bash
# View the log
gt log

# Follow in real-time
gt log --follow

# Filter by severity
gt log --level error
gt log --level warn

# Filter by component
gt log --component refinery
gt log --component daemon
```

---

## System Health

### `gt doctor` -- Health Checks

Run a comprehensive diagnostic check on your Gas Town installation.

```bash
gt doctor
```

The doctor checks:

| Check | What it verifies |
|-------|------------------|
| Dependencies | `git`, `bd`, `claude`, `tmux` versions |
| Daemon | Process running, heartbeat recent |
| Agents | Mayor, Deacon sessions alive |
| Rigs | Config valid, worktrees intact |
| Beads | Database accessible, no corruption |
| Hooks | No orphaned hooks, no stale attachments |
| Processes | No zombie Claude processes |
| Disk | Worktree disk usage, temp file cleanup |
| Network | Git remotes reachable |

```bash
# Check a specific rig
gt doctor --rig myproject

# Check only a specific subsystem
gt doctor --check daemon
gt doctor --check agents
gt doctor --check beads

# JSON output for automation
gt doctor --json

# Auto-fix what it can
gt doctor --fix
```

Sample output:

```
Gas Town Doctor v1.2.0

  [PASS] Dependencies: all present
  [PASS] Daemon: running (PID 12345, heartbeat 8s ago)
  [PASS] Mayor: session active
  [PASS] Deacon: session active
  [WARN] myproject/witness: session idle 15m
  [PASS] myproject/refinery: session active, queue empty
  [PASS] docs/witness: session active
  [FAIL] docs/refinery: session not found
  [PASS] Beads: 3 databases healthy
  [WARN] 2 orphaned polecat worktrees found

Summary: 8 passed, 2 warnings, 1 failure
Run 'gt doctor --fix' to attempt automatic repairs.
```

:::warning

`gt doctor --fix` will attempt to restart failed agents, clean orphaned worktrees, and repair database issues. Review the output before running it in production.

:::

### Patrol Digests -- `gt patrol digest`

Persistent agents (Deacon, Witness, Refinery) run periodic patrol cycles. You can request a digest of recent patrol findings.

```bash
# Get the Deacon's patrol digest
gt patrol digest

# Get a specific agent's patrol digest
gt patrol digest --agent witness --rig myproject

# Request a fresh patrol cycle
gt patrol start
```

The patrol digest summarizes:

- Agent health status across all rigs
- Stale or stuck polecats detected
- Merge queue backlogs
- Unacknowledged escalations
- Resource usage anomalies

---

## Dashboard & Costs

### `gt dashboard` -- Convoy Tracking Web Dashboard

Launch a web-based dashboard for visual convoy and system tracking.

```bash
gt dashboard
```

This opens a local web server (default: `http://localhost:8420`) with:

- **Convoy overview** -- All active convoys with progress bars
- **Agent map** -- Visual hierarchy of running agents per rig
- **Merge queue** -- Real-time refinery pipeline status
- **Activity timeline** -- Scrollable event history
- **Escalation panel** -- Open and recent escalations

```bash
# Specify a custom port
gt dashboard --port 9000

# Run headless (no browser auto-open)
gt dashboard --no-open

# Stop the dashboard
gt dashboard stop
```

:::tip

The dashboard auto-refreshes. Keep it open on a second monitor for at-a-glance fleet visibility.

:::

### `gt costs` -- Claude Session Costs

Track token consumption and estimated costs across all agent sessions.

```bash
gt costs
```

```bash
# Show costs for the current day
gt costs --today

# Show costs for a specific period
gt costs --since 24h
gt costs --since 2025-06-01 --until 2025-06-15

# Break down by agent type
gt costs --by-agent

# Break down by rig
gt costs --by-rig

# JSON output
gt costs --json
```

Sample output:

```
Gas Town Costs (last 24h)

  Total tokens:  2,450,000 input / 890,000 output
  Est. cost:     $47.20

  By agent:
    Mayor:       $4.10  (8.7%)
    Deacon:      $2.80  (5.9%)
    Witnesses:   $5.40  (11.4%)
    Refineries:  $3.20  (6.8%)
    Polecats:    $31.70 (67.2%)

  By rig:
    myproject:   $32.40 (68.6%)
    docs:        $14.80 (31.4%)
```

:::warning[Cost Awareness]

Gas Town at peak usage with 10+ polecats can burn approximately $100/hour in API costs. Monitor `gt costs` regularly and see the [Cost Management](../guides/cost-management.md) guide for optimization strategies.

:::

---

## Monitoring Quick Reference

| Command | Purpose | Mode |
|---------|---------|------|
| `gt feed` | Live event stream | Real-time |
| `gt trail` | Recent activity summary | Historical |
| `gt peek <agent>` | View agent output | Snapshot |
| `gt activity` | Structured event query | Historical |
| `gt audit <actor>` | Work history by actor | Historical |
| `gt log` | Town activity log | Historical / follow |
| `gt doctor` | System health check | Diagnostic |
| `gt patrol digest` | Patrol cycle summary | Periodic |
| `gt dashboard` | Web-based visual dashboard | Real-time |
| `gt costs` | Token usage and costs | Historical |

---

## What to Watch For

Knowing what the tools show you is only half the story. Knowing which signals matter is the other half.

### Red Flags (Investigate Immediately)

| Signal | Where to See It | What It Means |
|--------|----------------|---------------|
| `STALE` events in feed | `gt feed` | An agent has stopped making progress |
| Multiple escalations from one rig | `gt escalate list` | Systemic issue in that project |
| Refinery queue growing | `gt mq status` | Merges are failing or backing up |
| `[FAIL]` in doctor output | `gt doctor` | A subsystem is down |
| Cost spike in hourly rate | `gt costs --today` | Runaway agent or infinite loop |
| No events for 10+ minutes | `gt feed` | System may have stalled silently |

### Warning Signs (Check When Convenient)

| Signal | Where to See It | What It Means |
|--------|----------------|---------------|
| Increasing polecat durations | `gt audit` | Tasks may be too large or poorly scoped |
| Frequent context handoffs | `gt trail` | Tasks are filling context windows regularly |
| Escalations acknowledged but not closed | `gt escalate list --all` | Issues being noticed but not resolved |
| Orphaned worktrees accumulating | `gt orphans` | Cleanup not running often enough |

### Healthy System Indicators

A well-running Gas Town installation shows these patterns:

- Polecats spawn, work, and complete in steady cycles
- Merge queue stays near-empty (items move through quickly)
- Escalation count stays low (under 2-3 open at a time)
- Cost rate is stable and predictable
- `gt doctor` shows all green

---

## Log Analysis Patterns

When `gt feed` and `gt trail` are not enough, dig into the raw event log for patterns.

### Finding Repeated Failures

```bash
# Count events by type in the last 24h
gt activity --since 24h --json | jq -r '.type' | sort | uniq -c | sort -rn

# Find all error events for a specific rig
gt log --level error --component myproject --since 24h

# Find agents that restarted more than twice
gt trail --since 24h | grep RESTART
```

### Tracking a Specific Bead Through the System

```bash
# See the full lifecycle of a piece of work
gt audit --bead gt-a1b2c

# Check if it is stuck somewhere
bd show gt-a1b2c
gt mq show gt-a1b2c
```

### Correlating Events Across Agents

```bash
# See what happened around a specific time
gt trail --since 2h --last 100

# Cross-reference Witness actions with polecat events
gt activity --actor witness --since 1h --json
gt activity --type polecat --since 1h --json
```

---

## Monitoring Best Practices

1. **Always keep `gt feed` running** in a dedicated terminal. It is the fastest way to notice problems.

2. **Check `gt doctor` daily.** Run it first thing when starting a work session to catch overnight issues.

3. **Review `gt costs --today`** before and after intensive work sessions to understand burn rate.

4. **Use `gt trail --since 1h`** as a quick check when returning after a break.

5. **Set up `gt dashboard`** on a second screen for visual fleet tracking during active development sessions.

6. **Review patrol digests** when the Deacon or Witnesses flag anomalies -- they contain the context you need to decide on action.

7. **Watch for silence.** A quiet feed is not always good news. If no events appear for an extended period, verify the daemon is running and agents are active.

8. **Track trends, not just snapshots.** A single stale polecat is normal. Three stale polecats in the same rig within an hour suggests a systemic problem.

## See Also

- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- The core monitoring pattern
- **[Diagnostics CLI](../cli-reference/diagnostics.md)** -- CLI tools for monitoring
- **[Witness](../agents/witness.md)** -- Per-rig monitoring agent
- **[Deacon](../agents/deacon.md)** -- Town-wide health coordinator


========================================================================
# Escalation System
# URL: /docs/operations/escalations
========================================================================

# Escalation System

Gas Town's escalation system provides priority-routed alerts that flow upward through the supervision hierarchy. When an agent encounters a problem it cannot resolve, it creates an escalation that routes to the appropriate handler based on severity. This ensures critical issues reach the human operator quickly while low-severity problems are tracked without interruption.

---

## Creating Escalations

### `gt escalate`

Create a new escalation from any context.

```bash
# Create an escalation with default severity (medium/P2)
gt escalate "Refinery merge failing on myproject - rebase conflict"

# Create with specific severity
gt escalate --severity critical "Database migration broke all tests"
gt escalate --severity high "Polecat toast stuck for 30+ minutes"
gt escalate --severity low "Flaky test in auth module"

# Attach to a bead
gt escalate --bead gt-a1b2c "Blocked: needs API key configuration"

# Attach to a rig
gt escalate --rig myproject "All polecats failing to spawn"
```

:::note

Agents create escalations automatically when they detect problems they cannot resolve. Manual `gt escalate` is for cases where you (or the Mayor) need to flag something explicitly.

:::

---

## Severity Levels

Gas Town uses four severity levels, aligned with standard incident management practices:

| Level | Code | Description | Response Time |
|-------|------|-------------|---------------|
| **Critical** | P0 | System down, data loss risk, all work blocked | Immediate |
| **High** | P1 | Major functionality broken, significant work blocked | < 1 hour |
| **Medium** | P2 | Degraded operation, workaround available | < 4 hours |
| **Low** | P3 | Minor issue, cosmetic, or improvement suggestion | Best effort |

### When Each Severity is Used

#### P0 -- Critical

- The daemon is unresponsive and agents are not receiving heartbeats
- Database corruption detected in beads
- All polecats in a rig are failing simultaneously
- Merge to main produced broken code that blocks all downstream work
- Data loss or irreversible state corruption

#### P1 -- High

- A Witness or Refinery is down and not auto-recovering
- Multiple polecats stuck on the same blocker
- Merge queue backed up with conflicts that require human judgment
- Agent repeatedly crashing on restart
- External service dependency is down (GitHub API, etc.)

#### P2 -- Medium

- A single polecat is stuck but others are progressing
- Flaky tests causing intermittent merge failures
- Non-critical agent running slowly or consuming excess resources
- Configuration issue affecting one rig but not others

#### P3 -- Low

- Cosmetic issues in generated code
- Optimization suggestions from agents
- Stale worktrees that need cleanup
- Documentation gaps noticed during work

---

## Routing Configuration

Escalation routing is configured in `settings/escalation.json` at the town level.

```json
{
  "routing": {
    "critical": {
      "channels": ["bead", "mail:mayor", "email:human", "sms:human"],
      "auto_escalate_after": "15m",
      "max_re_escalations": 3
    },
    "high": {
      "channels": ["bead", "mail:mayor", "email:human"],
      "auto_escalate_after": "1h",
      "max_re_escalations": 2
    },
    "medium": {
      "channels": ["bead", "mail:mayor"],
      "auto_escalate_after": "4h",
      "max_re_escalations": 2
    },
    "low": {
      "channels": ["bead"],
      "auto_escalate_after": null,
      "max_re_escalations": 0
    }
  },
  "contacts": {
    "human": {
      "email": "you@example.com",
      "sms": "+1234567890"
    }
  },
  "quiet_hours": {
    "enabled": false,
    "start": "22:00",
    "end": "08:00",
    "timezone": "America/Los_Angeles",
    "override_for": ["critical"]
  }
}
```

### Routing Paths by Severity

```mermaid
graph LR
    subgraph P0 ["P0 - Critical"]
        C0[Bead] --> C1[Mail: Mayor]
        C1 --> C2[Email: Human]
        C2 --> C3[SMS: Human]
    end

    subgraph P1 ["P1 - High"]
        H0[Bead] --> H1[Mail: Mayor]
        H1 --> H2[Email: Human]
    end

    subgraph P2 ["P2 - Medium"]
        M0[Bead] --> M1[Mail: Mayor]
    end

    subgraph P3 ["P3 - Low"]
        L0[Bead]
    end
```

Each channel in the routing path is attempted in order. If the escalation is not acknowledged within the `auto_escalate_after` window, it re-escalates through the same channels.

:::tip[Customize Routing]

You can add custom channels like Discord or Slack webhooks by adding entries to the `channels` array and configuring the integration in `settings/integrations.json`.

:::

---

## Managing Escalations

### `gt escalate list`

View all open escalations.

```bash
# List all open escalations
gt escalate list

# Filter by severity
gt escalate list --severity critical
gt escalate list --severity high

# Filter by rig
gt escalate list --rig myproject

# Include closed escalations
gt escalate list --all

# JSON output
gt escalate list --json
```

Sample output:

```
Open Escalations (3):

  ESC-001  P0  CRITICAL  "Database migration broke all tests"
           Rig: myproject  Bead: gt-a1b2c  Age: 12m  ACK: no
           Route: bead -> mail:mayor -> email:human (pending)

  ESC-002  P1  HIGH      "Polecat toast stuck 35m"
           Rig: myproject  Bead: gt-d3e4f  Age: 35m  ACK: yes (mayor)
           Route: bead -> mail:mayor (acknowledged)

  ESC-003  P2  MEDIUM    "Flaky test in auth module"
           Rig: docs  Age: 2h  ACK: no
           Route: bead -> mail:mayor (pending)
```

### `gt escalate ack`

Acknowledge an escalation. This stops re-escalation and signals that someone is handling it.

```bash
# Acknowledge by ID
gt escalate ack ESC-001

# Acknowledge with a note
gt escalate ack ESC-001 --note "Looking into the migration issue"

# Acknowledge all open escalations
gt escalate ack --all
```

:::warning

Acknowledging an escalation does **not** resolve it. It only signals that the issue is being actively worked on. Use `gt escalate close` when the issue is resolved.

:::

### `gt escalate close`

Close a resolved escalation.

```bash
# Close by ID
gt escalate close ESC-001

# Close with resolution note
gt escalate close ESC-001 --note "Fixed: migration script had wrong column type"

# Close with a linked commit
gt escalate close ESC-001 --commit abc1234
```

### `gt escalate stale`

Find escalations that have gone unacknowledged past their threshold.

```bash
gt escalate stale
```

Sample output:

```
Stale Escalations (1):

  ESC-003  P2  MEDIUM  "Flaky test in auth module"
           Age: 4h 12m (threshold: 4h)
           Re-escalations: 1 of 2
           Next re-escalation in: 3h 48m
```

---

## Auto Re-Escalation

When an escalation is not acknowledged within its severity's `auto_escalate_after` window, Gas Town automatically re-sends notifications through the configured channels.

### How It Works

1. An escalation is created and notifications are sent through the routing channels
2. A timer starts based on the severity's `auto_escalate_after` value
3. If the timer expires without an `ack`, the escalation is re-sent
4. This repeats up to `max_re_escalations` times (default: 2 for P1/P2)
5. After max re-escalations, the escalation is marked as `stale` but remains open

### Default Thresholds

| Severity | Auto Re-Escalate After | Max Re-Escalations | Total Window |
|----------|----------------------|---------------------|--------------|
| P0 Critical | 15 minutes | 3 | ~1 hour |
| P1 High | 1 hour | 2 | ~3 hours |
| P2 Medium | 4 hours | 2 | ~12 hours |
| P3 Low | Never | 0 | N/A |

:::note

The 4-hour default threshold for medium-severity escalations means an unacknowledged P2 will re-escalate twice, at the 4h and 8h marks. After the second re-escalation (at 8h), it becomes stale.

:::

### Re-Escalation Flow

```mermaid
sequenceDiagram
    participant A as Agent
    participant S as Escalation System
    participant M as Mayor
    participant H as Human

    A->>S: Create escalation (P1)
    S->>M: Mail notification
    S->>H: Email notification

    Note over S: 1h timer starts

    alt Acknowledged
        M->>S: gt escalate ack
        Note over S: Timer cancelled
    else Timer expires (no ack)
        S->>M: Re-escalation #1
        S->>H: Re-escalation #1 email
        Note over S: 1h timer restarts
    else Timer expires again
        S->>M: Re-escalation #2 (final)
        S->>H: Re-escalation #2 email
        Note over S: Marked as stale
    end
```

---

## Escalation Lifecycle

```mermaid
stateDiagram-v2
    [*] --> Open: gt escalate
    Open --> Acknowledged: gt escalate ack
    Open --> ReEscalated: auto (threshold hit)
    ReEscalated --> Acknowledged: gt escalate ack
    ReEscalated --> Stale: max re-escalations reached
    Acknowledged --> Closed: gt escalate close
    Stale --> Acknowledged: gt escalate ack
    Stale --> Closed: gt escalate close
    Closed --> [*]
```

---

## Common Escalation Scenarios

These are real-world scenarios you will encounter and how to handle them.

### Scenario: Flaky Test Causing Repeated Merge Failures

A test intermittently fails in the Refinery, causing merges to bounce. Polecats' work is correct but the gate keeps rejecting it.

**What you will see:**

- Multiple P2 escalations from the Refinery about merge failures
- The same test name appearing in different escalation messages
- Merge queue growing as items back up behind the flaky test

**What to do:**

```bash
# Acknowledge the escalations
gt escalate ack ESC-001 --note "Flaky test identified: test_auth_timeout"

# Fix the flaky test (or skip it temporarily)
# Work from a crew workspace to fix main directly
cd ~/gt/myproject/crew/yourname
# Fix the test, commit, push to main

# Close the escalation
gt escalate close ESC-001 --note "Fixed flaky test: added retry logic to test_auth_timeout"
```

### Scenario: Polecat Stuck in a Loop

A polecat keeps trying the same approach to a problem, failing, and trying again. The Witness has nudged it but it cannot break out.

**What you will see:**

- P1 escalation from the Witness about a stale polecat
- `gt peek` shows the polecat repeating similar actions

**What to do:**

```bash
# Acknowledge
gt escalate ack ESC-002 --note "Reviewing stuck polecat"

# Stop the polecat and release its work
gt polecat stop toast --rig myproject
gt release gt-a1b2c

# Add more detail to the bead before re-slinging
bd update gt-a1b2c --notes "Previous attempt failed because <reason>. Try <alternative approach>."

# Re-sling to a fresh polecat
gt sling gt-a1b2c myproject

gt escalate close ESC-002 --note "Respawned with updated guidance"
```

### Scenario: Cost Spike During Off-Hours

You return to find an escalation about token costs exceeding the daily budget.

**What you will see:**

- P1 or P2 escalation with cost details
- `gt costs` showing a spike

**What to do:**

```bash
# Check what caused the spike
gt costs --since 12h --by-agent
gt trail --since 12h

# If a runaway agent, stop it
gt polecat stop <name> --rig myproject

# Acknowledge and close
gt escalate ack ESC-003 --note "Cost spike from runaway polecat"
gt escalate close ESC-003 --note "Stopped runaway polecat, reviewing task scoping"
```

---

## Tuning Escalation Thresholds

The default thresholds work well for most setups, but you may want to adjust them based on your operational pattern.

### When to Tighten Thresholds (Shorter Timers)

- You are actively monitoring and want faster alerts
- The project is in a critical phase (launch, migration)
- You have multiple rigs and need early warning

```json
{
  "routing": {
    "high": {
      "auto_escalate_after": "30m",
      "max_re_escalations": 3
    }
  }
}
```

### When to Loosen Thresholds (Longer Timers)

- Running overnight unattended (combine with quiet hours)
- Low-priority project where delays are acceptable
- Reducing alert fatigue from a noisy rig

```json
{
  "routing": {
    "medium": {
      "auto_escalate_after": "8h",
      "max_re_escalations": 1
    }
  }
}
```

### Per-Rig Overrides

You can override escalation settings at the rig level by placing an `escalation.json` in `<rig>/settings/`. Rig-level settings merge with (and override) town-level defaults.

---

## Escalation Best Practices

1. **Acknowledge promptly.** Even if you cannot fix the issue immediately, acknowledging stops the re-escalation timer and signals to the system that a human is aware.

2. **Use severity levels correctly.** P0 should be reserved for genuine emergencies. Over-using P0 leads to alert fatigue and reduces the signal value of critical escalations.

3. **Configure quiet hours** if you run Gas Town overnight. Critical escalations override quiet hours by default, but you can control this in the routing configuration.

4. **Review `gt escalate stale` daily.** Stale escalations represent issues that fell through the cracks -- they need attention.

5. **Close with notes.** Future-you will appreciate knowing how past escalations were resolved. Use `--note` and `--commit` flags to leave a trail.

6. **Let agents escalate.** Do not suppress agent escalations. If an agent is escalating too frequently, the root cause is usually a systemic issue (flaky tests, misconfigured rig, etc.) rather than the agent being too aggressive.

## See Also

- **[Agent Hierarchy](../architecture/agent-hierarchy.md)** -- The supervision tree that defines escalation paths
- **[Deacon](../agents/deacon.md)** -- Primary escalation handler
- **[Witness](../agents/witness.md)** -- First-level escalation for polecat issues


========================================================================
# Troubleshooting
# URL: /docs/operations/troubleshooting
========================================================================

# Troubleshooting

This guide covers common Gas Town problems, their diagnosis, and resolution. Start with `gt doctor` for automated diagnostics, then consult the specific sections below for detailed troubleshooting steps.

---

## First Steps

Before diving into specific issues, always run these commands:

```bash
# Automated health check
gt doctor

# Check what's running
gt rig list

# Check recent activity for clues
gt trail --since 1h

# Check for open escalations
gt escalate list
```

---

## Agent Loses Connection

**Symptom:** An agent session is running but not responding to mail or nudges. The agent appears to be stuck in an inactive state.

**Diagnosis:**

```bash
# Check if the session is alive
gt peek <agent>

# Check the agent's hook state
gt hook --agent <agent>

# Check convoy status for the agent's work
gt convoy list
```

**Solutions:**

1. **Verify hooks are intact.** If the agent's hook was corrupted, the agent may not know what to do.

    ```bash
    # Check hooks for the agent
    gt hook --agent witness --rig myproject

    # If hooks look wrong, re-prime the agent
    gt prime --agent witness --rig myproject
    ```

2. **Check convoy status.** If the convoy tracking a piece of work is in a bad state, the agent may be waiting on something that will never arrive.

    ```bash
    gt convoy list
    gt convoy show <convoy-id>
    ```

3. **Restart the agent.** If hooks and state look correct, a session restart often resolves transient issues.

    ```bash
    gt witness restart --rig myproject
    # Or for a fresh start:
    gt witness restart --rig myproject --fresh
    ```

:::tip

After restarting an agent, it automatically runs `gt prime` to reload context. All hook-attached work persists -- the agent will resume where it left off.

:::

---

## Convoy Stuck

**Symptom:** A convoy shows `ACTIVE` but no progress is being made. Issues within the convoy are not advancing.

**Diagnosis:**

```bash
# Check convoy details
gt convoy show <convoy-id>

# Check bead states for the convoy's issues
bd list --convoy <convoy-id>

# Check for stranded convoys
gt convoy stranded
```

**Solutions:**

1. **Review bead states.** Look for beads stuck in `hooked` or `in_progress` that are not being worked on.

    ```bash
    # List beads with their states
    bd list --status in_progress
    bd list --status hooked
    ```

2. **Manually advance stuck beads.** If a bead is hooked but no polecat is running it:

    ```bash
    # Release the bead back to pending
    gt release gt-a1b2c

    # Re-sling to a rig
    gt sling gt-a1b2c myproject
    ```

3. **Check for blocked dependencies.** If beads have dependency links, one stuck bead can block the entire chain.

    ```bash
    bd show gt-a1b2c
    # Look for "blocked_by" or "depends_on" fields
    ```

4. **Check the merge queue.** Completed work may be stuck in the refinery.

    ```bash
    gt mq list --rig myproject
    gt mq status --rig myproject
    ```

---

## Mayor Not Responding

**Symptom:** The Mayor session is alive but not processing mail or giving instructions.

**Diagnosis:**

```bash
# Check if the session is active
gt peek mayor

# Check the Mayor's mailbox
gt mail inbox --agent mayor

# Check for recent Mayor activity
gt audit mayor --since 1h
```

**Solutions:**

1. **Context recovery with `gt prime`.** The Mayor may have lost context after compaction or a long idle period.

    ```bash
    gt mayor attach
    # Then inside the session:
    # Run gt prime
    ```

    Or from outside:

    ```bash
    gt nudge mayor "Run gt prime to recover context"
    ```

2. **Restart the Mayor.** If priming does not help:

    ```bash
    gt mayor restart
    ```

3. **Check for a blocking escalation.** The Mayor may be waiting for human input on a critical escalation.

    ```bash
    gt escalate list --severity critical
    gt escalate list --severity high
    ```

:::warning

If the Mayor is unresponsive and you have urgent work, you can bypass it by directly slinging work to rigs: `gt sling gt-a1b2c myproject`. The Mayor is not required for individual task assignment.

:::

---

## Stale Polecats

**Symptom:** Polecats that have been running for an unusually long time without producing output or completing their work.

**Diagnosis:**

```bash
# List stale polecats
gt polecat stale

# Check individual polecat status
gt polecat list --rig myproject
gt peek polecat:toast --rig myproject
```

**Solutions:**

1. **Let the Witness handle it.** The Witness detects stale polecats during its patrol cycle and will nudge them first, then escalate if they remain stuck.

    ```bash
    # Check if the Witness has already acted
    gt trail --agent witness --rig myproject
    ```

2. **Manually nudge the polecat.**

    ```bash
    gt nudge polecat:toast --rig myproject "Check your progress - you appear to be stalled"
    ```

3. **Terminate and respawn.** If the polecat is truly stuck:

    ```bash
    # Stop the polecat
    gt polecat stop toast --rig myproject

    # Release its work
    gt release gt-a1b2c

    # Re-sling to spawn a fresh polecat
    gt sling gt-a1b2c myproject
    ```

---

## Orphaned Processes

**Symptom:** Resources consuming disk or compute that are not connected to any active agent or convoy.

### Orphaned Worktrees

```bash
# Find orphaned worktrees
gt orphans

# Clean them up
gt cleanup
```

`gt orphans` finds:

- Polecat directories with no running session
- Worktrees not attached to any active bead
- Stale temporary directories from crashed agents

### Zombie Claude Processes

```bash
# Scan for zombie Claude processes
gt deacon zombie-scan

# Or manually check
ps aux | grep claude | grep -v grep
```

If zombie processes are found:

```bash
# Let the Deacon clean them up
gt deacon zombie-scan --cleanup

# Or manually kill specific processes
kill <PID>
```

### Orphaned Commits

```bash
# Find unreachable commits (lost work from crashed polecats)
gt orphans --commits

# Recover work from an orphaned commit
gt orphans --recover <commit-hash>
```

:::tip[Lost Work Recovery]

`gt orphans --commits` uses `git fsck` under the hood to find unreachable commits. If a polecat crashed before pushing, its work may still be recoverable from the local git object store.

:::

---

## Merge Conflicts

**Symptom:** The Refinery reports merge conflicts that prevent code from landing on main.

**Diagnosis:**

```bash
# Check merge queue status
gt mq list --rig myproject
gt mq status --rig myproject

# Check for conflict details
gt mq show <mr-id>
```

**Solutions:**

1. **Let the Refinery handle it.** The Refinery's default behavior on conflict is to spawn a fresh polecat to resolve the conflict. This works for straightforward conflicts.

2. **Manual resolution.** For complex conflicts that polecats cannot resolve:

    ```bash
    # Attach to the Refinery
    gt refinery attach --rig myproject

    # Or work from a crew workspace
    cd ~/gt/myproject/crew/yourname
    git fetch origin
    git merge origin/main
    # Resolve conflicts manually
    git push
    ```

3. **Skip and retry.** If one MR is blocking the queue:

    ```bash
    # Skip the problematic MR
    gt mq skip <mr-id>

    # The bead goes back to pending for reassignment
    gt sling gt-a1b2c myproject
    ```

:::note

The Refinery always rebases onto the latest `main` before merging. Conflicts are most common when multiple polecats modify the same files. Consider assigning related work to a single polecat or serializing via convoy dependencies.

:::

---

## Daemon Issues

### Daemon Not Starting

```bash
# Check daemon status
gt daemon status

# Check for port conflicts
gt daemon start --verbose

# Check daemon logs
gt daemon logs --level error
```

### Heartbeat Failures

If the Deacon is not receiving heartbeats:

```bash
# Verify the daemon is running
gt daemon status

# Check for network issues between daemon and Deacon
gt daemon logs --follow

# Restart the daemon
gt daemon stop && gt daemon start
```

---

## Context Window Filling

**Symptom:** An agent becomes sluggish, loses track of its work, starts repeating itself, or produces incoherent output. Often preceded by the agent running for an extended period on a large task.

**Diagnosis:**

```bash
# Check how long the agent has been running
gt audit polecat:toast --rig myproject

# Look for signs of context pressure in agent output
gt peek polecat:toast --rig myproject --lines 50
```

**Solutions:**

1. **For polecats: use `gt handoff`.** The polecat should cycle itself to a fresh session with context notes before the window fills completely.

    ```bash
    # From inside the polecat session:
    gt handoff -s "Context filling, continuing work" -m "Issue: gt-a1b2c
    Progress: implemented X, Y remains
    Next step: finish Y and run tests"
    ```

2. **For persistent agents (Witness, Mayor, Deacon): trigger a context reset.**

    ```bash
    # Nudge the agent to prime
    gt nudge witness --rig myproject "Run gt prime to reset context"

    # Or restart with fresh context
    gt witness restart --rig myproject --fresh
    ```

3. **Prevent the issue.** Large tasks should be broken into smaller beads. If a polecat consistently fills context on a task type, the task decomposition needs improvement.

:::note

Persistent agents (Mayor, Deacon, Witness) experience automatic compaction but may lose nuance. If an agent seems confused after compaction, `gt prime` reloads the full role context.

:::

---

## Agent Boot Failures

**Symptom:** An agent fails to start, or starts but immediately exits or becomes unresponsive. The `gt start` command completes but `gt doctor` shows the agent is not running.

**Diagnosis:**

```bash
# Check if the session exists in tmux
gt peek <agent> --rig myproject

# Check daemon logs for startup errors
gt daemon logs --level error --since 10m

# Check if the tmux session was created
tmux list-sessions
```

**Solutions:**

1. **Missing dependencies.** The agent's Claude session may fail if required tools are not installed.

    ```bash
    gt doctor --check dependencies
    ```

2. **Stale tmux sessions.** A previous session may still hold the slot.

    ```bash
    # Kill stale tmux sessions
    tmux kill-session -t <session-name>

    # Restart the agent
    gt witness start --rig myproject
    ```

3. **Configuration errors.** Check rig configuration for invalid settings.

    ```bash
    # Validate rig config
    gt rig show myproject
    ```

4. **Disk space.** Claude sessions need space for logs and context.

    ```bash
    df -h ~/gt/
    # Clean up if needed
    gt cleanup
    ```

---

## Git Worktree Issues

**Symptom:** A polecat reports git errors, has a dirty working tree it did not expect, or cannot commit/push its changes.

**Diagnosis:**

```bash
# Check worktree state from outside
cd ~/gt/myproject/polecats/<name>/myproject
git status
git log --oneline -5
```

**Solutions:**

1. **Dirty working tree from a previous crash.** If a polecat crashed mid-work, its worktree may have uncommitted changes.

    ```bash
    # If the changes are salvageable, commit them
    cd ~/gt/myproject/polecats/<name>/myproject
    git add -A
    git commit -m "chore: recover work from crashed polecat session"
    git push

    # Then stop and respawn
    gt polecat stop <name> --rig myproject
    gt sling <bead-id> myproject
    ```

2. **Detached HEAD.** The worktree may have ended up in a detached HEAD state.

    ```bash
    # Check the current state
    git branch --show-current  # Empty output means detached HEAD

    # Recover by creating a branch from the current position
    git checkout -b recovery/<bead-id>
    git push origin recovery/<bead-id>
    ```

3. **Worktree pointing to deleted branch.** If the branch was cleaned up but the worktree remains:

    ```bash
    # Remove the broken worktree
    git worktree remove ~/gt/myproject/polecats/<name>/myproject --force

    # Let gt recreate it on next spawn
    gt sling <bead-id> myproject
    ```

---

## Beads Database Issues

**Symptom:** `bd` commands fail with lock errors, return unexpected results, or report corruption. Agents may be unable to create, update, or close beads.

**Diagnosis:**

```bash
# Run the beads doctor
bd doctor

# Check for lock files
ls -la ~/gt/myproject/.beads/*.lock 2>/dev/null
```

**Solutions:**

1. **Database locked.** Concurrent writes from multiple agents can cause locking. Usually resolves itself.

    ```bash
    # Wait a few seconds and retry
    bd list --status=open

    # If persistent, check for zombie bd processes
    ps aux | grep "bd " | grep -v grep

    # Kill zombies if found
    kill <PID>
    ```

2. **Database corruption.** Rare but possible after unclean shutdowns.

    ```bash
    # Run repair
    bd doctor --fix

    # If repair fails, the JSONL export is the backup
    # Check for the last good export
    ls -la ~/gt/myproject/.beads/*.jsonl
    ```

3. **Sync issues between agents.** If agents have divergent views of bead state:

    ```bash
    # Force a flush to JSONL
    bd sync --flush-only

    # Each agent should re-read on next bd command
    ```

---

## Polecat Self-Clean Failures

**Symptom:** A polecat runs `gt done` but the command fails. The polecat may be left in a limbo state -- finished with work but not properly cleaned up.

**Diagnosis:**

```bash
# Check if the polecat's work was pushed
cd ~/gt/myproject/polecats/<name>/myproject
git status
git log --oneline origin/main..HEAD
```

**Solutions:**

1. **Uncommitted changes.** `gt done` requires a clean git state.

    ```bash
    # Commit remaining changes
    git add <files>
    git commit -m "fix: remaining changes before gt done"
    git push

    # Retry
    gt done
    ```

2. **Push failures.** Network issues or remote conflicts can prevent push.

    ```bash
    # Check remote connectivity
    git remote -v
    git fetch origin

    # If behind remote, rebase
    git rebase origin/main
    git push

    gt done
    ```

3. **If `gt done` keeps failing:** Escalate to the Witness with context about what failed.

    ```bash
    gt mail send myproject/witness -s "HELP: gt done failing" -m "Polecat: <name>
    Issue: <bead-id>
    Error: <error message>
    Git state: <clean/dirty>
    Branch pushed: <yes/no>"
    ```

---

## Mail Delivery Problems

**Symptom:** Messages sent via `gt mail send` are not appearing in the recipient's inbox, or agents are not responding to mail they should have received.

**Diagnosis:**

```bash
# Check outbox
gt mail sent

# Check the recipient's inbox directly
gt mail inbox --agent <recipient>

# Verify the mail address format
gt mail send <rig>/<agent> -s "test" -m "ping"
```

**Solutions:**

1. **Wrong address format.** Mail addresses follow the pattern `<rig>/<agent>` or `mayor/` for town-level agents.

2. **Agent not checking mail.** Persistent agents check mail during patrol cycles. If an agent is stuck, it may not be polling.

    ```bash
    # Nudge the agent to check mail
    gt nudge <agent> --rig myproject "Check your inbox"
    ```

3. **Mail queue corruption.** Rare, but can happen after crashes.

    ```bash
    # Check mail queue health
    gt doctor --check agents
    ```

---

## Common Error Messages

| Error | Cause | Fix |
|-------|-------|-----|
| `no rig found` | Command run outside a rig context | Use `--rig <name>` or `cd` into a rig directory |
| `agent session not found` | Agent is not running | Start the agent: `gt <agent> start` |
| `hook already attached` | Bead is already hooked to another agent | Release first: `gt release <bead-id>` |
| `merge queue full` | Refinery is backed up | Check `gt mq list`, clear stuck MRs |
| `daemon not running` | Daemon process has stopped | Run `gt daemon start` |
| `beads database locked` | Concurrent write conflict | Wait and retry; check for zombie `bd` processes |
| `worktree already exists` | Stale worktree from previous run | Clean up: `gt cleanup` |
| `convoy not found` | Invalid convoy ID or convoy was auto-cleaned | Check `gt convoy list --all` for closed convoys |
| `context window exceeded` | Agent session ran too long | Use `gt handoff` to cycle to a fresh session |
| `git push rejected` | Remote has diverged from local | Run `git fetch && git rebase origin/main`, then push |
| `molecule step not found` | Bead ID typo or step was already closed | Check `bd show <molecule-id>` for step list |
| `permission denied` | File or directory owned by another agent | Check `ls -la` ownership; may need `gt cleanup` |

---

## Diagnostic Commands Summary

| Command | Purpose |
|---------|---------|
| `gt doctor` | Comprehensive health check |
| `gt doctor --fix` | Auto-repair known issues |
| `gt orphans` | Find disconnected resources |
| `gt cleanup` | Remove stale resources |
| `gt polecat stale` | List stuck polecats |
| `gt deacon zombie-scan` | Find zombie Claude processes |
| `gt escalate stale` | Find unacknowledged escalations |
| `gt convoy stranded` | Find convoys with unassigned work |
| `gt mq status` | Check merge queue health |
| `gt daemon status` | Verify daemon is running |
| `gt trail --since 1h` | Recent activity for diagnosis |
| `gt peek <agent>` | View agent session output |
| `bd doctor` | Check beads database health |
| `bd doctor --fix` | Attempt beads database repair |

---

## Decision Tree: Which Section to Read

Use this to quickly find the right troubleshooting section:

```
Is anything running at all?
├── No → Daemon Issues / Agent Boot Failures
└── Yes
    ├── Is a specific agent unresponsive?
    │   ├── Mayor → Mayor Not Responding
    │   ├── Polecat → Stale Polecats
    │   └── Witness/Refinery → Agent Loses Connection
    ├── Is work not progressing?
    │   ├── Convoy shows ACTIVE → Convoy Stuck
    │   ├── Merge queue backed up → Merge Conflicts
    │   └── Polecat can't finish → Polecat Self-Clean Failures
    ├── Are bd commands failing?
    │   └── Beads Database Issues
    ├── Is an agent confused or repeating itself?
    │   └── Context Window Filling
    └── Are there leftover resources?
        └── Orphaned Processes
```

---

## When to Reboot

If multiple systems are failing simultaneously and individual fixes are not resolving the problem, a full restart is often the fastest path to recovery:

```bash
# Nuclear option: stop everything and start fresh
gt shutdown --all
gt start --all
```

This preserves all persistent state (beads, config, hooks) but gives every agent a clean session. Agents will automatically pick up their hooked work on restart.

:::warning

Before running `gt shutdown --all`, check if any polecats have uncommitted work: `gt polecat list --rig myproject`. Work that has been committed to the polecat's worktree branch is safe. Uncommitted changes in an active session will be lost.


:::

## See Also

- **[Monitoring](monitoring.md)** -- Proactive monitoring to prevent issues
- **[Diagnostics CLI](../cli-reference/diagnostics.md)** -- Diagnostic commands for debugging
- **[Lifecycle](lifecycle.md)** -- Agent lifecycle management


========================================================================
# Plugins
# URL: /docs/operations/plugins
========================================================================

# Plugins

Gas Town's plugin system extends the platform with custom automation, gates, and scheduled tasks. Plugins can operate at the town level (affecting all rigs) or at the rig level (scoped to a single project).

---

## Plugin Locations

### Town-Level Plugins

Town-level plugins live in `~/gt/plugins/` and are available across all rigs.

```
~/gt/plugins/
├── eslint-gate/
│   ├── plugin.json
│   └── run.sh
├── deploy-notify/
│   ├── plugin.json
│   └── run.sh
└── cost-alert/
    ├── plugin.json
    └── run.sh
```

### Rig-Level Plugins

Rig-level plugins live in `<rig>/plugins/` and only affect that specific project.

```
~/gt/myproject/plugins/
├── integration-tests/
│   ├── plugin.json
│   └── run.sh
└── coverage-check/
    ├── plugin.json
    └── run.sh
```

:::note

When a plugin exists at both levels with the same name, the rig-level plugin takes precedence for that rig.

:::

---

## Gate Types

Gates are the primary mechanism plugins use to control workflow execution. A gate blocks progress until its condition is met.

### Cooldown Gate

Enforces a minimum time delay between actions.

```json
{
  "name": "deploy-cooldown",
  "type": "cooldown",
  "config": {
    "duration": "10m",
    "scope": "rig"
  }
}
```

Use cases:

- Prevent rapid-fire deployments
- Rate-limit API calls
- Enforce review periods between merges

### Cron Gate

Opens at scheduled times based on a cron expression.

```json
{
  "name": "nightly-tests",
  "type": "cron",
  "config": {
    "schedule": "0 2 * * *",
    "timezone": "America/Los_Angeles"
  }
}
```

Use cases:

- Nightly test suite execution
- Scheduled deployments
- Periodic cleanup tasks

### Condition Gate

Opens when a boolean condition evaluates to true.

```json
{
  "name": "tests-pass",
  "type": "condition",
  "config": {
    "command": "npm test",
    "success_exit_code": 0,
    "retry_interval": "5m",
    "max_retries": 3
  }
}
```

Use cases:

- Block merges until tests pass
- Wait for a service to be healthy
- Check external API availability

### Event Gate

Opens in response to a specific event in the activity stream.

```json
{
  "name": "convoy-complete",
  "type": "event",
  "config": {
    "event_type": "convoy.completed",
    "filter": {
      "convoy_id": "$CONVOY_ID"
    }
  }
}
```

Use cases:

- Trigger deployment after all convoy work merges
- Send notifications on escalation events
- Chain workflows based on activity events

### Manual Gate

Requires explicit human approval to open.

```json
{
  "name": "production-deploy",
  "type": "manual",
  "config": {
    "approvers": ["human"],
    "prompt": "Approve production deployment?",
    "notify_channels": ["email:human"]
  }
}
```

Use cases:

- Production deployment approval
- Sensitive data access authorization
- High-risk operation confirmation

---

## Plugin Structure

Every plugin requires a `plugin.json` manifest and an executable entry point.

### plugin.json

```json
{
  "name": "integration-tests",
  "version": "1.0.0",
  "description": "Run integration test suite as a merge gate",
  "author": "yourname",
  "type": "gate",
  "gate_type": "condition",
  "trigger": "pre-merge",
  "config": {
    "command": "./run.sh",
    "timeout": "10m",
    "success_exit_code": 0,
    "retry_on_failure": true,
    "max_retries": 2
  },
  "inputs": {
    "rig": "$RIG_NAME",
    "branch": "$BRANCH_NAME",
    "bead": "$BEAD_ID"
  },
  "outputs": {
    "report": "test-results.json"
  }
}
```

### Manifest Fields

| Field | Required | Description |
|-------|----------|-------------|
| `name` | Yes | Unique plugin identifier |
| `version` | Yes | Semantic version |
| `description` | Yes | Human-readable description |
| `author` | No | Plugin author |
| `type` | Yes | Plugin type: `gate`, `action`, `hook`, `schedule` |
| `gate_type` | If gate | One of: `cooldown`, `cron`, `condition`, `event`, `manual` |
| `trigger` | Yes | When to execute: `pre-merge`, `post-merge`, `on-spawn`, `on-done`, `manual` |
| `config` | Yes | Plugin-specific configuration |
| `inputs` | No | Variables passed to the plugin (supports `$VAR` substitution) |
| `outputs` | No | Expected output files or values |

### Entry Point (run.sh)

The entry point receives inputs as environment variables and should exit with the appropriate code:

```bash
#!/bin/bash
# run.sh - Integration test plugin

echo "Running integration tests for rig: $RIG_NAME"
echo "Branch: $BRANCH_NAME"
echo "Bead: $BEAD_ID"

cd "$RIG_PATH/refinery/rig"

# Run tests
npm run test:integration 2>&1 | tee test-output.log
EXIT_CODE=${PIPESTATUS[0]}

# Generate report
if [ $EXIT_CODE -eq 0 ]; then
    echo '{"status": "passed", "tests": "all"}' > test-results.json
else
    echo '{"status": "failed", "log": "test-output.log"}' > test-results.json
fi

exit $EXIT_CODE
```

---

## Plugin Commands

### `gt plugin list`

List all available plugins.

```bash
# List all plugins
gt plugin list

# List town-level plugins only
gt plugin list --scope town

# List rig-level plugins
gt plugin list --rig myproject

# JSON output
gt plugin list --json
```

Sample output:

```
Town plugins:
  eslint-gate       gate/condition   pre-merge   v1.0.0
  deploy-notify     action           post-merge  v1.2.0
  cost-alert        schedule         cron        v0.9.0

Rig: myproject
  integration-tests gate/condition   pre-merge   v1.0.0
  coverage-check    gate/condition   pre-merge   v1.1.0
```

### `gt plugin show`

View details of a specific plugin.

```bash
gt plugin show integration-tests
gt plugin show integration-tests --rig myproject
```

### `gt plugin run`

Manually trigger a plugin execution.

```bash
# Run a plugin
gt plugin run integration-tests --rig myproject

# Run with custom inputs
gt plugin run integration-tests --rig myproject --input branch=feature/auth

# Dry run (show what would happen)
gt plugin run integration-tests --dry-run
```

### `gt plugin history`

View the execution history of a plugin.

```bash
# View recent executions
gt plugin history integration-tests

# Filter by status
gt plugin history integration-tests --status failed

# Filter by time
gt plugin history integration-tests --since 7d

# JSON output
gt plugin history --json
```

Sample output:

```
Plugin: integration-tests (last 5 runs)

  2025-06-15 14:23  PASSED  branch: fix/login    12.4s
  2025-06-15 13:01  FAILED  branch: feat/auth     8.2s  (exit code 1)
  2025-06-15 12:45  PASSED  branch: fix/typo      6.1s
  2025-06-14 22:00  PASSED  branch: feat/api     15.3s
  2025-06-14 18:30  PASSED  branch: fix/css        4.8s
```

---

## Creating Custom Plugins

### Step 1: Create the Plugin Directory

```bash
# Town-level plugin
mkdir -p ~/gt/plugins/my-plugin

# Or rig-level plugin
mkdir -p ~/gt/myproject/plugins/my-plugin
```

### Step 2: Write the Manifest

Create `plugin.json`:

```json
{
  "name": "my-plugin",
  "version": "1.0.0",
  "description": "Custom quality gate that checks code coverage",
  "type": "gate",
  "gate_type": "condition",
  "trigger": "pre-merge",
  "config": {
    "command": "./run.sh",
    "timeout": "5m",
    "success_exit_code": 0
  },
  "inputs": {
    "rig": "$RIG_NAME",
    "branch": "$BRANCH_NAME",
    "min_coverage": "80"
  }
}
```

### Step 3: Write the Entry Point

Create `run.sh`:

```bash
#!/bin/bash
set -e

echo "Checking code coverage for $RIG_NAME ($BRANCH_NAME)"
echo "Minimum required: ${MIN_COVERAGE}%"

cd "$RIG_PATH/refinery/rig"

# Run coverage tool
COVERAGE=$(go test -coverprofile=cover.out ./... 2>&1 | grep "coverage:" | awk '{print $2}' | tr -d '%')

echo "Coverage: ${COVERAGE}%"

if (( $(echo "$COVERAGE >= $MIN_COVERAGE" | bc -l) )); then
    echo "PASS: Coverage meets threshold"
    exit 0
else
    echo "FAIL: Coverage ${COVERAGE}% is below minimum ${MIN_COVERAGE}%"
    exit 1
fi
```

### Step 4: Make Executable and Test

```bash
chmod +x ~/gt/plugins/my-plugin/run.sh

# Test manually
gt plugin run my-plugin --rig myproject

# Check it appears in the list
gt plugin list
```

---

## Plugin Environment Variables

Plugins receive these variables automatically:

| Variable | Description |
|----------|-------------|
| `$RIG_NAME` | Name of the current rig |
| `$RIG_PATH` | Absolute path to the rig directory |
| `$BRANCH_NAME` | Git branch being processed |
| `$BEAD_ID` | Bead ID associated with the work |
| `$CONVOY_ID` | Convoy ID if work is part of a convoy |
| `$AGENT_NAME` | Name of the agent triggering the plugin |
| `$AGENT_ROLE` | Role of the triggering agent |
| `$GT_HOME` | Path to the town directory |
| `$PLUGIN_DIR` | Path to the plugin's own directory |

Custom inputs defined in `plugin.json` are also passed as uppercase environment variables (e.g., `min_coverage` becomes `$MIN_COVERAGE`).

---

## Debugging Plugins

When a plugin fails, the first step is understanding where and why.

### Viewing Execution Output

Plugin stdout and stderr are captured in the execution history:

```bash
# See recent runs with output
gt plugin history my-plugin --last 5

# See detailed output of a specific run
gt plugin history my-plugin --verbose
```

### Running with Debug Output

Add verbose logging to your `run.sh` during development:

```bash
#!/bin/bash
set -euo pipefail

# Debug: print all environment variables available to the plugin
echo "=== Plugin Environment ==="
echo "RIG_NAME: $RIG_NAME"
echo "RIG_PATH: $RIG_PATH"
echo "BRANCH_NAME: $BRANCH_NAME"
echo "BEAD_ID: $BEAD_ID"
echo "PLUGIN_DIR: $PLUGIN_DIR"
echo "========================="

# Your plugin logic here...
```

### Common Failure Causes

| Symptom | Likely Cause | Fix |
|---------|-------------|-----|
| Plugin not found | Wrong directory or missing `plugin.json` | Check `gt plugin list` and verify path |
| Permission denied | `run.sh` not executable | `chmod +x run.sh` |
| Exit code 127 | Command not found in plugin script | Check `$PATH` or use absolute paths |
| Timeout | Plugin exceeds configured timeout | Increase `timeout` in `plugin.json` or optimize |
| Works manually, fails in pipeline | Missing environment variables | Check Plugin Environment Variables table |

### Testing Locally Before Deploying

Always test plugins outside the pipeline first:

```bash
# Dry run: shows what would happen without executing
gt plugin run my-plugin --rig myproject --dry-run

# Manual run with custom inputs to simulate different scenarios
gt plugin run my-plugin --rig myproject --input branch=main
gt plugin run my-plugin --rig myproject --input branch=feature/untested
```

---

## Plugin Types Beyond Gates

While gates are the most common plugin type, Gas Town supports four plugin types:

### Action Plugins

Actions run in response to triggers but do not block workflow progress. Use these for notifications, logging, or side effects.

```json
{
  "name": "deploy-notify",
  "type": "action",
  "trigger": "post-merge",
  "config": {
    "command": "./notify.sh",
    "timeout": "30s"
  }
}
```

### Hook Plugins

Hooks intercept lifecycle events and can modify behavior. They run synchronously at specific points in the agent lifecycle.

```json
{
  "name": "pre-spawn-check",
  "type": "hook",
  "trigger": "on-spawn",
  "config": {
    "command": "./check-capacity.sh",
    "timeout": "10s"
  }
}
```

### Schedule Plugins

Scheduled plugins run on a cron schedule independently of workflow events.

```json
{
  "name": "nightly-cleanup",
  "type": "schedule",
  "config": {
    "schedule": "0 3 * * *",
    "timezone": "America/Los_Angeles",
    "command": "./cleanup.sh",
    "timeout": "5m"
  }
}
```

---

## Plugin Best Practices

1. **Keep plugins fast.** Plugins that run as gates block the merge pipeline. Aim for under 60 seconds; use the `timeout` config to prevent runaway executions.

2. **Make plugins idempotent.** Plugins may be retried on failure. Ensure they produce the same result when run multiple times.

3. **Use exit codes correctly.** Exit code 0 means success (gate opens). Any non-zero exit code means failure (gate stays closed).

4. **Log meaningful output.** Plugin stdout is captured in the execution history. Write clear messages that help diagnose failures.

5. **Version your plugins.** Use semantic versioning in `plugin.json` and keep a changelog for team visibility.

6. **Test plugins manually first.** Use `gt plugin run --dry-run` and then `gt plugin run` before relying on them in the merge pipeline.

:::tip[Plugin Ideas]

- **Lint gate**: Run ESLint, Pylint, or golangci-lint before merge
- **Coverage gate**: Enforce minimum test coverage thresholds
- **Security scan**: Run dependency vulnerability checks
- **Deploy notifier**: Post to Slack/Discord after successful merge
- **Cost alert**: Notify when daily token spend exceeds a budget
- **Changelog enforcer**: Require changelog updates with each PR


:::

## See Also

- **[Configuration CLI](../cli-reference/configuration.md)** -- Plugin configuration commands
- **[Deacon](../agents/deacon.md)** -- Runs plugins during patrol cycles


========================================================================
# Guides
# URL: /docs/guides
========================================================================

# Guides

This section provides in-depth guides for using Gas Town effectively, understanding its design philosophy, and getting the most out of multi-agent AI development.

---

## Sections

| Guide | Description |
|-------|-------------|
| [Usage Guide](usage-guide.md) | Comprehensive walkthrough for day-to-day Gas Town usage |
| [The 8 Stages of AI Coding](eight-stages.md) | Understanding where Gas Town fits in the AI coding maturity model |
| [Multi-Runtime Support](multi-runtime.md) | Using Gas Town with Claude, Gemini, Codex, Cursor, and more |
| [Cost Management](cost-management.md) | Monitoring and optimizing token spend |
| [Background & Philosophy](philosophy.md) | Why Gas Town exists, its history, and design philosophy |
| [Architecture Guide](architecture.md) | Comprehensive tour of Gas Town's multi-agent architecture |
| [Troubleshooting](troubleshooting.md) | Solutions for common Gas Town problems and workarounds |
| [Glossary](glossary.md) | Complete terminology reference for all Gas Town concepts |

## Who Are These Guides For?

Gas Town is a power tool. These guides assume you are:

- Comfortable with the command line and git
- Already using AI coding agents (Claude Code, Gemini CLI, Codex, etc.)
- Ready to coordinate **multiple** agents working in parallel
- Willing to invest time learning a new operational model

If you are new to AI-assisted coding, start with a single agent (Stage 5-6 in the [8 Stages model](eight-stages.md)) before adopting Gas Town.

## Reading Order

For new Gas Town users, we recommend this reading order:

1. **[The 8 Stages of AI Coding](eight-stages.md)** -- Understand where Gas Town fits and whether you are ready for it
2. **[Background & Philosophy](philosophy.md)** -- Understand why Gas Town exists and the mental model behind it
3. **[Architecture Guide](architecture.md)** -- Understand how the agents, rigs, and pipelines fit together
4. **[Usage Guide](usage-guide.md)** -- Learn the day-to-day workflows and commands
5. **[Multi-Runtime Support](multi-runtime.md)** -- If you use agents other than Claude Code
6. **[Cost Management](cost-management.md)** -- Essential for anyone running at scale

:::tip[Already Running Gas Town?]

If you have completed the [Getting Started](../getting-started/index.md) guide and have a working installation, jump straight to the [Usage Guide](usage-guide.md) for practical workflows and patterns.


:::


========================================================================
# Usage Guide
# URL: /docs/guides/usage-guide
========================================================================

# Usage Guide

This guide covers practical day-to-day Gas Town usage patterns -- from working with the Mayor, to managing multiple rigs, to the mandatory session completion workflow. It assumes you have a working installation (see [Getting Started](../getting-started/index.md)) and are familiar with the basic concepts.

:::info[Source]

Much of the operational advice in this guide comes from the [Gas Town Emergency User Manual](https://steve-yegge.medium.com/gas-town-emergency-user-manual-cf0e4556d74b) by Steve Yegge, written after the first two weeks of Gas Town's public release.

:::

---

## Entry Points

Your two main entry points into Gas Town are:

```bash
gt may at    # Attach to the Mayor
gt crew at <name> --rig <rig>   # Attach to a Crew workspace
```

**`gt may at`** (short for `gt mayor attach`) drops you into the Mayor's tmux session. From there you can give instructions, check progress, and coordinate all your rigs. This is the primary interface for most Gas Town work.

**`gt crew at`** (short for `gt crew at <name> --rig <rig>`) attaches you to a named Crew workspace -- a persistent, long-lived development environment where you work hands-on with code alongside your AI agents.

These two commands represent the two modes of Gas Town operation: **directing** (via the Mayor) and **doing** (via Crew). Most sessions involve both -- you direct work through the Mayor, then drop into Crew workspaces for design decisions, code review, or hands-on implementation.

:::tip[Mayor-Only Mode]

You technically only need the Mayor. Running with just the Mayor is a good tutorial and introduction to Gas Town. The Mayor can file and fix issues itself, or you can ask it to sling work to polecats. Even alone, it still gets all the benefits of GUPP, MEOW, and the full agent lifecycle.

:::

---

## The Three Developer Loops

Gas Town operations follow three nested feedback loops. Understanding these loops helps you work at the right cadence for each type of task.

### Outer Loop (Days to Weeks)

The outer loop covers strategic planning and infrastructure-level decisions:

- **System upgrades** -- Gas Town ships updates frequently; upgrade daily if you are actively using it
- **Capacity planning** -- How many rigs, how many polecats per rig, which projects are active
- **Town-level maintenance** -- Cleanup, cost review, architectural decisions
- **Workflow evolution** -- Your workflows will change as you gain experience; expect to adjust weekly

The outer loop is where you decide *what* to build and *how* to organize your fleet.

### Middle Loop (Hours to Days)

The middle loop covers the active work session:

- **Agent spawning decisions** -- When to spin up polecats, when to throttle back
- **Mayor + Polecat coordination** -- Breaking work into beads, creating convoys, monitoring progress
- **Capacity throttling** -- Adjusting agent count based on workload and cost targets
- **Rig cycling** -- Moving between rigs as work progresses; with one rig fully spinning, move to the next and start the same loop

The middle loop is where you turn plans into tracked work and keep the fleet productive.

### Inner Loop (Minutes)

The inner loop is the moment-to-moment operational cadence:

- **Frequent handoffs** -- Use `gt handoff` after every task in every worker; only let sessions go long if they need to accumulate important context for a big design or decision
- **Clear task specification** -- Be specific when delegating to agents
- **Output review** -- Read what agents produce; some finish simple tasks and just need another assignment, others surface questions or complex summaries that require your time
- **Real-time adjustments** -- Redirect agents, escalate blockers, reassign work

The inner loop is where execution happens. Speed here comes from clear instructions and fast feedback cycles.

```
Outer Loop (Days-Weeks)
│  Strategic planning, upgrades, capacity decisions
│
├── Middle Loop (Hours-Days)
│   │  Spawning agents, creating convoys, rig cycling
│   │
│   └── Inner Loop (Minutes)
│       Task delegation, output review, handoffs
```

---

## Working with the Mayor

The Mayor is your primary interface for multi-agent coordination. You talk to the Mayor in natural language, and it handles issue creation, work assignment, convoy management, and progress tracking.

### Starting a Session

```bash
gt mayor attach
```

This attaches your terminal to the Mayor's tmux session. You can now interact with it directly.

### Giving Instructions

Be specific and action-oriented:

```
Good:  "Fix the 5 failing tests in the auth module and add input
        validation to the user registration endpoint."

Bad:   "Make the code better."
```

The Mayor will:

1. Break your request into discrete beads (issues)
2. Create a convoy to track the batch
3. Assign work to polecats across appropriate rigs
4. Monitor progress and handle escalations
5. Report back when the convoy completes

### Checking on Progress

You can ask the Mayor directly:

```
"What's the status of the auth fixes?"
"How many polecats are running right now?"
"Are there any blocked items?"
```

Or use CLI commands from any terminal:

```bash
gt convoy list
gt feed
gt trail --since 30m
```

### Detaching from the Mayor

Press `Ctrl+B` then `D` to detach from the tmux session (the Mayor continues running in the background).

---

## Managing Multiple Rigs

A common Gas Town setup involves 2-5 rigs (projects) running simultaneously.

### Listing Rigs

```bash
gt rig list
```

Sample output:

```
Rigs (3):
  myapp        active   3 polecats   witness: up   refinery: up
  api-server   active   1 polecat    witness: up   refinery: up
  docs         parked   -            witness: -    refinery: -
```

### Working Across Rigs

```bash
# Sling work to a specific rig
gt sling gt-a1b2c myapp
gt sling gt-d3e4f api-server

# Check a specific rig's status
gt rig status myapp

# View a rig's merge queue
gt mq list --rig api-server
```

### Rig-Specific Operations

```bash
# Start agents for one rig
gt rig start api-server

# Park a rig you're not currently using
gt rig park docs

# Bring it back later
gt rig unpark docs
gt rig start docs
```

:::tip[Focus Mode]

Park rigs you are not actively working on. This reduces resource consumption and keeps the feed cleaner.

:::

---

## Session Management and Handoffs

Gas Town agents are designed for long-running sessions with context preservation.

### Context Recovery

When an agent loses context (after compaction, crash, or long idle):

```bash
gt prime
```

This reloads the agent's full context from its CLAUDE.md file, hooks, and beads state. All persistent agents run `gt prime` automatically on startup.

### Handoffs Between Sessions

When transitioning from one work session to another (e.g., end of day):

```bash
# From within an agent session
gt handoff
```

This:

1. Summarizes current state
2. Writes handoff notes to the agent's context
3. Ensures hooks are up to date
4. The next session picks up the handoff notes automatically

### Agent Session Restart

```bash
# Restart the Mayor session
gt mayor restart

# Restart with a different agent
gt mayor restart --agent claude
```

---

## Day-to-Day Usage Patterns

### Morning Startup

```bash
# 1. Start the fleet
gt start --all

# 2. Check overnight activity
gt trail --since 12h

# 3. Review escalations
gt escalate list

# 4. Check convoy progress
gt convoy list

# 5. Attach to Mayor and give today's instructions
gt mayor attach
```

### During Active Development

```bash
# Watch the live feed (keep this in a dedicated terminal)
gt feed

# Periodically check costs
gt costs --today

# If something looks wrong
gt doctor

# Quick status check
gt rig list
```

### End of Day

```bash
# 1. Check what's still running
gt polecat list

# 2. Review convoy status
gt convoy list

# 3. Option A: Leave it running overnight
#    (check gt costs first to estimate overnight spend)

# 4. Option B: Pause until morning
gt down
```

---

## Working with Crew

Crew workspaces are your most powerful tool for sustained, hands-on development within Gas Town. While polecats handle well-specified tasks autonomously, your Crew members are where design work, code review, and complex decision-making happen. See the [Crew agent docs](../agents/crew.md) for setup and commands.

### The Crew Cycle

When you have multiple Crew workspaces running across your rigs, the natural workflow is a **cycling pattern**: rotating through each Crew member in sequence, giving tasks, reviewing output, and moving on.

Here is what a typical Crew cycle looks like:

1. **Attach to a Crew workspace**: `gt crew at <name>` (use `--rig` if needed)
2. **Give a meaty task** -- design decisions, multi-file refactors, documentation, code review
3. **Detach and move to the next Crew** -- while the first one works, attach to the next
4. **Cycle back** -- by the time you have visited each Crew, the earlier ones are finishing

As you cycle through, you will find Crew members in various states:

- **Still working** -- Let them continue; move on
- **Finished with simple output** -- Hand off immediately with `gt handoff` and give the next task
- **Finished with questions or complex summaries** -- Reserve time to read carefully and respond

The Crew for a rig are organized on a tmux cycle group, making it easy to rotate between them with tmux key bindings. Start with 3 Crew per rig and scale up to 7-8 as you become comfortable with the cycling rhythm.

```bash
# Add a new Crew member to a rig
gt crew add alice

# Cycle through Crew members (in tmux, use Ctrl+B then N/P for next/prev)
gt crew at alice
# ... give task, detach ...
gt crew at bob
# ... give task, detach ...
gt crew at carol
# ... cycle back to alice when ready ...
```

:::tip[Crew vs. Polecats]

Use **Crew** for thoughtful work: design, review, complex refactors, and tasks where you need back-and-forth dialog. Use **Polecats** for well-specified, fast tasks where the acceptance criteria are clear up front. Your Crew creates the *guzzoline* (specifications and plans) that the polecat swarms consume.

:::

### The PR Sheriff Pattern

A useful ad-hoc role for Crew members is the **PR Sheriff** -- a Crew workspace that has a permanent hook with standing orders to manage pull requests.

On every session startup, the PR Sheriff:

1. Checks all open PRs across the rig's repository
2. Classifies them into **easy wins** (straightforward, tests pass, small diff) and **needs human review** (complex changes, failing CI, architectural implications)
3. Slings the easy wins to other Crew or polecats for processing
4. Flags the complex ones for your attention

```bash
# Create a PR Sheriff bead with standing orders
bd create --title "PR Sheriff Standing Orders" --type task \
  --description "On each session: check open PRs, classify by complexity, sling easy wins to crew"

# Hook it permanently to a Crew member
gt crew at dave
gt hook <bead-id>
```

The PR Sheriff pattern keeps your PR backlog from growing unbounded. Instead of PRs piling up while you focus on new work, they get triaged and handled continuously.

---

## Landing the Plane (Session Completion)

Landing the plane is the **mandatory** workflow for completing a Gas Town session. Skipping any step risks losing work, leaving stale state, or creating confusion for the next session.

:::danger[Mandatory Workflow]

Every Gas Town session must end with a proper landing. Incomplete landings lead to orphaned work, missed pushes, and broken state.

:::

### The 7-Step Landing Checklist

#### Step 1: File Issues for Remaining Work

Any incomplete work or follow-up items must be tracked:

```bash
# Create beads for remaining work
bd create --title "TODO: finish API pagination" --type task --priority 2
bd create --title "TODO: add tests for edge case X" --type task
```

Do not leave work undocumented. If it is not in a bead, it will be forgotten.

#### Step 2: Run Quality Gates

Ensure all code passes quality checks before pushing:

```bash
# Run tests
cd ~/gt/myproject/refinery/rig
npm test          # or: go test ./...  or: pytest

# Run linting
npm run lint      # or your project's lint command

# Check for build errors
npm run build
```

#### Step 3: Update Issue Status

Close completed beads and update in-progress ones:

```bash
# Close completed beads
bd close gt-a1b2c --reason "Implemented and merged"
bd close gt-d3e4f

# Update in-progress beads
bd defer gt-g5h6i    # Defer until later (waiting for API spec)
```

#### Step 4: Push to Remote (MANDATORY)

This is the most critical step. **All changes must be pushed to the remote repository.**

```bash
# Pull latest and rebase
git pull --rebase

# Sync beads (export to JSONL)
bd sync --flush-only

# Push everything
git push
```

:::danger[Always Push]

Work that is committed locally but not pushed is effectively invisible to other developers and agents. A machine crash or cleanup will lose it. **Always push.**

:::

#### Step 5: Clean Up

```bash
# Clean up stale resources
gt cleanup

# Stop polecats that have finished
gt shutdown --polecats-only
```

#### Step 6: Verify All Changes Committed and Pushed

Double-check that nothing was missed:

```bash
# Check for uncommitted changes
git status

# Verify remote is up to date
git log --oneline origin/main..HEAD
# Should show nothing (all commits pushed)
```

#### Step 7: Hand Off with Context

Write handoff notes for the next session:

```bash
gt handoff
```

Or if you are the human operator, leave a note in the Mayor's mail:

```bash
gt mail send mayor/ -s "End of day handoff" -m "All auth work landed. Remaining: API pagination (gt-g5h6i) deferred until spec is ready. Tests all green."
```

### Landing Checklist Summary

| Step | Command | Purpose |
|------|---------|---------|
| 1 | `bd create` | File remaining work |
| 2 | `npm test` / `go test` | Run quality gates |
| 3 | `bd close` / `bd update` | Update issue status |
| 4 | `git pull --rebase && bd sync --flush-only && git push` | **Push to remote** |
| 5 | `gt cleanup` | Clean up resources |
| 6 | `git status` + `git log` | Verify everything pushed |
| 7 | `gt handoff` | Hand off with context |

---

## 5 Tips for Effective Gas Town Usage

These tips come directly from operational experience with Gas Town in its first weeks of production use.

### 1. Learn tmux

tmux is the backbone of Gas Town's session management. Every agent, every Crew workspace, and the Mayor itself runs in a tmux session. Investing time in tmux proficiency pays off immediately:

- **Learn a new keybinding or feature each day** -- pane splitting, window navigation, copy mode
- **Customize it** -- ask your agent to help you set up a tmux configuration that works for you
- **Learn how to copy text out of the terminal** -- this trips people up more than anything else
- **Use tmux cycle groups** to rotate efficiently between Crew members and agents

```bash
# Essential tmux operations for Gas Town
Ctrl+B D       # Detach from session (agent keeps running)
Ctrl+B N/P     # Next/previous window (cycle through agents)
Ctrl+B [       # Enter copy mode (scroll through output)
Ctrl+B %       # Split pane vertically
Ctrl+B "       # Split pane horizontally
```

:::tip[tmux Is Your Friend]

The tmux tip is the single highest-leverage investment you can make. Gas Town without tmux proficiency is like driving with the parking brake on.

:::

### 2. Bring Your Own Workflow

Gas Town does not force any particular workflow, any more than an IDE does. The basic pattern is always the same -- file beads, then ask agents to implement them -- but the details are entirely up to you.

Expect your workflows to change frequently. In early Gas Town usage, workflows evolve just about every week as you discover what works for your projects and your working style. This is normal and encouraged.

### 3. Start with the Mayor

Do most of your talking to the Mayor until you are comfortable with it. The Mayor is your coordination layer -- it handles issue creation, work assignment, and progress tracking.

You can run Gas Town in **Mayor-only mode** as a tutorial and introduction:

```bash
gt may at
# Give the Mayor instructions directly
# It can file issues, fix code, and sling work to polecats
```

Once you are comfortable with Mayor interactions, expand to using Crew and polecats for parallel execution.

### 4. Handoff Liberally

Use `gt handoff` after every task in every worker. Only let sessions go long if they need to accumulate important context for a big design or decision.

Polecats take this to the extreme -- they self-destruct after submitting their work. For all other workers, there are several ways to hand off:

```bash
# From within an agent session
gt handoff

# Or say "let's hand off" in conversation

# Or shell out
!gt handoff
```

The worker will be spun up on a fresh shift, preserving the tmux session. Short sessions with frequent handoffs prevent context bloat, reduce token costs, and keep agents sharp.

### 5. Work with Your Crew

This is the biggest tip. Your Crew are your named, long-lived workers on each project rig. They are your design team, and they create the specifications and plans that polecat swarms execute.

The difference between a productive Gas Town session and a frustrating one often comes down to how effectively you use your Crew:

- **Give Crew members meaty tasks** -- design reviews, architecture decisions, documentation, complex refactors
- **Cycle through them regularly** -- do not let Crew sit idle; rotate through giving tasks and reviewing output
- **Scale up gradually** -- start with 3 Crew per rig, work up to 7-8 as you get comfortable
- **Use Crew for human-judgment work** -- anything that requires back-and-forth dialog or nuanced decisions

Your Crew are the bridge between your intent and the fleet's execution. Invest time in them.

---

## Quick Reference Commands

### Lifecycle

| Command | Description |
|---------|-------------|
| `gt start` | Start Mayor + Deacon |
| `gt start --all` | Start full fleet |
| `gt down` | Pause (keep state) |
| `gt shutdown` | Stop + cleanup |
| `gt shutdown --all` | Full stop including crew |

### Work Management

| Command | Description |
|---------|-------------|
| `gt sling <bead> <rig>` | Assign work to a rig |
| `gt hook` | Check current hook |
| `gt done` | Mark work complete, submit MR |
| `gt release <bead>` | Release a stuck bead |
| `gt convoy list` | List active convoys |
| `gt convoy stranded` | Find convoys with unassigned work |

### Monitoring

| Command | Description |
|---------|-------------|
| `gt feed` | Live activity stream |
| `gt trail` | Recent activity summary |
| `gt peek <agent>` | View agent output |
| `gt doctor` | Health check |
| `gt costs` | Token usage |
| `gt rig list` | Rig status overview |

### Communication

| Command | Description |
|---------|-------------|
| `gt mail inbox` | Check your inbox |
| `gt mail send <to> -s <subj> -m <msg>` | Send a message |
| `gt nudge <agent> <msg>` | Send sync message |
| `gt escalate <msg>` | Create escalation |
| `gt broadcast <msg>` | Message all agents |

### Beads (Issue Tracking)

| Command | Description |
|---------|-------------|
| `bd create` | Create a new issue |
| `bd list` | List issues |
| `bd show <id>` | Show issue details |
| `bd close <id>` | Close an issue |
| `bd sync --flush-only` | Export beads to JSONL for git |

### Session

| Command | Description |
|---------|-------------|
| `gt prime` | Reload agent context |
| `gt handoff` | Write handoff notes |
| `gt may at` | Attach to Mayor |
| `gt crew at <name>` | Attach to a Crew workspace |
| `gt session at <name>` | Attach to a polecat session |

---

## See Also

- **[Getting Started](../getting-started/quickstart.md)** -- Quick setup guide
- **[Mayor Workflow](../workflows/mayor-workflow.md)** -- The recommended workflow for most users
- **[CLI Reference](../cli-reference/index.md)** -- Full command reference


========================================================================
# The 8 Stages of AI Coding
# URL: /docs/guides/eight-stages
========================================================================

# The 8 Stages of AI Coding

This guide maps out the progression of AI-assisted software development, from basic code completions to building your own multi-agent orchestrator. Gas Town targets users at **Stage 7 and above** -- if you are not yet there, this guide will help you understand the path.

The framework is based on the work of [Steve Yegge](https://steve-yegge.medium.com/), Gas Town's creator, who has been tracking and predicting AI coding adoption since 2023. Yegge's earlier article [Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer) identified six overlapping "modality waves" -- traditional coding, completions, chat, coding agents, agent clusters, and agent fleets -- each roughly 5x more productive than the last. The 8 Stages model refines this into a practitioner-focused maturity framework.

> "If you extrapolated from completions in 2023 to chat in 2024 to agents in early 2025, you arrived inescapably at orchestration arriving in early 2026."
>
> -- Steve Yegge, *[Software Survival 3.0](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b)*

---

## The Stages

### Stage 1: Zero AI

You write all code by hand. No AI assistance of any kind.

```
You: *types every character*
AI:  *does not exist in your workflow*
```

**Characteristics:**

- Pure manual coding
- Traditional IDE features only (syntax highlighting, basic autocomplete)
- "AI is a toy / not ready / will take my job"

**When this was common:** Before 2023

---

### Stage 2: Completions

You use AI-powered code completion -- a smarter version of autocomplete that predicts multiple lines at once.

```
You: def calculate_tax(
AI:  income, rate=0.25):
         return income * rate
```

**Characteristics:**

- Tab-complete suggestions (GitHub Copilot, Codeium, etc.)
- AI fills in boilerplate and obvious patterns
- You accept or reject each suggestion
- Still fundamentally human-driven development

**Tools:** GitHub Copilot, Codeium, TabNine, Amazon CodeWhisperer

**When this became mainstream:** Early 2023

> "Vibe coding just means letting the AI do the work."
>
> -- Steve Yegge, *[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)*

---

### Stage 3: Chat in IDE

You use an AI chat panel alongside your editor. You ask questions, get explanations, request code snippets, and paste them into your project.

```
You: "How do I add JWT authentication to this Express app?"
AI:  *provides explanation and code snippet*
You: *copies and adapts the code*
```

**Characteristics:**

- Side panel chat in your IDE
- Context-aware (can see your current file)
- Copy-paste workflow
- AI is a knowledgeable assistant you consult

**Tools:** Copilot Chat, Cursor Chat, Cody, Continue

**When this became mainstream:** Late 2023

> "Each successive modality wave, beginning with chat, is conservatively about 5x as productive as the previous wave."
>
> -- Steve Yegge, *The Future of Coding Agents*

---

### Stage 4: Agent in IDE with YOLO Mode

You let the AI agent edit files directly in your IDE. In "YOLO mode," you approve changes in batches rather than line by line. The agent can read your codebase, run commands, and make multi-file changes.

```
You: "Refactor the auth module to use middleware pattern"
AI:  *edits 6 files, runs tests, shows diff*
You: "Looks good, apply it"
```

**Characteristics:**

- Agent makes edits directly (not just suggestions)
- Multi-file changes
- Can run terminal commands
- You review and approve batches of changes
- The agent fills your screen with its work

**Tools:** Cursor Agent, Windsurf, Cline, Aider

**When this became mainstream:** Early 2024

> "Once you have tried coding agents, and figured out how to be effective with them, you will never want to go back."
>
> -- Steve Yegge, *[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)*

---

### Stage 5: CLI Single Agent, YOLO

You move from the IDE to a dedicated CLI agent running in your terminal. The agent has full access to your project, runs commands, edits files, and operates with minimal supervision.

```bash
claude "Add input validation to all API endpoints and write tests"
# Agent works for 10-30 minutes autonomously
```

**Characteristics:**

- Full terminal-based agent
- Complete project access
- Runs unsupervised for extended periods
- Single agent, single project
- You review results, not process

**Tools:** Claude Code, Gemini CLI, Codex CLI, Aider

**When this became mainstream:** Late 2024 / Early 2025

:::note[The CLI Turning Point]

Stage 5 is where most developers realize the bottleneck shifts from "writing code" to "managing agents." A single agent can do the work of 2-3 developers, but you can only run one at a time.

> "It's not AI's job to prove it's better than you. It's your job to get better using AI."
>
> -- Steve Yegge, *[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)*

:::

---

### Stage 6: CLI Multi-Agent (3-5 Parallel)

You run 3-5 CLI agents in parallel, each working on a different task. You manage them manually using tmux panes or multiple terminals.

```bash
# Terminal 1
claude "Fix the login bug"

# Terminal 2
claude "Add email validation"

# Terminal 3
claude "Write integration tests for the API"
```

**Characteristics:**

- Multiple agents working simultaneously
- Manual coordination (you assign tasks and track progress)
- Merge conflicts start to appear
- Context management becomes challenging
- 3-5x throughput increase over Stage 5

**When this became common:** Early-Mid 2025

> "They are so incredibly effective that it's easy to get greedy and smother the goose."
>
> -- Steve Yegge, *[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)*

:::warning[The Coordination Wall]

At 3-5 agents, manual coordination is still manageable. Beyond that, you hit the coordination wall: merge conflicts multiply, you lose track of what each agent is doing, and context management becomes a full-time job.

:::

---

### Stage 7: 10+ Agents, Hand-Managed

You scale to 10 or more agents running simultaneously. You develop ad-hoc scripts and workflows to manage them: automated git worktree creation, tmux scripting, custom issue tracking, manual merge coordination.

```bash
# Your custom scripts
./spawn-agent.sh feature-auth worktree-1
./spawn-agent.sh fix-tests worktree-2
./spawn-agent.sh refactor-db worktree-3
# ... 7 more
./check-status.sh
./merge-next.sh
```

**Characteristics:**

- 10+ simultaneous agents
- Custom scripts for lifecycle management
- Home-grown worktree management
- Manual merge sequencing
- You spend significant time on coordination vs. directing work
- Some agents crash; you restart them manually
- Work gets lost; you build recovery tools

**The realization:** You are building an orchestrator whether you planned to or not.

> "The new job of a software developer going forward will soon be managing dashboards of coding agents."
>
> -- Steve Yegge, *[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)*

> "If you're not at least Stage 7, or maybe Stage 6 and very brave, then you will not be able to use Gas Town."
>
> -- Steve Yegge, *[Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04)*

---

### Stage 8: Building Your Own Orchestrator

You formalize the ad-hoc scripts from Stage 7 into a proper orchestration system. You build or adopt a framework that handles agent lifecycle, work distribution, merge coordination, health monitoring, and escalations.

```bash
# A proper orchestration system
gt start --all
gt mayor attach
# "Build the auth system, run all tests, deploy to staging"
# 15 agents coordinate automatically
```

**Characteristics:**

- Formal agent hierarchy and supervision
- Automated work distribution
- Serialized merge queue
- Health monitoring and auto-recovery
- Persistent work state that survives crashes
- Escalation routing for human intervention
- Cost tracking and resource management

**This is where Gas Town lives.**

> "Gas Town is a new take on the IDE for 2026. Gas Town helps you with the tedium of running lots of Claude Code instances."
>
> -- Steve Yegge, *[Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04)*

> "Working effectively in Gas Town involves committing to vibe coding. Work becomes fluid, an uncountable that you sling around freely."
>
> -- Steve Yegge, *[Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04)*

> "Most work gets done; some work gets lost. Fish fall out of the barrel. Some escape back to sea, or get stepped on. The focus is throughput: creation and correction at the speed of thought."
>
> -- Steve Yegge, *[Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04)*

---

## The Progression Timeline

Each stage transition represents roughly a 5x productivity gain, compounding as you climb:

> "Each successive modality wave, beginning with chat, is conservatively about 5x as productive as the previous wave."
>
> -- Steve Yegge, *[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)*

| Stage | Era | Agent Count | Coordination |
|-------|-----|-------------|--------------|
| 1-2 | Pre-2023 / 2023 | 0 | N/A |
| 3-4 | 2023-2024 | 1 (in IDE) | N/A |
| 5 | Late 2024 | 1 (CLI) | N/A |
| 6 | Early 2025 | 3-5 | Manual |
| 7 | Mid 2025 | 10+ | Ad-hoc scripts |
| 8 | Late 2025+ | 10-30+ | Orchestration system |

---

## Where Gas Town Fits

Gas Town is a **Stage 8** orchestration system. It provides the infrastructure that Stage 7 users end up building by hand:

| Stage 7 (DIY) | Stage 8 (Gas Town) |
|----------------|---------------------|
| Custom spawn scripts | `gt sling` / `gt start --all` |
| Manual worktree management | Automatic per-polecat worktrees |
| Ad-hoc issue tracking | Beads (`bd`) integrated tracking |
| Manual merge queue | Refinery with auto-rebase |
| No health monitoring | Witness + Deacon + Daemon |
| Lost work on crash | Hooks persist across restarts |
| Manual escalation (Slack msg) | `gt escalate` with severity routing |
| Unknown costs | `gt costs` with per-agent breakdown |

> "Gas Town helps with all that yak shaving, and lets you focus on what your Claude Codes are working on."
>
> -- Steve Yegge, *[Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04)*

> "What I did was make their hallucinations real, over and over, by implementing whatever I saw the agents trying to do."
>
> -- Steve Yegge, *[Software Survival 3.0](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b)*

---

## Readiness Assessment

### You ARE Ready for Gas Town If:

- [x] You have run 5+ CLI agents simultaneously
- [x] You have written scripts to manage agent lifecycles
- [x] You have dealt with merge conflicts from parallel agents
- [x] You understand git worktrees
- [x] You are comfortable with tmux
- [x] You have a project large enough to benefit from parallel work
- [x] You are willing to spend time learning a new operational model
- [x] You accept the cost implications (~$100/hour at peak)

### You Are NOT Ready If:

- [ ] You have not used a CLI coding agent (Stage 5)
- [ ] You only run one agent at a time
- [ ] You are not comfortable with git branching and merging
- [ ] You prefer GUI-based workflows exclusively
- [ ] You are not willing to invest in learning the system

:::tip[Recommendation]

Be solidly at **Stage 7** before adopting Gas Town. Spend time at Stage 6 (3-5 agents) to build intuition for multi-agent coordination. Then move to Stage 7 (10+ with scripts) to understand the pain points Gas Town solves. Stage 8 will feel natural once you have experienced the limitations of hand-managing many agents.

:::

---

## The Inevitability Argument

Steve Yegge's core thesis: each stage is **inevitable**. Once developers discover that the next stage is possible, competitive pressure ensures widespread adoption within 6-12 months.

> "If you believe the AI researchers -- who have been spot-on accurate for literally four decades"
>
> -- Steve Yegge, *[Software Survival 3.0](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b)*

```
Stage 5 developers: "Why would I need more than one agent?"
Stage 6 developers: "OK, 3 agents is amazing, but 10 is chaos"
Stage 7 developers: "I've built a pile of scripts... wait, is this an orchestrator?"
Stage 8 developers: "Yes. Yes it is."
```

The pattern is consistent: what seems excessive today becomes standard practice within a year. Gas Town is built on the assumption that Stage 8 multi-agent orchestration will be as normal in 2026-2027 as AI completions are today.

> "These systems are, in a meaningful sense, crystallized cognition, a financial asset, very much like (as Brendan Hopper has observed) money is crystallized human labor."
>
> -- Steve Yegge, *[Software Survival 3.0](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b)*

> "Supervisor agents can and will start doing most of that for us very soon."
>
> -- Steve Yegge, *[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)*

> "Gas Town has illuminated and kicked off the next wave for everyone."
>
> -- Steve Yegge, *[Software Survival 3.0](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b)*

---

## See Also

- **[Philosophy](philosophy.md)** -- The broader philosophy behind Gas Town
- **[Mayor Workflow](../workflows/mayor-workflow.md)** -- Stage 7+ workflow automation


========================================================================
# Multi-Runtime Support
# URL: /docs/guides/multi-runtime
========================================================================

# Multi-Runtime Support

Gas Town is not locked to a single AI coding agent. While Claude Code is the default and most thoroughly supported runtime, Gas Town supports multiple agent runtimes -- allowing you to mix and match based on task requirements, cost, or preference.

---

## Supported Runtimes

| Runtime | CLI Command | Provider | Hook Support | Prompt Mode |
|---------|-------------|----------|-------------|-------------|
| **Claude Code** | `claude` | Anthropic | Full | CLAUDE.md |
| **Gemini CLI** | `gemini` | Google | Partial | GEMINI.md / prompt injection |
| **Codex** | `codex` | OpenAI | Partial | AGENTS.md / prompt injection |
| **Cursor** | `cursor` | Cursor | Limited | .cursorrules |
| **Auggie** | `auggie` | Augment | Limited | Prompt injection |
| **Amp** | `amp` | Sourcegraph | Limited | Prompt injection |

### Runtime Capabilities

| Capability | Claude | Gemini | Codex | Cursor | Auggie | Amp |
|-----------|--------|--------|-------|--------|--------|-----|
| Session hooks (startup/shutdown) | Yes | Partial | Partial | No | No | No |
| File-based context (CLAUDE.md) | Yes | Via adapter | Via adapter | Via .cursorrules | No | No |
| Mail integration | Yes | Yes | Yes | Limited | Limited | Limited |
| Nudge support | Yes | Yes | Yes | No | No | No |
| `gt done` integration | Yes | Yes | Yes | Yes | Yes | Yes |
| `gt prime` support | Yes | Yes | Yes | Partial | Partial | Partial |

---

## Configuration

### Default Runtime

Set the default runtime for the entire town:

```bash
gt config default-agent claude
```

This applies to all new polecats unless overridden at the rig or sling level.

### Per-Rig Configuration

Each rig can specify its own default runtime in `settings/config.json`:

```json
{
  "rigs": {
    "myproject": {
      "default_agent": "claude",
      "agent_config": {
        "claude": {
          "model": "opus",
          "max_tokens": 200000
        }
      }
    },
    "ml-pipeline": {
      "default_agent": "gemini",
      "agent_config": {
        "gemini": {
          "model": "2.5-pro"
        }
      }
    }
  }
}
```

### Per-Agent Configuration

```bash
# Set the runtime for a specific agent type
gt config agent set gemini "gemini"
gt config agent set codex "codex"

# Get current agent configuration
gt config agent get claude
```

---

## Using `--agent` with Sling

Override the runtime when slinging work to a polecat:

```bash
# Default runtime
gt sling gt-a1b2c myproject

# Specify a runtime
gt sling gt-a1b2c myproject --agent gemini
gt sling gt-d3e4f myproject --agent codex
gt sling gt-g5h6i myproject --agent claude
```

This spawns the polecat using the specified runtime instead of the rig or town default.

:::tip[When to Mix Runtimes]

- Use **Claude** for complex architectural work requiring deep reasoning
- Use **Gemini** for tasks involving large context windows or Google ecosystem integration
- Use **Codex** for rapid prototyping or OpenAI-ecosystem projects
- Use **Cursor** when you need tight IDE integration for a specific task

:::

---

## Runtime-Specific Considerations

### Claude Code (Default)

Claude Code has the deepest Gas Town integration:

- **Startup hooks:** Claude reads CLAUDE.md automatically on session start, giving it full Gas Town context
- **Session persistence:** Supports long-running sessions with context compaction
- **Native commands:** `gt prime`, `gt hook`, `gt done` work natively within Claude sessions
- **Prompt mode:** CLAUDE.md files in the project root and agent directories

```bash
# No special configuration needed
gt sling gt-a1b2c myproject --agent claude
```

### Gemini CLI

Gemini CLI integration uses adapters to bridge Gas Town's hook system:

- **Startup hooks:** Partial support via GEMINI.md adapter files
- **Context injection:** Gas Town generates a GEMINI.md context file that Gemini reads on startup
- **Prompt mode:** Supports both file-based context and prompt injection

```bash
# Configure Gemini
gt config agent set gemini "gemini"

# The adapter creates a GEMINI.md alongside CLAUDE.md
gt sling gt-a1b2c myproject --agent gemini
```

### Codex

OpenAI's Codex CLI integration:

- **Startup hooks:** Partial support via AGENTS.md adapter
- **Context injection:** Gas Town writes AGENTS.md files for Codex to read
- **Prompt mode:** Supports file-based context via AGENTS.md

```bash
gt config agent set codex "codex"
gt sling gt-a1b2c myproject --agent codex
```

### Cursor

Cursor agent operates differently from CLI-based agents:

- **No startup hooks:** Cursor does not read startup hooks in the same way
- **Context via .cursorrules:** Gas Town generates .cursorrules files with context
- **Limited nudge support:** Cursor sessions cannot receive real-time nudges

```bash
gt config agent set cursor "cursor"
gt sling gt-a1b2c myproject --agent cursor
```

:::warning[Cursor Limitations]

Cursor runs as an IDE extension, not a standalone CLI process. Gas Town's tmux-based session management has limited visibility into Cursor sessions. Health monitoring and nudge support are constrained.

:::

### Auggie and Amp

These runtimes have more limited Gas Town integration:

- **No native hook support:** Context must be injected manually
- **Prompt injection mode:** Gas Town prepends context to the agent's initial prompt
- **Limited monitoring:** Witness health checks have reduced coverage

```bash
gt config agent set auggie "auggie"
gt config agent set amp "amp"
```

---

## Startup Fallback for Hook-Limited Runtimes

Runtimes without full hook support (Cursor, Auggie, Amp) require a startup fallback to ensure agents receive their context and work assignments.

### `gt prime` -- Manual Context Injection

After spawning an agent with a hook-limited runtime, manually inject context:

```bash
# In the agent's terminal/session
gt prime
```

This reads all relevant context (hooks, beads, mail, agent identity) and presents it as a structured prompt that the agent can process.

### `gt mail inbox` -- Check Pending Mail

For runtimes that support text input but not file-based hooks, check for pending messages after priming:

```bash
gt mail inbox
```

This shows all pending messages in the agent's mailbox. Read any relevant messages with `gt mail read <id>` to get assignment context.

### Recommended Startup Sequence for Limited Runtimes

```bash
# 1. Spawn the polecat
gt sling gt-a1b2c myproject --agent auggie

# 2. Attach to the session
gt session at <name> --rig myproject

# 3. Inject context
gt prime

# 4. Check pending mail
gt mail inbox

# 5. The agent now has full context and can begin work
```

:::note

For Claude Code and Gemini, this manual process is not necessary -- hooks handle context injection automatically.

:::

---

## Mixing Runtimes in a Single Rig

You can have polecats running different runtimes in the same rig simultaneously:

```bash
gt sling gt-a1b2c myproject --agent claude    # Complex refactor
gt sling gt-d3e4f myproject --agent gemini    # Documentation update
gt sling gt-g5h6i myproject --agent codex     # Quick bug fix
```

All three polecats work in separate git worktrees within the same rig, and all submit MRs to the same Refinery for merge.

:::warning[Merge Queue is Runtime-Agnostic]

The Refinery does not know or care which runtime produced the code. It validates and merges based on tests and conflict resolution, regardless of which AI wrote the code.

:::

---

## Configuration Reference

### Town-Level (`settings/config.json`)

```json
{
  "default_agent": "claude",
  "agents": {
    "claude": {
      "command": "claude",
      "model": "opus",
      "context_file": "CLAUDE.md",
      "hooks_supported": true
    },
    "gemini": {
      "command": "gemini",
      "model": "2.5-pro",
      "context_file": "GEMINI.md",
      "hooks_supported": "partial",
      "adapter": "gemini-adapter"
    },
    "codex": {
      "command": "codex",
      "context_file": "AGENTS.md",
      "hooks_supported": "partial",
      "adapter": "codex-adapter"
    },
    "cursor": {
      "command": "cursor",
      "context_file": ".cursorrules",
      "hooks_supported": false,
      "startup_fallback": true
    },
    "auggie": {
      "command": "auggie",
      "hooks_supported": false,
      "startup_fallback": true,
      "prompt_injection": true
    },
    "amp": {
      "command": "amp",
      "hooks_supported": false,
      "startup_fallback": true,
      "prompt_injection": true
    }
  }
}
```

### Per-Rig Override

```json
{
  "default_agent": "gemini",
  "agent_overrides": {
    "max_polecats": 5,
    "preferred_runtimes": ["gemini", "claude"]
  }
}
```

---

## Multi-Runtime Best Practices

1. **Start with Claude Code.** It has the deepest integration and is the most thoroughly tested. Add other runtimes once you are comfortable with the basics.

2. **Use the right runtime for the task.** Claude excels at complex reasoning and multi-step refactors. Gemini handles large contexts well. Match the runtime to the work.

3. **Test runtime compatibility.** Before relying on a new runtime in production, run a test polecat and verify that `gt done`, hook reading, and merge submission all work correctly.

4. **Monitor runtime-specific issues.** Different runtimes have different failure modes. Watch `gt feed` for patterns specific to a runtime.

5. **Keep fallback procedures documented.** If you use hook-limited runtimes, document the startup sequence for your team so everyone knows the manual context injection steps.

---

## See Also

- **[Rigs](../concepts/rigs.md)** -- Project containers that can span runtimes
- **[Configuration CLI](../cli-reference/configuration.md)** -- Runtime configuration options


========================================================================
# Cost Management
# URL: /docs/guides/cost-management
========================================================================

# Cost Management

Gas Town burns tokens. A lot of tokens. At peak usage with 10+ polecats running simultaneously, you can expect to spend approximately **$100/hour** -- roughly 10x what a single Claude Code session costs. This guide covers monitoring, understanding, and optimizing your token spend.

---

## Understanding Gas Town Costs

### Where Tokens Go

Every agent session consumes tokens. The breakdown varies by workload, but a typical distribution looks like this:

| Agent Type | % of Total Cost | Why |
|-----------|----------------|-----|
| **Polecats** | 60-70% | Writing code, running tests, iterating on implementations |
| **Witnesses** | 8-12% | Patrol cycles, health checks, nudging stuck polecats |
| **Refineries** | 5-10% | Merge conflict resolution, validation runs |
| **Mayor** | 5-10% | Strategic coordination, convoy management |
| **Deacon** | 3-5% | Health monitoring, lifecycle management |
| **Dogs/Boot** | 1-3% | Infrastructure tasks, triage |

Polecats dominate costs because they do the actual coding work -- reading files, reasoning about changes, running tests, and iterating until done.

### Cost Factors

| Factor | Impact |
|--------|--------|
| Number of polecats | Linear increase in cost |
| Task complexity | Complex tasks = more tokens per polecat |
| Codebase size | Larger codebases = more context tokens |
| Test suite runtime | Longer tests = longer sessions = more tokens |
| Merge conflicts | Conflicts spawn additional polecats for resolution |
| Patrol frequency | More frequent patrols = higher monitoring costs |

### Typical Cost Ranges

| Usage Level | Polecats | Approx. Cost/Hour | Use Case |
|-------------|----------|--------------------|----------|
| **Minimal** | 1-2 | $10-20 | Focused work, single rig |
| **Normal** | 3-5 | $30-50 | Active development, 1-2 rigs |
| **Heavy** | 6-10 | $50-100 | Multi-rig parallel development |
| **Peak** | 10-20+ | $100-200+ | Full fleet, aggressive timeline |

:::warning[Costs Add Up Fast]

An 8-hour workday at "Normal" usage (3-5 polecats) costs $240-400. At "Peak" usage, a full day can exceed $1,000. Monitor continuously and adjust your polecat count based on budget constraints.

:::

---

## Monitoring Costs

### `gt costs` Command

The primary cost monitoring tool:

```bash
# Current session costs
gt costs

# Today's costs
gt costs --today

# This week's costs
gt costs --week

# Breakdown by role
gt costs --by-role

# Breakdown by rig
gt costs --by-rig

# JSON output for external tools
gt costs --json
```

### Sample Output

```
Gas Town Costs (today)

  Total tokens:  4,800,000 input / 1,620,000 output
  Est. cost:     $89.40

  By role:
    Polecats:    $61.30  (68.6%)  [12 sessions]
    Witnesses:   $9.80   (11.0%)  [3 sessions]
    Refineries:  $6.20   (6.9%)   [3 sessions]
    Mayor:       $7.10   (7.9%)   [1 session]
    Deacon:      $3.80   (4.3%)   [1 session]
    Other:       $1.20   (1.3%)

  By rig:
    myapp:       $52.10  (58.3%)
    api-server:  $24.80  (27.7%)
    docs:        $12.50  (14.0%)

  Hourly rate (last 3h avg): $32.40/hr
```

### Real-Time Cost Monitoring

Keep a cost watch running alongside your feed:

```bash
# In terminal 1: activity feed
gt feed

# In terminal 2: cost updates every 5 minutes
watch -n 300 gt costs --today
```

---

## Cost Optimization Strategies

### Strategy 1: Fewer Polecats

The most direct way to reduce costs. Each polecat you remove saves its proportional token spend.

```bash
# Check how many polecats are running
gt polecat list

# Remove unnecessary polecats
gt polecat remove alpha --rig myproject

# Reduce the max polecat count via rig settings
gt rig settings set --rig myproject max_polecats 3
```

:::tip[Quality Over Quantity]

Three well-targeted polecats often outperform eight unfocused ones. Assign specific, well-defined tasks rather than vague directives. Specific tasks complete faster, consuming fewer tokens.

:::

### Strategy 2: Focused Convoys

Bundle related work tightly so polecats do not waste tokens on context-switching or redundant file reads.

```bash
# Instead of many small convoys:
gt convoy create "Fix A" gt-a1
gt convoy create "Fix B" gt-b2
gt convoy create "Fix C" gt-c3

# Create one focused convoy:
gt convoy create "Auth Module Fixes" gt-a1 gt-b2 gt-c3
```

When related beads are in the same convoy, the Mayor can assign them to fewer polecats that share context, reducing redundant file reading.

### Strategy 3: Minimal Mode

Run Gas Town with reduced monitoring overhead for cost-sensitive workloads.

```bash
# Enable minimal mode
gt rig settings set minimal_mode true
```

Minimal mode:

- Increases patrol cycle intervals (5 min to 15 min)
- Reduces Witness verbosity
- Skips non-essential health checks
- Defers non-critical escalations

:::note

Minimal mode trades monitoring responsiveness for lower costs. Problems may take longer to detect. Use it for stable workloads where you are confident agents will not stall.

:::

### Strategy 4: Park Idle Rigs

Rigs you are not actively developing on still consume tokens through their Witnesses and Refineries:

```bash
# Park rigs you are not using today
gt rig park docs
gt rig park staging-env

# Unpark when needed
gt rig unpark docs
```

### Strategy 5: Time-Box Sessions

Rather than running the fleet all day, work in focused sprints:

```bash
# Morning sprint (2 hours)
gt start --all
# ... intensive work ...
gt down

# Afternoon sprint (2 hours)
gt start --all
# ... intensive work ...
gt down
```

A 2-hour sprint at "Normal" usage costs $60-100, compared to $240-400 for an 8-hour continuous run.

### Strategy 6: Reduce Context Size

Large codebases mean more input tokens per agent interaction. Reduce context costs by:

- Using focused CLAUDE.md files that only include relevant context
- Keeping project documentation concise
- Using `.gitignore` patterns to exclude unnecessary files from agent view
- Breaking monorepos into separate rigs so each polecat sees only its relevant code

### Strategy 7: Optimize Merge Flow

Merge conflicts spawn additional polecats for resolution. Reduce conflicts by:

- Assigning related files to the same polecat
- Serializing work on highly-contended files via convoy dependencies
- Running smaller, more frequent merges rather than large batches

---

## Budget-Conscious Workflows

### The "$50/day" Workflow

For teams with a $50/day budget:

```bash
# Use 2 polecats max
gt rig settings set max_polecats 2

# Enable minimal mode
gt rig settings set minimal_mode true

# Work in 2-hour focused sprints
gt start --all
# ... 2 hours of focused work ...
gt down

# Monitor costs
gt costs --today
```

### The "$200/day" Workflow

For teams with a $200/day budget:

```bash
# Use up to 5 polecats
gt rig settings set max_polecats 5

# Normal monitoring
gt rig settings set minimal_mode false

# Work in 4-hour sprints with breaks
gt start --all
# ... 4 hours ...
gt down
# ... break ...
gt start --all
# ... 4 hours ...
gt down

# Monitor costs every hour
gt costs --today --by-role
```

### The "Unlimited" Workflow

For teams where speed matters more than cost:

```bash
# Max polecats
gt rig settings set max_polecats 15

# Full monitoring
gt start --all

# Run all day, monitor hourly
watch -n 3600 gt costs --today
```

---

## Cost Alerts

Set up automated cost alerts using the plugin system:

```json
{
  "name": "cost-alert",
  "type": "schedule",
  "gate_type": "cron",
  "config": {
    "schedule": "0 * * * *",
    "command": "./check-costs.sh",
    "alert_threshold_hourly": 50,
    "alert_threshold_daily": 300
  }
}
```

Example `check-costs.sh`:

```bash
#!/bin/bash
DAILY_COST=$(gt costs --today --json | jq '.total_cost')

if (( $(echo "$DAILY_COST > 300" | bc -l) )); then
    gt escalate --severity high "Daily cost threshold exceeded: \$$DAILY_COST"
fi
```

---

## Cost Comparison

To put Gas Town costs in perspective:

| Resource | Cost/Hour | Equivalent |
|----------|-----------|------------|
| Gas Town (3 polecats) | ~$30-50 | 1/3 of a senior developer hourly rate |
| Gas Town (10 polecats) | ~$100 | ~1 senior developer hourly rate |
| Gas Town (20 polecats) | ~$200 | ~2 senior developers hourly rate |
| Single Claude Code session | ~$10 | Baseline AI coding cost |

The question is not "is this expensive?" but "is the throughput worth the cost?" Gas Town at 10 polecats can move 5-10x faster than a single developer, at roughly the same hourly cost.

---

## Cost Tracking Best Practices

1. **Check `gt costs --today` at least twice per day** -- once in the morning and once before ending your session.

2. **Set a daily budget** and configure cost alerts to notify you when approaching it.

3. **Review `gt costs --by-role` weekly** to identify agents that are disproportionately expensive. A Witness consuming 20% of total cost may indicate a health monitoring loop.

4. **Track cost per completed bead** over time. This metric tells you whether you are getting more efficient: `total_daily_cost / beads_closed_today`.

5. **Park unused rigs aggressively.** Even idle Witnesses and Refineries consume tokens during patrol cycles.

6. **Use `gt down` during breaks.** Do not leave the fleet running during lunch or meetings unless you have active work in progress.

---

## See Also

- **[Patrol Cycles](../concepts/patrol-cycles.md)** -- Tuning patrol intervals to manage costs
- **[Monitoring](../operations/monitoring.md)** -- Monitoring cost-related metrics
- **[Configuration CLI](../cli-reference/configuration.md)** -- Cost-related configuration


========================================================================
# Background & Philosophy
# URL: /docs/guides/philosophy
========================================================================

# Background & Philosophy

Gas Town is not just a tool -- it is a thesis about the future of software development. This guide covers why Gas Town exists, how it evolved, the intellectual foundations behind its design, and the community response to its ideas.

---

## Why Gas Town Exists: The Inevitability Argument

Gas Town's creator, Steve Yegge, makes a straightforward argument: multi-agent AI orchestration is **inevitable**. The progression from code completions to chat to agents to orchestrators follows an exponential curve that has been consistent since 2023.

The argument:

1. AI coding capabilities improve exponentially
2. Each improvement enables a new "stage" of AI-assisted development
3. Competitive pressure forces adoption of each stage within 6-12 months
4. Multi-agent orchestration (Stage 8) is the next inevitable step after hand-managing many agents (Stage 7)

Therefore, someone needs to build the orchestration infrastructure. Gas Town is one such attempt.

:::note[Not a Prediction -- An Observation]

Yegge argues this is not a prediction about the future but an observation of an already-visible trend. The exponential curve has been running for three years. Extrapolating it forward is not speculation; it is pattern recognition.

:::

---

## Development History

Gas Town has gone through four major architectural revisions, each driven by hard-won lessons about what works and what does not in multi-agent coordination.

### v1: The Prototype

As described in [Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04), Yegge began building his first orchestrator in August 2025. The first version was a collection of bash scripts managing Claude Code sessions in tmux. It proved the concept -- multiple agents could work in parallel on the same project -- but coordination was fragile and everything broke constantly.

**Lessons learned:**

- Agents need persistent state that survives crashes
- Manual coordination does not scale beyond 5 agents
- Merge conflicts are the #1 operational problem

### v2: Beads

The second version introduced **Beads**, the git-backed issue tracking system. This gave agents a shared, persistent record of work state. Beads became the coordination primitive -- instead of agents talking directly to each other, they communicated through shared work items.

**Lessons learned:**

- Shared state works, but agents need explicit communication too
- A merge queue is essential once you have more than 3 parallel workers
- Health monitoring cannot be an afterthought

### v3: Python

The third version rewrote the core in Python, adding the agent hierarchy (Mayor, Deacon, Witness), the Refinery merge queue, and the mail/nudge communication system. This was the first version that could reliably run 10+ agents.

**Lessons learned:**

- Python's GIL and process model created performance bottlenecks at scale
- The Erlang/OTP supervision pattern (implemented in Python) was the right model
- The system needed to be faster and more reliable for production use

### v4: Go (Current)

The current version is written in Go, providing:

- Fast daemon process with low overhead
- Native concurrency for lifecycle management
- Single binary distribution
- Sub-millisecond command response times

Go was chosen for operational characteristics, not language preference. The daemon needs to be fast, reliable, and simple. Go excels at all three.

Gas Town is the fourth complete, functioning orchestrator Yegge built in 2025. Each failure taught hard lessons that informed the next iteration.

---

## Mad Max Naming and Theming

Gas Town's naming comes from the Mad Max: Fury Road universe. As Yegge describes in [Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04):

> "Gas Town is a new take on the IDE for 2026. Gas Town helps you with the tedium of running lots of Claude Code instances."

The fictional Gas Town is an oil refinery citadel controlled by a warlord. The metaphor maps surprisingly well to a multi-agent orchestration system:

| Mad Max | Gas Town Concept |
|---------|------------------|
| **Gas Town** | The workspace -- the central hub of operations |
| **Mayor** | The coordinator who runs Gas Town |
| **Rigs** | War rigs -- the vehicles (projects) being managed |
| **Polecats** | The warriors who swing between vehicles on poles, performing quick raids |
| **Refinery** | Where crude output is processed into usable product (code is merged to main) |
| **Witness** | "Witness me!" -- the monitor who watches and validates |
| **Convoy** | A group of vehicles (tasks) traveling together |
| **Deacon** | A religious figure who keeps order -- the health monitor |

The alternative name "Gastown" (one word) references Vancouver B.C.'s historic Gastown district, a nod to the software industry's Pacific Northwest roots.

:::tip[Embracing the Theme]

The Mad Max theme is intentional. Managing 20 AI agents simultaneously is chaotic, high-stakes, and occasionally explosive. The theme sets appropriate expectations. If you want a calm, predictable development experience, Gas Town is not it.

:::

---

## Erlang/OTP Inspiration

Gas Town's architecture borrows directly from Erlang/OTP, the telecommunications platform known for extreme reliability (nine-nines uptime):

### Supervisor Trees

```
                      Daemon
                        |
                      Deacon
                     /      \
              Witness:A    Witness:B
              /    \          |
        Polecat  Polecat   Polecat
```

Each level monitors the level below it. When a child crashes, its supervisor decides what to do (restart, escalate, or ignore). This creates a self-healing system where individual agent failures do not cascade.

### Mailbox Pattern

Agents communicate through mailboxes -- asynchronous message queues. An agent can send mail to any other agent, and the recipient processes messages on its own schedule. This decouples agents temporally -- they do not need to be active simultaneously to communicate.

### "Let It Crash" Philosophy

Rather than writing defensive code to prevent every possible failure, Gas Town follows Erlang's "let it crash" philosophy:

- Polecats are expected to crash sometimes
- Witnesses detect the crash and handle recovery
- Work persists on hooks -- nothing is lost
- A fresh polecat can pick up where the crashed one left off

### Process Isolation

Each agent runs in its own session with its own state. A crash in one agent cannot corrupt another agent's state. This isolation is what makes Gas Town reliable at scale.

---

## Software Survival 3.0

Yegge's "Software Survival 3.0" thesis, published on [Medium](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b) in January 2026, extends beyond coding tools to a broader claim about the evolutionary pressures shaping all software in the AI era.

### The Three Eras

| Era | Selection Pressure | Survivors |
|-----|-------------------|-----------|
| **1.0** (1960s-2000s) | Can you build it at all? | Teams with engineering capability |
| **2.0** (2000s-2020s) | Can you build it faster? | Teams with agile processes and tooling |
| **3.0** (2025+) | Can you build it with AI? | Teams that harness multi-agent AI |

### The Core Thesis

Inference costs tokens, which cost energy, which cost money. For the purposes of computing software survival odds, tokens, energy, and money are all equivalent -- and all are perpetually constrained.

This resource constraint creates a selection pressure that shapes the entire software ecosystem. The rule is simple: **software tends to survive if it saves cognition**.

Teams that effectively use multi-agent AI ship faster. Shipping faster wins. This is not a moral argument ("you should use AI") but an evolutionary one. The selection pressure is already operating.

### The Squirrel Selection Model

Yegge calls his survival framework the **Squirrel Selection Model** -- a deliberately humble name acknowledging it as a "Good Enough" model, while arguing it is better than what most C-suites are working with today. The model is grounded in an evolutionary argument: in any environment with constrained resources, entities that use those resources efficiently tend to outcompete those that do not.

### The Survival Formula

The model expresses software fitness as a ratio of cognitive value to cognitive cost:

```
Survival(T) = (Savings x Usage x H) / (Awareness + Friction)
```

Where:

| Variable | Meaning |
|----------|---------|
| **Savings** | How much cognition (tokens, compute, human thought) the tool saves per use |
| **Usage** | How broadly and frequently the tool applies across different situations |
| **H** | The Human Coefficient -- a multiplier for domains where humans prefer human-made output |
| **Awareness** | The cost of knowing the tool exists (discovery, learning it is available) |
| **Friction** | The cost of actually using the tool (setup, API complexity, error handling) |

`Survival(T)` is a **fitness function** -- when the ratio exceeds 1, the tool tends to survive. When it falls below 1, the tool is selected against; LLMs will synthesize alternatives or route around it.

The Usage term amortizes the awareness cost and lowers the threshold for token savings. A tool does not need to save much per invocation if it is invoked constantly.

### Plot Armor

Tools with extremely high survival ratios -- in the thousands -- acquire what Yegge calls **"plot armor."** They become effectively indestructible.

`grep` is the canonical example. It compresses a nontrivial insight (regular expression matching over streams of text), runs on a radically cheaper substrate than GPU inference, and applies to a near-universal niche. Its survival ratio is so high that no LLM will ever waste tokens re-synthesizing what grep already does perfectly. grep has plot armor.

The concept is borrowed from fiction writing: some characters are so central to the narrative that they cannot die regardless of the danger they face. In software terms, some tools are so efficient that no amount of AI advancement threatens them -- it would always be cheaper to invoke them than to replicate them.

### The Six Levers

The survival formula gives you **six levers** you can pull to improve your software's chances of survival in the AI era:

#### Lever 1: Compressed Insights

Your software survives by compressing hard-won insights into reusable form. The software industry has accumulated decades of knowledge that would be expensive to rediscover from scratch. Tools that encode this knowledge densely are worth preserving.

**Git** is the prime example. Its model -- the DAG of commits, refs as pointers, the index, the reflog -- represents decades of accumulated wisdom about how to track changes. An LLM could theoretically re-derive version control concepts, but it would be wildly wasteful compared to just using git.

#### Lever 2: Cheaper Substrate

Software survives by running on a cheaper computation substrate than GPU inference. Pattern matching over text, image transformation, data parsing -- these are tasks where CPUs beat GPUs by orders of magnitude. It would be irrational to spin up inference to do what `grep`, ImageMagick, or a JSON parser already does.

Tools that leverage this lever include parsers, complex transformers, and many Unix CLI tools. They do their work on traditional compute that costs a fraction of what LLM inference would cost for equivalent output.

#### Lever 3: Broad Usage (Near-Universal Niche)

Niche tools may require incredible savings per invocation to compensate for their narrow applicability. Conversely, tools that apply broadly -- to many situations, many domains, many workflows -- can survive with modest per-use savings because the Usage multiplier in the formula does the heavy lifting.

**Code search engines** exemplify all three levers together: they solve a nontrivial problem with lots of hard-to-discover edge cases (compressed insights), they do it on a cheaper computation substrate than inference (cheaper substrate), and they have found a large, near-universal niche (broad usage). As LLMs produce 10x to 100x as much code, agents will need good search as much as humans ever did.

#### Lever 4: Awareness (Pre-Sales)

Agent attention is becoming a key battleground. For a tool to be used, agents must know it exists. In a niche domain without much competition -- say DNA sequencing -- any tool that saves a few tokens might be quickly noticed and see heavy use. But in crowded domains, awareness is not automatic. Big mediocre players may have all the recognition, and you may have to pay extra to be noticed by agents.

Awareness is the pre-sales problem in the survival formula. Your tool might be excellent, but if neither humans nor agents know about it, the numerator might as well be zero.

#### Lever 5: Reducing Friction (Post-Sales)

If Awareness is a pre-sales problem, then Friction is the post-sales problem. An agent may be perfectly aware that it has a useful tool, but even a small amount of friction may change its calculation. Agents always act like they are in a hurry, and if something appears to be failing, they will rapidly switch to workarounds.

Friction includes API complexity, poor documentation, unreliable behavior, and slow response times. Every bit of friction pushes the denominator higher and the survival ratio lower. Tools that are easy to invoke and predictable in their behavior have an inherent survival advantage.

#### Lever 6: The Human Coefficient (H)

The Human Coefficient is a fundamentally different selection pressure -- not efficiency, but **human preference**. There will be a large set of domains where people prefer a human's work even when an AI can do "better" by some measurable standard.

Even if AIs become the best teachers, some people will insist on human teachers. Even if AI-generated art is technically flawless, some collectors will prefer human-made art. The Human Coefficient is a multiplier that captures this preference -- software operating in high-H domains has an extra survival advantage that pure efficiency calculations miss.

You can still benefit from the other five levers, but in high-H domains, the human element is itself a form of savings that the formula accounts for.

### Temporal: A Case Study

Yegge uses [Temporal](https://temporal.io/) -- the workflow orchestration engine -- as a detailed case study for applying the survival formula.

Temporal scores highly on the first three levers:

- **Compressed Insights**: Temporal encodes deep knowledge about distributed workflow orchestration -- retry logic, state management, failure recovery, and durable execution. These are problems with subtle edge cases that took years of engineering to solve correctly.
- **Cheaper Substrate**: Temporal handles complex distributed coordination in traditional compute, which is far cheaper than re-synthesizing equivalent orchestration logic through LLM inference every time.
- **Broad Usage**: As agentic workflows take center focus in 2026, Temporal's applicability is near-universal. Yegge describes it as "as broadly useful as PostgreSQL" for the emerging world of agent-driven software.

However, Temporal faces challenges on levers 4 and 5. Compared to PostgreSQL -- which has been around for decades and has massive training data representation -- Temporal has comparatively higher awareness and friction costs. It is less well-known, and LLMs have less training data about its APIs and patterns. This means Temporal's raw survival ratio is moderated by the denominator, even though its numerator is strong.

The case study illustrates a key insight: **a tool can be genuinely excellent (high numerator) and still face survival pressure if its denominator is too high.** The six levers are not just about building good software -- they are about the full equation.

### Competition and Survivability Thresholds

The survival formula does not operate in a vacuum. Your software's survivability threshold **floats above 1** when competitors exist.

A tool with a survival ratio of 1.2 is technically viable in isolation -- it saves more cognition than it costs. But if a competitor achieves a ratio of 2.5 in the same niche, the 1.2 tool is effectively dead. Agents and developers will choose the higher-ratio option every time.

This means survival is not a fixed bar. It is a relative competition:

- In an **uncontested niche**, a ratio slightly above 1 may suffice
- In a **crowded market**, you need to significantly outperform alternatives
- **Dominant incumbents** with high ratios raise the bar for everyone else

The competitive dynamic also explains why some categories consolidate rapidly. Once a tool establishes a high survival ratio and gains awareness (low denominator from widespread adoption), it becomes very difficult to displace -- it has effectively acquired plot armor through competitive dominance.

### Implications for Software Teams

The Software Survival 3.0 framework leads to concrete strategic advice:

- **Build something that would be crazy to re-synthesize.** Compress genuine insights. Encode hard-won knowledge. The denser your insight compression, the higher your Savings term.
- **Make it easy to find.** Invest in awareness. Agent-discoverable documentation, MCP tool listings, and prominent placement in training data all lower the Awareness cost in the denominator.
- **Make it easy to use.** Reduce friction ruthlessly. Clean APIs, predictable behavior, fast response times. Every bit of friction you remove improves your survival ratio.
- **Identify your niche breadth.** Understand whether your Usage multiplier compensates for your per-invocation Savings. Broad tools can survive with modest savings; niche tools need to save dramatically per use.
- **Consider the Human Coefficient.** If you operate in a domain where human preference matters, lean into it. The H multiplier is a legitimate survival advantage.

### The Vibe Coding Commitment

Gas Town embraces what Yegge calls "vibe coding" -- the practice of letting AI do the work while you focus on direction and review. As he writes in [Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04):

> "Working effectively in Gas Town involves committing to vibe coding. Work becomes fluid, an uncountable that you sling around freely."

> "Most work gets done; some work gets lost. Fish fall out of the barrel. Some escape back to sea, or get stepped on. The focus is throughput: creation and correction at the speed of thought."

This tolerance for imperfection in exchange for throughput is a conscious design choice, not a flaw.

### Broader Implications

- Solo developers with orchestrated AI can match the output of small teams
- Small teams with orchestrated AI can match the output of large teams
- Large teams that do not adopt AI orchestration will lose their advantage of scale
- The "10x developer" becomes the "100x developer" not through skill but through leverage

---

## Prediction Track Record

Yegge has publicly tracked his predictions about AI coding adoption:

### The Exponential Curve

| Year | Stage | Prediction | Outcome |
|------|-------|-----------|---------|
| 2023 | 1-2 | "Completions are just the beginning" | Correct -- chat and agents emerged within 12 months |
| 2024 | 3-4 | "IDE agents will go YOLO" | Correct -- Cursor, Windsurf, Cline all shipped autonomous modes |
| 2024 | 5 | "CLI agents will replace IDE agents for power users" | Correct -- Claude Code, Gemini CLI, Codex CLI all launched |
| 2025 | 6-7 | "Multi-agent is coming, hand-managed first" | Correct -- widespread adoption of 3-10 parallel agents |
| 2025-2026 | 8 | "Orchestration systems will emerge" | In progress -- Gas Town and competitors exist |

### Exponential Curve Intuition

```
     Capability
         ^
         |                                    *
         |                                *
         |                            *
         |                        *
         |                    *
         |                *
         |            *
         |        *
         |    *
         | *
         +---------------------------------> Time
         2023  2024  2025  2026  2027
```

The key insight: **exponential curves look flat at the beginning and vertical at the end**. Each stage seemed like a modest step when it arrived, but looking back, the cumulative progress is staggering.

From completions (early 2023) to multi-agent orchestration (2025-2026) took less than three years.

---

## "You Will Die"

One of the most polarizing aspects of Gas Town's documentation is its honest warning:

> "You will die."

This is not hyperbole. It means:

- **Your first installation will break.** Gas Town is complex software managing complex AI agents.
- **You will lose work** until you learn the landing-the-plane discipline.
- **You will burn money** before you learn to manage costs effectively.
- **You will spend hours debugging** agent coordination issues.
- **You will question whether this is worth it** approximately once per day.

The warning serves as a filter: if reading "you will die" makes you want to stop, Gas Town is not for you yet. Come back at Stage 7.

If reading it makes you want to figure out how to survive, you are the target audience.

:::warning[This is Real]

Gas Town is not a polished consumer product. It is an advanced tool for power users who are willing to invest significant time and money to get 10x throughput. The learning curve is steep, the costs are real, and the failure modes are creative. Approach it with eyes open.

:::

---

## Community Reception

Gas Town launched on GitHub and was covered on Hacker News, Reddit, and several AI development communities. The reception was characteristically divided.

### The "Concept Car" Camp

> "This is fascinating as a concept but too early for production. It's showing us what 2027 will look like."

Proponents in this camp see Gas Town as a proof of concept -- valuable for exploring what multi-agent orchestration could be, but not ready for daily use by most teams.

### The "How We Code in 2 Years" Camp

> "I've been building exactly this with bash scripts. Gas Town formalizes what Stage 7 users already do."

Proponents in this camp are already managing multiple agents and recognize Gas Town as the next logical step from their ad-hoc scripts.

### The Skeptics

> "Managing 20 AI agents is a solution looking for a problem. One good developer with one agent is fine."

Skeptics question whether the complexity of orchestration is justified by the throughput gains. This maps to historical skepticism about every new development paradigm.

### By the Numbers

As of early 2026:

- **7.3k stars** on GitHub
- **600+ forks**
- Active development with regular releases
- Growing community of contributors

---

## Further Reading

The following articles by Steve Yegge provide the foundational thinking behind Gas Town:

- [Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04) -- Launch announcement and overview
- [Gas Town Emergency User Manual](https://steve-yegge.medium.com/gas-town-emergency-user-manual-cf0e4556d74b) -- Practical usage guide and operational patterns
- [Software Survival 3.0](https://steve-yegge.medium.com/software-survival-3-0-97a2a6255f7b) -- The broader thesis on AI's impact on software development
- [The 8 Stages of AI Coding](eight-stages.md) -- Maturity model for AI-assisted development
- [GitHub Repository](https://github.com/steveyegge/gastown) -- Source code and issue tracker

---

## See Also

- **[Design Principles](../architecture/design-principles.md)** -- Concrete design principles derived from the philosophy
- **[GUPP & NDI](../concepts/gupp.md)** -- Core reliability principles
- **[Eight Stages](eight-stages.md)** -- The maturity model for AI-assisted development


========================================================================
# Architecture Guide
# URL: /docs/guides/architecture
========================================================================

# Architecture Guide

Gas Town is a multi-agent AI orchestration system where dozens of autonomous agents coordinate to build software. This guide walks through the entire architecture in one place -- from the high-level metaphor down to the merge pipeline that lands code on `main`.

If you want reference-style documentation, see the [Architecture](../architecture/index.md) section. This guide is the narrative version: how the pieces fit together and *why* they are shaped the way they are.

---

## The Mental Model: A Steam Engine

Gas Town is a steam engine. Work enters as fuel. Agents are pistons. Code on `main` is the output.

The system's throughput depends on one thing: **when an agent finds work on its hook, it executes immediately.** No confirmation. No planning meetings. No waiting for approval. This is the **Propulsion Principle** ([GUPP](../concepts/gupp.md)), and every architectural decision flows from it.

```mermaid
flowchart LR
    Human["Human<br/>Instructions"] --> Mayor
    Mayor -->|creates beads<br/>slings work| Rigs
    Rigs -->|polecats execute| Code["Feature<br/>Branches"]
    Code -->|gt done| Refinery
    Refinery -->|merge| Main["main branch"]
```

The rest of this guide explains each component in that pipeline.

---

## Town: The Root of Everything

A **town** is the root directory (typically `~/gt/`) that contains all projects, agents, and coordination state. Think of it as the factory floor where everything happens.

```
~/gt/
├── mayor/              # Global coordinator
├── deacon/             # Town-level health monitor
├── daemon/             # Background scheduler (Go process)
├── .beads/             # Town-level issue tracking (hq-* prefix)
├── settings/           # Global configuration
├── scripts/            # Automation scripts
├── logs/               # System logs
├── myproject/          # ← A rig
├── docs/               # ← Another rig
└── ...
```

Town-level state includes the global beads database (prefixed `hq-*`), Mayor and Deacon contexts, daemon process state, and configuration. Everything below this root is either a rig or a town-level agent.

---

## Rigs: Project Containers

A **rig** is a self-contained project unit. Each rig wraps a git repository and provides the full agent infrastructure needed to work on that project autonomously. See the [Rigs concept page](../concepts/rigs.md) for reference details.

```
myproject/                # One rig
├── .beads/               # Rig-level issue tracking
├── metadata.json         # Rig identity and configuration
├── refinery/rig/         # Canonical git clone (Refinery owns this)
├── mayor/rig/            # Mayor's reference worktree
├── witness/              # Witness agent workspace
├── crew/                 # Human developer workspaces
│   └── dave/
├── polecats/             # Ephemeral worker directories
│   ├── toast/            # ← git worktree
│   └── alpha/            # ← git worktree
└── plugins/              # Rig-level plugins
```

Each rig has its own beads prefix (e.g., `ga-` for gastowndocs, `gt-` for gastown). The `bd` CLI routes commands to the correct rig automatically based on issue ID prefix.

A rig can be in one of three states:

| State | Meaning |
|-------|---------|
| **Active** | Agents running, work in progress |
| **Parked** | Agents stopped, state preserved |
| **Docked** | Archived, minimal footprint |

---

## The Agent Hierarchy

Gas Town uses seven agent roles organized in an [Erlang-inspired supervisor tree](../architecture/agent-hierarchy.md). Each level monitors the level below it, providing fault tolerance and automatic recovery.

```mermaid
graph TD
    Daemon["Daemon<br/><i>Go process</i>"]
    Deacon["Deacon<br/><i>Town supervisor</i>"]
    Mayor["Mayor<br/><i>Global coordinator</i>"]
    Boot["Boot<br/><i>Triage dog</i>"]

    Daemon -->|"heartbeat (3m)"| Deacon
    Mayor -->|"strategic<br/>direction"| Deacon
    Deacon -->|"spawns for<br/>triage"| Boot

    subgraph Rig1 ["Rig: myproject"]
        W1["Witness"]
        R1["Refinery"]
        P1["Polecat: Toast"]
        P2["Polecat: Alpha"]
    end

    subgraph Rig2 ["Rig: docs"]
        W2["Witness"]
        R2["Refinery"]
        P3["Polecat: Bravo"]
    end

    Deacon -->|monitors| W1
    Deacon -->|monitors| W2
    W1 -->|watches| P1
    W1 -->|watches| P2
    W2 -->|watches| P3
    P1 -->|"gt done → MR"| R1
    P2 -->|"gt done → MR"| R1
    P3 -->|"gt done → MR"| R2
    R1 -->|merge| Main1["main"]
    R2 -->|merge| Main2["main"]
```

| Agent | Scope | Lifecycle | Count | Purpose |
|-------|-------|-----------|-------|---------|
| **[Mayor](../agents/mayor.md)** | Town | Persistent | 1 | Global coordination and strategy |
| **[Deacon](../agents/deacon.md)** | Town | Persistent | 1 | Health monitoring and lifecycle |
| **[Witness](../agents/witness.md)** | Per-rig | Persistent | 1 per rig | Polecat supervision |
| **[Refinery](../agents/refinery.md)** | Per-rig | Persistent | 1 per rig | Merge queue processing |
| **[Polecats](../agents/polecats.md)** | Per-rig | Ephemeral | Many per rig | Feature implementation |
| **[Dogs](../agents/dogs.md)** | Town | Reusable | Pool | Infrastructure tasks |
| **[Crew](../agents/crew.md)** | Per-rig | Managed | Named | Human developer workspaces |

The hierarchy has a clear division: **persistent agents** (Mayor, Deacon, Witness, Refinery) run continuously and survive restarts. **Ephemeral agents** (Polecats) spawn for a single task, complete it, and self-destruct. **Reusable agents** (Dogs) handle multiple tasks but are managed by the Deacon.

### Mayor: The Coordinator

The Mayor is the human-facing brain of Gas Town. You give the Mayor natural language instructions, and it handles the rest:

1. Decomposes instructions into discrete [beads](../concepts/beads.md) (issues)
2. Bundles related beads into [convoys](../concepts/convoys.md) for tracking
3. Slings work to rigs using `gt sling`
4. Monitors convoy progress and reports back

The Mayor follows the [MEOW workflow](../concepts/meow-stack.md) (Mayor Execution and Orchestration Workflow): **Receive** instructions, **Analyze** scope, **Create** beads, **Bundle** into convoy, **Assign** to rigs, **Monitor** progress, **Report** completion.

The Mayor does *not* monitor health -- that is the Deacon's job. This role separation is deliberate: strategic coordination and operational health monitoring are different concerns.

### Deacon: The Immune System

The Deacon is the town-level watchdog. It receives heartbeats from the Daemon every 3 minutes and runs a patrol cycle every 5 minutes:

1. Check all Witnesses across all rigs
2. Nudge stale Witnesses that have stopped responding
3. Restart unresponsive Witnesses
4. Clean stale hooks (work with no active agent)
5. Detect orphaned work and zombie sessions
6. Spawn the Boot dog for triage when needed

The Deacon monitors **operational health** (is everything running?) while the Mayor handles **strategic coordination** (what should we build?). If a Witness dies, the Deacon restarts it. If a rig has problems the Deacon cannot resolve, it escalates to the Mayor.

### Witness: The Rig Guardian

Each rig has one Witness that supervises all polecats in that rig. The Witness runs a 5-minute patrol cycle:

1. List all polecats in the rig
2. Check each one's activity (last progress timestamp)
3. Nudge stalled polecats (no progress for 15+ minutes)
4. Escalate unresponsive polecats to the Deacon
5. Detect and clean up zombie sessions (crashed without completing)
6. Verify Refinery health

The Witness is also responsible for **nuking** polecat sandboxes after they complete. When a polecat runs `gt done`, the Witness removes its git worktree and reclaims the polecat name for future use.

### Refinery: The Gatekeeper

Each rig has one Refinery that holds the **canonical git clone** and processes the merge queue. All code merges to `main` go through the Refinery -- there is no other path.

The Refinery processes merge requests (MRs) strictly one at a time to prevent race conditions. For each MR:

1. Rebase the feature branch onto latest `main`
2. Check for merge conflicts
3. If conflicts exist: spawn a fresh polecat to resolve them
4. Run validation (tests, linting, builds)
5. If validation passes: fast-forward merge to `main`
6. Mark the bead as complete

The Refinery follows the **Scotty Principle**: it never proceeds past a failure. If validation fails, the MR is rejected and removed from the queue.

### Polecats: The Hands

Polecats are ephemeral worker agents -- the hands of Gas Town. They spawn, implement one feature or fix, submit their work, and self-destruct. A polecat is never idle.

Each polecat gets:

- A **name** from a pool (Toast, Alpha, Bravo, Obsidian, etc.)
- A **git worktree** under `<rig>/polecats/<name>/`
- A unique **git identity** (`toast@rig.gt`, etc.)
- A **hook** with its assigned work

Polecats exist in exactly three states:

| State | Meaning | Response |
|-------|---------|----------|
| **Working** | Actively making progress | Healthy -- leave it alone |
| **Stalled** | No progress for 15+ minutes | Witness nudges it |
| **Zombie** | Session crashed or exited abnormally | Witness cleans up and recovers work |

---

## The Polecat Lifecycle

The polecat lifecycle is the core execution loop of Gas Town. Understanding it explains most of the system's behavior.

```mermaid
stateDiagram-v2
    [*] --> Spawn: Mayor slings work
    Spawn --> LoadContext: gt prime
    LoadContext --> CheckHook: gt hook
    CheckHook --> Execute: Work found on hook

    state Execute {
        [*] --> BranchSetup: Create feature branch
        BranchSetup --> PreflightTests: Verify main is green
        PreflightTests --> Implement: Write code
        Implement --> SelfReview: Review changes
        SelfReview --> RunTests: Validate
        RunTests --> Cleanup: Prepare workspace
        Cleanup --> [*]
    }

    Execute --> Submit: gt done
    Submit --> MR: Merge request to Refinery
    MR --> Nuke: Witness removes worktree
    Nuke --> [*]

    Execute --> Stalled: No progress 15m+
    Stalled --> Execute: Witness nudge
    Stalled --> Zombie: Unresponsive
    Zombie --> Recover: Witness cleans up
    Recover --> [*]: Work re-slung to new polecat
```

### Step by Step

1. **Spawn**: The Mayor slings a bead to a rig. The system creates a git worktree and spawns a new agent session.
2. **Load Context**: The polecat runs `gt prime` to load its role, rig context, and instructions.
3. **Check Hook**: `gt hook` reveals the assigned work. The polecat reads the bead and begins immediately -- no confirmation step.
4. **Execute**: The polecat follows its [molecule](../concepts/molecules.md) steps: set up a branch, run preflight tests, implement the solution, self-review, run tests, clean up.
5. **Submit**: `gt done` commits, pushes the feature branch, submits an MR to the Refinery, and exits.
6. **Nuke**: The Witness removes the git worktree and reclaims the polecat name.

If the polecat crashes at any point, the hook persists. The Witness detects the zombie, cleans up, and the work can be re-slung to a fresh polecat. No work is lost.

---

## Beads: The Coordination Primitive

[Beads](../concepts/beads.md) is Gas Town's AI-native issue tracking system. Every unit of work -- features, bugs, tasks, escalations, merge requests -- is a bead. Beads are the shared language that all agents use to coordinate.

### Two-Level Architecture

Beads exist at two levels:

| Level | Location | Prefix | Purpose |
|-------|----------|--------|---------|
| **Town** | `~/gt/.beads/` | `hq-*` | Mayor coordination, cross-rig tracking |
| **Rig** | `<rig>/.beads/` | Per-rig (e.g., `ga-*`) | Project-level issues |

The `bd` CLI routes commands automatically based on prefix: `bd show ga-abc` routes to the gastowndocs rig, `bd show hq-xyz` routes to town-level beads.

### Bead Lifecycle

```mermaid
stateDiagram-v2
    [*] --> Open: bd create
    Open --> Hooked: gt sling
    Hooked --> InProgress: Agent picks up work
    InProgress --> Done: gt done / bd close
    Done --> [*]

    InProgress --> Escalated: Blocker hit
    Escalated --> InProgress: Resolved
    InProgress --> Deferred: Paused
    Deferred --> Open: gt release
```

Key bead types:

| Type | Purpose |
|------|---------|
| `task` | General work item |
| `bug` | Defect to fix |
| `feature` | New functionality |
| `escalation` | Problem needing attention |
| `merge-request` | Code waiting for merge |
| `convoy` | Batch tracker |

### How Beads Flow Through the System

```mermaid
flowchart TD
    Human["Human gives instructions"]
    Mayor["Mayor analyzes<br/>and creates beads"]
    Convoy["Convoy bundles<br/>related beads"]
    Sling["gt sling assigns<br/>bead to rig"]
    Hook["Bead hooks to<br/>polecat worktree"]
    Work["Polecat implements<br/>the solution"]
    Done["gt done submits<br/>merge request"]
    Refinery["Refinery rebases,<br/>validates, merges"]
    Main["Code lands on main"]
    Close["Refinery closes<br/>the bead"]
    ConvoyCheck["Convoy checks:<br/>all beads done?"]
    ConvoyClose["Convoy auto-closes"]

    Human --> Mayor
    Mayor --> Convoy
    Convoy --> Sling
    Sling --> Hook
    Hook --> Work
    Work --> Done
    Done --> Refinery
    Refinery --> Main
    Main --> Close
    Close --> ConvoyCheck
    ConvoyCheck -->|Yes| ConvoyClose
    ConvoyCheck -->|No| Sling
```

The bead is the unit of work that travels through the entire pipeline. It starts as a human instruction, becomes a tracked issue, gets assigned to an agent, drives implementation, accompanies the merge request, and closes when code lands on `main`.

---

## Convoys: Batch Tracking

A [convoy](../concepts/convoys.md) bundles related beads together for coordinated progress tracking. When you ask the Mayor to "add dark mode support," it might create five beads (CSS variables, toggle component, persistence, tests, docs) and bundle them into one convoy.

```bash
# Mayor creates a convoy
gt convoy create "Dark mode support" ga-a1 ga-b2 ga-c3 ga-d4 ga-e5

# Check progress
gt convoy status hq-cv-001
```

Convoys auto-close when all tracked beads reach a terminal state. The Deacon checks this during its patrol cycle.

Convoys can span multiple rigs -- a single convoy might track beads in `gastowndocs` (documentation), `gastown` (CLI changes), and `beads` (tracking updates). Cross-prefix tracking makes this seamless.

---

## The Merge Pipeline

The merge pipeline is how code gets from a polecat's feature branch to `main`. It is the most operationally critical part of Gas Town.

```mermaid
flowchart TD
    PC["Polecat completes work"]
    Commit["git commit + push<br/>feature branch"]
    GtDone["gt done<br/>submits MR to Refinery"]
    Queue["MR enters<br/>Refinery queue"]
    Dequeue["Refinery dequeues<br/>next MR"]
    Rebase["Rebase onto<br/>latest main"]

    Conflict{"Merge<br/>conflict?"}
    SpawnResolver["Spawn conflict-<br/>resolution polecat"]
    Resolve["Polecat resolves<br/>conflicts"]
    Resubmit["Resubmit to queue"]

    Validate["Run validation<br/>(tests, lint, build)"]
    Pass{"Validation<br/>passes?"}
    Reject["Reject MR<br/>remove from queue"]
    Merge["Fast-forward<br/>merge to main"]
    CloseBead["Mark bead<br/>complete"]
    Nuke["Witness nukes<br/>polecat sandbox"]

    PC --> Commit --> GtDone --> Queue --> Dequeue --> Rebase
    Rebase --> Conflict
    Conflict -->|Yes| SpawnResolver --> Resolve --> Resubmit --> Queue
    Conflict -->|No| Validate
    Validate --> Pass
    Pass -->|No| Reject
    Pass -->|Yes| Merge --> CloseBead --> Nuke
```

### Why This Design?

Multiple polecats work concurrently on the same repository. Without serialized merges, you get race conditions: two polecats both rebase onto the same `main`, both pass tests, both try to merge -- one wins, the other's merge is invalid.

The Refinery solves this by processing MRs strictly one at a time. Each merge updates `main`, and the next MR rebases onto the new `main`. This guarantees that every merge is valid.

### Conflict Resolution

When a rebase produces conflicts, the Refinery does not attempt to resolve them itself. Instead, it spawns a fresh polecat specifically tasked with conflict resolution. This polecat has full context about both the feature and the current state of `main`, resolves the conflicts, and resubmits to the queue. This approach is more reliable than automated resolution because the resolving agent understands the code's intent.

---

## Communication: How Agents Talk

Agents communicate through four mechanisms:

### Mail (Async)

The primary communication channel. Agents send and receive messages through a filesystem-backed mail system.

```bash
# Send mail
gt mail send gastowndocs/witness -s "Subject" -m "Message body"

# Check inbox
gt mail inbox
```

Mail is used for escalations, status reports, handoff context, and coordination messages. It is async -- the sender does not wait for a response.

### Nudge (Sync)

A synchronous message injected directly into a running agent's session. Used by Witnesses to wake stalled polecats.

```bash
gt nudge <agent> "Are you still working?"
```

Nudges are more intrusive than mail -- they interrupt the agent's current context. Used sparingly and only by supervisors.

### Escalations

Priority-routed alerts that travel up the supervision chain:

| Severity | Route |
|----------|-------|
| P0 (Critical) | Bead + Mail to Mayor + Email/SMS to Human |
| P1 (High) | Bead + Mail to Mayor + Email to Human |
| P2 (Medium) | Bead + Mail to Mayor |
| P3 (Low) | Bead only |

```bash
gt escalate "Description" -s HIGH -m "Details"
```

### Hooks (Persistent State)

[Hooks](../concepts/hooks.md) are not a communication channel per se, but they carry persistent work state between agent sessions. A hook survives crashes, restarts, handoffs, and context compaction. It is the mechanism that ensures no work is lost.

---

## Molecules: Structured Workflows

A [molecule](../concepts/molecules.md) is a multi-step workflow that guides an agent through a task. Molecules are instances of **formulas** -- TOML templates that define step sequences, dependencies, and gates.

The standard polecat work molecule (`mol-polecat-work`) includes nine steps:

1. **Load context** -- `gt prime`, verify assignment
2. **Branch setup** -- Create feature branch
3. **Preflight tests** -- Verify `main` is green
4. **Implement** -- Write the code
5. **Self-review** -- Review your own changes
6. **Run tests** -- Validate the implementation
7. **Cleanup workspace** -- Remove debug artifacts
8. **Prepare for review** -- Final commit, push
9. **Submit and exit** -- `gt done`

Molecules provide **crash recovery**. If an agent restarts mid-task, `bd ready` shows the next incomplete step, and work resumes from where it left off. Steps can have dependencies (step 6 depends on step 4), gates (human approval required), and failure handling.

```bash
# Check your current steps
bd ready

# Mark a step complete
bd close <step-id>

# See what's next
bd ready
```

Persistent agents (Witness, Refinery, Deacon) use **patrol molecules** that follow a squash-and-respawn pattern: complete the patrol, squash all step beads, spawn a fresh molecule for the next cycle. This prevents step bead accumulation over time.

---

## Git Worktree Management

Gas Town uses git worktrees to enable concurrent work on the same repository. Each polecat gets its own worktree -- an independent working directory linked to the same git repository.

```
myproject/
├── refinery/rig/      # Canonical clone (.repo.git)
├── mayor/rig/         # Mayor's reference worktree
├── polecats/
│   ├── toast/         # Polecat worktree (feature branch)
│   └── alpha/         # Polecat worktree (different branch)
└── crew/
    └── dave/          # Crew worktree
```

**The Refinery holds the canonical clone.** All other worktrees -- mayor, polecats, crew -- link back to it. This is efficient (one repository, many working directories) and safe (each agent has isolated state on its own branch).

When a polecat finishes and runs `gt done`, the Witness removes the worktree with `git worktree remove`. The repository data stays in the canonical clone. When a new polecat spawns, `git worktree add` creates a fresh working directory.

---

## Self-Healing: The Escalation Chain

Gas Town is designed to recover from failures automatically. The supervision hierarchy forms a chain where each level monitors and heals the level below:

```mermaid
flowchart TD
    subgraph Detection ["Detection Layer"]
        Daemon["Daemon sends<br/>heartbeat every 3m"]
        WPatrol["Witness patrols<br/>every 5m"]
        DPatrol["Deacon patrols<br/>every 5m"]
    end

    subgraph Recovery ["Recovery Actions"]
        NudgePolecat["Nudge stalled<br/>polecat"]
        NukeZombie["Nuke zombie<br/>polecat"]
        ReslingSlung["Re-sling work<br/>to new polecat"]
        RestartWitness["Restart dead<br/>Witness"]
        RestartDeacon["Restart dead<br/>Deacon"]
        EscalateMayor["Escalate to<br/>Mayor"]
        EscalateHuman["Escalate to<br/>Human"]
    end

    WPatrol -->|"polecat stalled<br/>15m+"| NudgePolecat
    NudgePolecat -->|still stuck| NukeZombie
    NukeZombie --> ReslingSlung

    DPatrol -->|"Witness dead"| RestartWitness
    Daemon -->|"Deacon dead"| RestartDeacon

    DPatrol -->|"can't resolve"| EscalateMayor
    EscalateMayor -->|"can't resolve"| EscalateHuman
```

The key principle is **Let It Crash** (borrowed from Erlang): agents can crash, and that is acceptable. What matters is that the supervisor detects the crash and recovers. A polecat that crashes mid-implementation is not a failure -- its work is on the hook, the Witness cleans up the zombie, and a fresh polecat picks up the work.

---

## Putting It All Together

Here is the complete flow from human instruction to code on `main`:

```mermaid
sequenceDiagram
    actor Human
    participant Mayor
    participant Beads as Beads DB
    participant Rig
    participant Polecat
    participant Refinery
    participant Witness
    participant Main as main branch

    Human->>Mayor: "Add dark mode to the docs site"
    Mayor->>Beads: Create beads (ga-a1, ga-b2, ga-c3)
    Mayor->>Beads: Create convoy (hq-cv-001)
    Mayor->>Rig: gt sling ga-a1 gastowndocs

    Rig->>Polecat: Spawn (create worktree, hook bead)
    Polecat->>Polecat: gt prime (load context)
    Polecat->>Polecat: gt hook (find work)
    Polecat->>Polecat: Implement solution
    Polecat->>Polecat: Run tests
    Polecat->>Refinery: gt done (submit MR)

    Refinery->>Refinery: Rebase onto main
    Refinery->>Refinery: Run validation
    Refinery->>Main: Fast-forward merge
    Refinery->>Beads: Close bead ga-a1

    Witness->>Polecat: Nuke sandbox

    Note over Mayor,Main: Mayor slings ga-b2, ga-c3...<br/>Repeat until convoy complete

    Beads->>Beads: All beads done → convoy auto-closes
    Mayor->>Human: "Dark mode convoy complete"
```

### The Five Invariants

These properties hold true at all times in a healthy Gas Town:

1. **Every piece of work is a bead.** Nothing happens without a tracked issue.
2. **Every agent has a clear role.** Roles do not overlap. The Mayor does not merge code. The Refinery does not assign work.
3. **Every hook persists.** Work survives crashes, restarts, and handoffs. No work is lost.
4. **Every merge goes through the Refinery.** There is no shortcut to `main`.
5. **Every agent is monitored.** Persistent agents patrol. Ephemeral agents are watched. Nothing runs unsupervised.

---

## State Management

All persistent state lives in git or the filesystem:

| State | Storage | Survives |
|-------|---------|----------|
| Issues (beads) | SQLite + JSONL export | Everything |
| Work hooks | Git worktrees | Crashes, restarts, handoffs |
| Mail | Filesystem JSONL | Session restarts |
| Configuration | JSON/YAML files | Everything |
| Agent context | CLAUDE.md files (committed) | Everything |
| Activity log | `.events.jsonl` | Everything |

Git is the ground truth. The Refinery's canonical clone is the single source of truth for code. The beads database is the single source of truth for work state. Everything else can be reconstructed from these two sources.

---

## Further Reading

- **[Architecture Overview](../architecture/overview.md)** -- Five-layer reference breakdown
- **[Agent Hierarchy](../architecture/agent-hierarchy.md)** -- Detailed supervision tree
- **[Work Distribution](../architecture/work-distribution.md)** -- Sling, hook, and convoy mechanics
- **[Design Principles](../architecture/design-principles.md)** -- The ten principles behind the architecture
- **[Usage Guide](usage-guide.md)** -- Practical day-to-day workflows
- **[Background & Philosophy](philosophy.md)** -- Why Gas Town exists and how it evolved

---

## See Also

- **[Architecture Overview](../architecture/overview.md)** -- The formal architecture documentation
- **[Agent Hierarchy](../architecture/agent-hierarchy.md)** -- How agents are organized
- **[Design Principles](../architecture/design-principles.md)** -- Core architectural principles


========================================================================
# Troubleshooting
# URL: /docs/guides/troubleshooting
========================================================================

# Troubleshooting

This guide covers the most frequently encountered Gas Town problems and their solutions. For a comprehensive reference of all troubleshooting topics, see [Operations: Troubleshooting](../operations/troubleshooting.md).

---

## `gt done` Fails: "Not Inside a Rig Directory"

**Symptom:** A polecat finishes its work, runs `gt done`, and gets:

```
Error: not inside a rig directory
```

This happens regardless of which directory you run the command from -- worktree, polecat root, rig root, or town root.

**Cause:** `gt done` infers the rig from the current working directory. In some configurations (particularly polecat worktrees nested under `polecats/<name>/`), the directory detection fails to resolve to a valid rig path.

**Workaround:**

1. **Ensure your work is committed and pushed first:**

    ```bash
    git status              # Verify clean
    git push origin HEAD    # Push the branch
    ```

2. **Notify the Witness manually:**

    ```bash
    gt mail send <rig>/witness -s "DONE: <bead-id>" -m "Work complete. Branch pushed. gt done fails (bug ga-w8vv)."
    ```

3. **Escalate if needed:**

    ```bash
    gt escalate "gt done fails: not inside rig directory" -s medium --related <bead-id>
    ```

The Witness will handle merge queue submission and polecat cleanup on your behalf.

:::note

This is a known issue tracked as **ga-w8vv**. Until it is fixed, the workaround above is the standard procedure for polecats that hit this error.

:::

---

## Polecat Churn Cycle

**Symptom:** A polecat spawns, crashes or fails quickly, gets cleaned up by the Witness, the work is re-slung to a new polecat, which also fails -- creating a loop of spawn-fail-respawn.

**Cause:** Usually one of:

- The bead describes work that is fundamentally impossible (missing dependency, invalid API, nonexistent file)
- The test suite is broken on `main`, so every polecat fails preflight
- The codebase has a configuration issue that prevents any agent from working
- A merge conflict keeps reoccurring because the underlying problem is not resolved

**Diagnosis:**

```bash
# Check recent polecat activity for a pattern
gt trail --since 1h

# Look at the bead's history -- has it been slung multiple times?
bd show <bead-id>

# Check if main is green
cd <rig>/refinery/rig
git checkout main && npm test   # or your project's test command
```

**Solutions:**

1. **Break the cycle by releasing the bead:**

    ```bash
    gt release <bead-id>
    ```

    This unhooks the bead and stops it from being automatically re-slung.

2. **Fix the root cause before re-slinging.** If `main` is broken, fix it first. If the bead is impossible, update or close it:

    ```bash
    bd update <bead-id> --description "Updated requirements: ..."
    # or
    bd close <bead-id> --reason "Not feasible: <explanation>"
    ```

3. **Check for a systemic issue:**

    ```bash
    gt doctor
    bd list --status=in_progress   # Look for beads that keep cycling
    ```

:::tip

If you see the same bead ID appearing repeatedly in `gt trail`, that is a churn cycle. The bead is the problem, not the polecats.

:::

---

## `origin/main` vs `origin/master`

**Symptom:** Git commands fail with errors like:

```
fatal: couldn't find remote ref main
```

or polecats push to the wrong branch.

**Cause:** Some repositories use `main` as the default branch, others use `master`. Gas Town defaults to `main` but the remote repository may use `master`.

**Diagnosis:**

```bash
# Check what the remote default branch is
git remote show origin | grep "HEAD branch"

# Check what branches exist
git branch -r
```

**Solutions:**

1. **Verify the remote default branch and use it consistently:**

    ```bash
    # If the remote uses master
    git branch -r | grep origin/master
    git branch -r | grep origin/main
    ```

2. **Set the rig's default branch in configuration** if it differs from `main`:

    ```bash
    # Check current rig configuration
    gt rig show <rig>
    ```

3. **For polecats that encounter this mid-work:** adjust the target branch in your git commands:

    ```bash
    # If origin/master is the correct target
    git rebase origin/master
    git push origin HEAD
    ```

:::warning

The Refinery merges to the repository's default branch. If there is a mismatch between what agents assume (`main`) and what the remote uses (`master`), merges will fail silently or target the wrong branch. Verify this during rig setup.

:::

---

## Bead Routing: Town vs Rig

**Symptom:** `bd show <id>` returns "not found" even though you know the bead exists, or `bd create` puts the bead in the wrong database.

**Cause:** Beads exist at two levels with different prefixes:

| Level | Location | Prefix | Example |
|-------|----------|--------|---------|
| **Town** | `~/gt/.beads/` | `hq-*` | `hq-abc123` |
| **Rig** | `<rig>/.beads/` | Per-rig | `ga-xyz789` (gastowndocs) |

The `bd` CLI routes based on the prefix in the bead ID. If you use the wrong prefix or are in the wrong directory, the command routes to the wrong database.

**Diagnosis:**

```bash
# Check routing rules
cat ~/gt/.beads/routes.jsonl

# Debug routing for a specific bead
BD_DEBUG_ROUTING=1 bd show <bead-id>
```

**Solutions:**

1. **Use the correct prefix.** Every rig has a unique prefix. Check yours:

    ```bash
    bd list   # Shows beads in the current rig's database
    ```

2. **Route explicitly with `--rig`:**

    ```bash
    bd show <bead-id> --rig gastowndocs     # Force rig-level lookup
    bd create --rig gastown --title "..."    # Create in a specific rig
    ```

3. **File in the right place.** The rule: file the bead in the rig that **owns the code** where the fix would be committed.

    | Issue is about... | File in |
    |-------------------|---------|
    | This rig's code | Default (current rig) |
    | `bd` CLI bugs | `beads` rig |
    | `gt` CLI bugs | `gastown` rig |
    | Cross-rig coordination | Town level (`hq-*`) |

---

## Hook Slot Failures

**Symptom:** `gt sling` fails with an error about hooks, or a polecat starts but finds nothing on its hook.

**Common errors:**

```
hook already attached
```

```
no work on hook
```

**Cause:** Hooks are persistent pointers from an agent to its current bead. Hook conflicts happen when:

- A previous polecat crashed without releasing its hook
- A bead is already hooked to another agent
- The hook file was corrupted during an unclean shutdown

**Diagnosis:**

```bash
# Check what's on the hook
gt hook

# Check hooks across a rig
gt hook --agent <polecat> --rig <rig>
```

**Solutions:**

1. **"hook already attached"** -- Release the stale hook first:

    ```bash
    gt release <bead-id>

    # Then re-sling
    gt sling <bead-id> <rig>
    ```

2. **"no work on hook"** -- The polecat was spawned but work was not attached:

    ```bash
    # Check if mail contains the assignment
    gt mail inbox

    # If mail has the work, hook it manually
    gt mol attach-from-mail <mail-id>

    # If no mail either, escalate to Witness
    gt mail send <rig>/witness -s "HELP: empty hook" -m "Polecat spawned with no work"
    ```

3. **Corrupted hook state** -- If the hook seems wrong or inconsistent:

    ```bash
    # Let the Witness clean up
    gt mail send <rig>/witness -s "Hook state inconsistent" -m "Details..."

    # Or if you have direct access, check the hook file
    ls -la <rig>/polecats/<name>/.hook*
    ```

:::tip

Hooks survive crashes -- that is their purpose. If a polecat crashes, the Witness detects the zombie, cleans up, and the hook persists for the next polecat. You should rarely need to manually manage hooks.

:::

---

## Refinery Stuck Merges

**Symptom:** The merge queue has items but nothing is being merged. `gt refinery queue` shows pending MRs but none are progressing.

**Diagnosis:**

```bash
# Check Refinery status
gt refinery status <rig>

# Check the merge queue
gt refinery queue <rig>

# Check for blocked MRs
gt refinery blocked <rig>

# Check Refinery session health
gt peek refinery --rig <rig>
```

**Common causes and solutions:**

### 1. Refinery Session Crashed

The Refinery agent may have crashed or lost context.

```bash
# Restart it
gt refinery restart <rig>
```

### 2. Merge Conflict in the First MR

The Refinery processes MRs strictly in order. If the first MR has an unresolvable conflict, it blocks everything behind it.

```bash
# Check what's at the head of the queue
gt refinery queue <rig>

# If the first MR is stuck on conflicts, skip it
gt refinery release <mr-id>

# Re-sling the problematic bead for a fresh attempt
gt sling <bead-id> <rig>
```

### 3. Validation Failures

Tests or linting fail during the Refinery's validation step.

```bash
# Attach to the Refinery to see what happened
gt refinery attach <rig>

# Check if main itself is broken
cd <rig>/refinery/rig
git pull origin main
npm test   # or your project's test command
```

If `main` is broken, fix it first -- no MR can pass validation against a broken `main`.

### 4. Stale Claim

An MR was claimed by a Refinery worker that crashed before completing.

```bash
# Check for unclaimed MRs
gt refinery unclaimed <rig>

# Release the stale claim
gt refinery release <mr-id>
```

:::note

The Refinery merges MRs **one at a time**, rebasing each onto the latest `main`. This is intentional -- it prevents race conditions. If your queue is backing up, the bottleneck is usually validation speed (test suite runtime) or unresolved conflicts.

:::

---

## Quick Diagnostic Reference

| Problem | First Command |
|---------|---------------|
| General health check | `gt doctor` |
| Something not running | `gt rig list` |
| Work not progressing | `gt trail --since 1h` |
| Polecat issues | `gt polecat list <rig>` |
| Merge queue stuck | `gt refinery queue <rig>` |
| Bead not found | `BD_DEBUG_ROUTING=1 bd show <id>` |
| Agent unresponsive | `gt peek <agent>` |
| Open escalations | `gt escalate list` |

For the full troubleshooting reference, see [Operations: Troubleshooting](../operations/troubleshooting.md).

---

## See Also

- **[Operations Troubleshooting](../operations/troubleshooting.md)** -- Detailed operational troubleshooting
- **[Diagnostics CLI](../cli-reference/diagnostics.md)** -- Diagnostic commands


========================================================================
# Glossary
# URL: /docs/guides/glossary
========================================================================

# Glossary

A comprehensive reference for Gas Town terminology, drawn from the documentation and Steve Yegge's articles on multi-agent AI orchestration.

---

## Agents

| Term | Definition |
|------|------------|
| **[Mayor](../agents/mayor.md)** | The top-level orchestrator agent. Receives instructions from the human, breaks work into beads, creates convoys, and coordinates all other agents. You interact with Gas Town primarily through the Mayor. |
| **[Deacon](../agents/deacon.md)** | The background health coordinator. Monitors agent health, handles escalations, and coordinates recovery when agents crash. Named after a religious figure who keeps order. |
| **[Witness](../agents/witness.md)** | Per-rig monitoring agent. Watches polecats, validates their work, detects crashes, and reports status. Inspired by Mad Max's "Witness me!" |
| **[Refinery](../agents/refinery.md)** | The merge queue agent. Processes completed work from polecats, rebasing and merging to main one at a time. Prevents merge chaos from parallel agents. |
| **[Polecats](../agents/polecats.md)** | Ephemeral worker agents. Spawn, claim a bead, execute a molecule, submit work, then exit. Named after the pole-swinging warriors in Mad Max. |
| **[Dogs](../agents/dogs.md)** | Cross-rig utility agents that handle tasks spanning multiple rigs (e.g., dependency updates, cross-project refactoring). |
| **[Crew](../agents/crew.md)** | Human-paired agents for interactive, collaborative work sessions. Unlike polecats, crew agents are long-lived and maintain context with a human operator. |
| **[Boot](../agents/boot.md)** | Triage agent that assesses incoming work, classifies complexity, and recommends assignment strategies. |
| **Daemon** | The background Go process that manages agent lifecycles, runs health checks, and provides the `gt` CLI interface. |

## Core Concepts

| Term | Definition |
|------|------------|
| **[Bead](../concepts/beads.md)** | An atomic unit of tracked work — an issue, task, bug, or feature. Stored in git via the `bd` CLI. Beads are the fundamental work primitive in Gas Town. |
| **[Convoy](../concepts/convoys.md)** | A batch of related beads that travel together. Created when the Mayor breaks a large request into individual tasks. Tracks overall progress of a batch. |
| **[Hook](../concepts/hooks.md)** | A persistent pointer from an agent to its current bead. The hook is what makes Gas Town crash-safe — agents read their hook on startup to know what they were working on. |
| **[Molecule](../concepts/molecules.md)** | A running instance of a multi-step workflow. Tracks which steps are done, in progress, or pending. Persists in beads so agents can resume after crashes. |
| **Formula** | A TOML-defined template for creating molecules. Formulas are blueprints; molecules are live instances. Gas Town ships with 30+ built-in formulas. See [Formula Workflow](../workflows/formula-workflow.md). |
| **[Gate](../concepts/gates.md)** | An async coordination primitive. A gate blocks a molecule step until a condition is met (e.g., "wait for code review approval" or "wait for dependent bead to complete"). |
| **[Rig](../concepts/rigs.md)** | A project container wrapping a git repository. Each rig has its own set of polecats, a witness, and a refinery. A Gas Town workspace typically runs 2-5 rigs. |
| **Town** | The top-level workspace directory containing all rigs, the Mayor, the Deacon, and shared configuration. Typically located at `~/gt`. See [Workspace Management](../cli-reference/workspace.md). |
| **Worktree** | A git worktree providing file-level isolation for each polecat. Each polecat gets its own worktree so agents never edit shared files simultaneously. |
| **[Wisp](../concepts/wisps.md)** | A sub-bead — a lightweight tracking unit that represents a single step within a molecule. Ephemeral by default (not exported to JSONL). |
| **Mail** | The asynchronous messaging system between agents. Messages are stored as beads and routed by address (e.g., `mayor/`, `myrig/witness`). See [Communication](../cli-reference/communication.md). |
| **Guzzoline** | Slang for the specifications, plans, and designs that Crew members create. Polecats consume guzzoline — they execute the plans Crew produces. See [Crew Collaboration](../workflows/crew-collaboration.md). |
| **DND** | Do Not Disturb mode. When enabled, an agent suppresses non-critical notifications and nudges. Toggle with `gt dnd`. |
| **Seance** | A command (`gt seance`) that lets you query predecessor sessions for context, useful when picking up work from a crashed or cycled agent. |

## The MEOW Stack

| Term | Definition |
|------|------------|
| **[MEOW Stack](../concepts/meow-stack.md)** | **M**olecules, **E**pics, **O**rchestration, **W**orkflows — Gas Town's layered abstraction model. Each layer builds on the one below, from individual molecule steps up to full workflow orchestration. |
| **Protomolecule** | A higher-order orchestration pattern that coordinates multiple molecules working in parallel at the convoy level. |
| **Pour** | The act of creating a molecule instance from a formula template. "Pour the shiny formula" creates a new shiny molecule. See [Formula Workflow](../workflows/formula-workflow.md). |
| **Squash** | Compressing a completed molecule into a single digest bead. Used by patrol agents to keep the beads database clean. |
| **Burn** | Archiving a completed molecule. The molecule is marked done and no longer appears in active status. |

## Design Principles

| Term | Definition |
|------|------------|
| **[GUPP](../concepts/gupp.md)** | Gas Town Universal Propulsion Principle — every operation must move the system forward or leave it unchanged. No operation should move backward. |
| **NDI** | Nondeterministic Idempotence — operations may produce different outputs each time (because AI is nondeterministic), but the system state after execution is equivalent. See [Design Principles](../architecture/design-principles.md). |
| **[Nudge](../cli-reference/nudge.md)** | A synchronous message that interrupts an agent with new context or instructions. The escape hatch for stuck agents. |
| **Let It Crash** | Erlang-inspired philosophy: rather than preventing every failure, design for recovery. Polecats are expected to crash; the system handles it gracefully. See [Design Principles](../architecture/design-principles.md). |
| **Discovery over Tracking** | Agents observe reality each patrol cycle rather than maintaining fragile in-memory state. See [Patrol Cycles](../concepts/patrol-cycles.md) and [Design Principles](../architecture/design-principles.md). |
| **Scotty Principle** | Named after the Star Trek engineer: never proceed past a failure. The Refinery does not merge code that fails validation. Polecats run preflight tests before starting. See [Design Principles](../architecture/design-principles.md). |

## Operations

| Term | Definition |
|------|------------|
| **Landing the Plane** | The mandatory session completion workflow. Includes filing remaining work, running quality gates, updating issues, pushing to remote, and writing handoff notes. |
| **[Patrol Cycle](../concepts/patrol-cycles.md)** | A recurring monitoring cycle. Witnesses, Refinery, and Deacon all run patrol molecules — looping through check-act cycles. See [Patrol Cycles](../concepts/patrol-cycles.md) and [Monitoring](../operations/monitoring.md). |
| **[Escalation](../operations/escalations.md)** | A routing mechanism for problems that an agent cannot resolve on its own. Escalations travel up the agent hierarchy until someone (or the human) handles them. |
| **[Sling](../cli-reference/sling.md)** | Assigning a bead to an agent. `gt sling gt-a1b2c myrig` sends the bead to the specified rig for a polecat to pick up. |
| **Feed** | The live activity stream showing real-time events from all agents across all rigs. See [Diagnostics](../cli-reference/diagnostics.md). |
| **Trail** | A summary of recent activity, more condensed than the live feed. See [Diagnostics](../cli-reference/diagnostics.md). |
| **Prime** | Reloading an agent's full context from its CLAUDE.md file, hooks, and beads state. Run `gt prime` after compaction, crashes, or new sessions. See [Session & Handoff](../cli-reference/sessions.md). |
| **[Handoff](../workflows/handoff-ceremony.md)** | Transferring context from one session to the next. Writes summary notes that the next session picks up automatically. See [Handoff Ceremony](../workflows/handoff-ceremony.md) and [Session Cycling](../concepts/session-cycling.md). |
| **[Session Cycling](../concepts/session-cycling.md)** | Refreshing an agent's context window without losing work. The hook persists across sessions; handoff mail carries context notes. |
| **[Death Warrant](../operations/lifecycle.md)** | A structured cleanup request filed by the Deacon when an agent becomes a zombie. Boot processes warrants to recover work before terminating the process. |
| **Park** | Pausing a rig. The rig's agents stop, but state is preserved. `gt rig park myrig`. See [Rig Management](../cli-reference/rigs.md). |

## Software Survival 3.0

| Term | Definition |
|------|------------|
| **Software Survival 3.0** | Steve Yegge's thesis that competitive selection pressure now favors teams that effectively use multi-agent AI. The third era of software selection pressure. See [The 8 Stages](eight-stages.md). |
| **[The 8 Stages](eight-stages.md)** | A maturity model for AI-assisted development, from Stage 1 (zero AI) through Stage 8 (building your own orchestrator). Gas Town targets Stage 7+ users. |
| **Survival Formula** | (Savings x Usage x H) / (Awareness + Friction) — the formula determining whether a software tool survives. |
| **Plot Armor** | A survival ratio so high (in the thousands) that a tool becomes effectively indestructible. No LLM will waste tokens re-synthesizing what the tool already does perfectly. `grep` is the canonical example. Extreme velocity from AI orchestration can provide plot armor. |
| **Human Coefficient (H)** | A multiplier in the survival formula representing human preference for human-made output. Higher H means the domain values human involvement, giving software in that domain an extra survival advantage beyond pure efficiency. |

## Mad Max Naming

| Gas Town Term | Mad Max Origin | System Role |
|---------------|---------------|-------------|
| **Gas Town** | The oil refinery citadel | The workspace — central hub of operations |
| **Mayor** | Ruler of Gas Town | The coordinator who runs everything |
| **Rig** | War rig (armored truck) | A project being managed |
| **Polecat** | Warriors on poles raiding vehicles | Ephemeral workers doing quick tasks |
| **Refinery** | Where crude oil becomes fuel | Where code is merged to main |
| **Witness** | "Witness me!" (validation cry) | The monitor who watches and validates |
| **Convoy** | Group of vehicles traveling together | A batch of tasks moving through the system |
| **Deacon** | A religious authority figure | The health monitor who keeps order |

## Usage Patterns

| Term | Definition |
|------|------------|
| **Three Developer Loops** | The nested feedback loops for Gas Town operations: Outer Loop (days-weeks, strategic planning), Middle Loop (hours-days, agent coordination), Inner Loop (minutes, task delegation and output review). |
| **PR Sheriff** | An ad-hoc Crew role with a permanent hook to manage pull requests. On each session startup, the PR Sheriff checks open PRs, classifies them by complexity, and slings easy wins to other Crew or polecats. See [Crew Collaboration](../workflows/crew-collaboration.md). |
| **Vibe Coding** | A development approach where you let the AI do the work while focusing on direction and review. Gas Town embraces vibe coding as a core philosophy -- tolerating some work loss in exchange for throughput. |
| **Guzzoline** | Slang for the specifications, plans, and design work that Crew members produce. Polecats consume this guzzoline to execute well-specified tasks. From the Mad Max fuel terminology. See [Crew Collaboration](../workflows/crew-collaboration.md). |

## CLI Quick Reference

| Command | Purpose |
|---------|---------|
| `gt` | The main Gas Town CLI |
| `bd` | The Beads issue tracking CLI |
| `gt start` | Start the Mayor and Deacon |
| `gt start --all` | Start the full agent fleet |
| `gt down` | Pause all agents (preserve state) |
| `gt shutdown` | Full stop and cleanup |
| `gt mayor attach` | Attach terminal to Mayor session |
| `gt sling <bead> <rig>` | Assign work to a rig |
| `gt feed` | Live activity stream |
| `gt trail` | Recent activity summary |
| `gt doctor` | System health check |
| `gt costs` | Token usage and cost tracking |
| `gt prime` | Reload agent context |
| `gt handoff` | Write handoff notes |
| `gt nudge <agent> <msg>` | Send sync message to agent |
| `gt mol status` | Show current molecule progress |
| `gt formula run <name>` | Pour a formula into a molecule |
| `gt may at` | Attach to Mayor (short for `gt mayor attach`) |
| `gt crew at <name>` | Attach to a Crew workspace (use `--rig <rig>` if needed) |
| `gt rig park <rig>` | Pause a rig (preserve state, stop agents) |
| `gt rig unpark <rig>` | Resume a parked rig |
| `gt escalate <msg>` | Create an escalation for problems agents cannot resolve |
| `gt polecat list` | List active polecats across rigs |
| `gt mail inbox` | Check your inbox |
| `gt done` | Signal work ready for merge queue |

## See Also

- [System Overview](../architecture/overview.md) — High-level architecture summary
- [Design Principles](../architecture/design-principles.md) — Core principles governing Gas Town
- [CLI Reference](../cli-reference/index.md) — Full command documentation
- [The 8 Stages](eight-stages.md) — Software Survival 3.0 maturity model
